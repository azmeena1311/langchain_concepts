{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"He learned from his mistakes and accomplished numerous achievements in his life. After completing his degree, Abdul Kalam entered the Defense Department of India. He has been one of the key figures in building the nuclear capabilities of India.\\n\\n \\n\\nAPJ Abdul Kalam was appointed to the Indian Ministry of Defense as a Technical Advisor in 1992, after which he served with DRDO and ISRO, the country's largest organization. Considered a national hero for successful nuclear tests in 1998, a second successful nuclear test was conducted in Pokhran the same year under his supervision, after which India was included in the list of nuclear-powered nations. Abdul Kalam has been active in all space programs and development programs in India as a scientist. For developing India's Agni missile, Kalam was called 'Missile Man.'Abdul Kalam made a special technological and scientific contribution, for which, along with Bharat Ratna, India's highest honour, he was awarded the Padma Bhushan, Padam Vibhushan, etc. He was also awarded an honorary doctorate by more than 30 universities in the world for the same. \\n\\n \\n\\nIn 2002, he was elected President of India and was the country's first scientist and non-political president. He visited many countries during his tenure as President and led India's youth through his lectures and encouraged them to move forward.  ‘My vision for India’ was a Famous Speech of APJ Abdul Kalam delivered at IIT Hyderabad in 2011, and is to this day my favourite speech. His far-reaching thinking gave India's growth a fresh path and became the youth's inspiration. Dr Abdul Kalam died on July 27, 2015, from an apparent cardiac arrest while delivering a lecture at IIM Shillong at the age of 83. He spent his entire life in service and inspiration for the nation and the youth, and his death is also while addressing the youth. His death is a never-ending loss to the country.\\n\\n\\nShort APJ Abdul Kalam Speech In English For Students\\nToday, I am here to deliver a speech on Dr APJ Abdul Kalam. APJ Abdul Kalam was born to Jainulabdeen and Ashiamma on October 15, 1931. His father was a boat owner and his mother was a homemaker. His family's economic situation was not strong, so at an early age, he began helping his family financially.\\n\\n\\nHe graduated in 1955 from the Madras Institute of Technology and graduated from St. Joseph's College, Tiruchirappalli, in Aerospace Engineering. He joined the Defense Research and Development Organization's (DRDO) Aeronautical Development Base as a Chief Scientist after his graduation. He won credit as Project Director-General for making India's first indigenous satellite (SLV III) rocket. It was his ultimate support that brought nuclear power to India. In July 1992, he was appointed Scientific Advisor to the Indian Ministry of Defence. As a national counsellor, he played a significant role in the world-famous nuclear tests at Pokhran II. In 1981, he was awarded the Padma Bhushan Award, in 1909 the Padma Vibhushan, and in 1997 the highest civilian award of India' Bharat Ratna 'for modernizing the defence technology of India and his outstanding contribution. \\n\\n\\nFrom July 25, 2002 - July 25, 2007, he served as President of India, becoming famous among Indians and receiving a lot of attention from Indian youth. He became popular as the People's President. Kalam worked as a professor, chancellor, and assistant at many institutions after leaving office. He experienced serious cardiac arrest on the evening of July 27, 2015, and fell unconscious and died 2 hours later.\\n\\n\\nIn 1999, Kalam published his autobiography and a book called The Wings of Fire. He has written many other books that are useful to the people of every generation.\", metadata={'source': 'speech.txt'})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataIngestion\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "## load,chunk and index the content of the html page\n",
    "\n",
    "loader=WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "\n",
    "                     )))\n",
    "\n",
    "text_documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pdf reader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('Deep Learning.pdf')\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Deep Learning Introduction\\nDeep learning is a branch of machine learning which is completely based on artificial neural\\nnetworks, as neural network is going to mimic the human brain so deep learning is also a kind\\nof mimic of human brain.\\nit is an artificial intelligence (AI) function that imitates the workings of the human brain in\\nprocessing data and creating patterns for use in decision making.\\nKEY T AKEA WAYS\\nDeep learning is an AI function that mimics the workings of the human brain in processing\\ndata for use in detecting objects, recognizing speech, translating languages, and making\\ndecisions.\\nDeep learning AI is able to learn without human supervision, drawing from data that is both\\nunstructured and unlabeled.\\nDeep learning, a form of machine learning, can be used to help detect fraud or money\\nlaundering, among other functions.\\nKey Difference between Machine Learning and Deep\\nLearning :', metadata={'source': 'Deep Learning.pdf', 'page': 0}),\n",
       " Document(page_content='Types of Deep Learning Algorithms That I Coverd In Notebook\\n1. Multilayer Perceptrons (MLPs)\\n2. Convolutional Neural Networks (CNNs)\\n3. Recurrent Neural Networks (RNNs)\\n4. Long Short T erm Memory Networks (LSTMs)\\n5. Generative Adversarial Networks (GANs)\\n6. Restricted Boltzmann Machines( RBMs)\\n7. Autoencoders\\n8. Self Organizing Maps (SOMs)\\nThere are so many techniques but in my note book i will focus on this 8 topic.', metadata={'source': 'Deep Learning.pdf', 'page': 1}),\n",
       " Document(page_content='Neural Networks\\nBefore Deep Dive in to deep learning first see some important terminology regarding this\\nQ1. What are Neural networks?\\nNeural networks are set of algorithms inspired by the functioning of human brian. Generally\\nwhen you open your eyes, what you see is called data and is processed by the Nuerons(data\\nprocessing cells) in your brain, and recognises what is around you. That’ s how similar the\\nNeural Networks works. They takes a large set of data, process the data(draws out the\\npatterns from data), and outputs what it is.  \\n \\nA neural network is composed of layers, which is a collection of neurons, with connections\\nbetween dif ferent layers. These layers transform data by first calculating the weighted sum of\\ninputs and then normalizing it using the activation functions assigned to the neurons.\\nThe leftmost layer in a Neural Network is called the input layer , and the rightmost layer is\\ncalled the output layer . The layers between the input and the output, are called the hidden\\nlayers. Any Neural Network has 1 input layer and 1 output layer .\\nThe number of hidden layers dif fer between dif ferent networks depending on the complexity of\\nthe problem. Also, each hidden layer can have its own activation function.\\n \\n \\nHere 3 terms Comes in picture 1. Neuron , 2. W eights , 3. Bias , 4. Actiation_Function', metadata={'source': 'Deep Learning.pdf', 'page': 2}),\n",
       " Document(page_content='1. Neuron\\nLike in a human brain, the basic building block of a Neural Network is a Neuron. Its\\nfunctionality is similar to a human brain, i.e, it takes in some inputs and fires an output. Each\\nneuron is a small computing unit that takes a set of real valued numbers as input, performs\\nsome computation on them, and produces a single output value.\\nThe basic unit of computation in a neural network is the neuron, often called as a node or unit.\\nIt receives input from some other nodes, or from an external source and computes an output.\\nEach input has an associated weight (w), which is assigned on the basis of its relative\\nimportance to other inputs. The node applies a activation function f (defined below) to the\\nweighted sum of its inputs as in figure below .\\nThe above network have:\\nnumerical inputs X1 and X2\\nweights w1 and w2 associated with those inputs\\nb (called the Bias) associated with it.\\nThe Left side Picture is Neuron Of Human Brain ,The Right Side is Artificial Neuron  \\nBiological Neuron W ork:\\nInformation from other neurons, in the form of electrical impulses, enters the dendrites  at\\nconnection points called synapses . The information flows from the dendrites to the cell where\\nit is processed. The output signal, a train of impulses, is then sent down the axon  to the\\nsynapse of other neurons.  \\n \\nArtificial Neuron W ork:\\nThe arrangements and connections of the neurons made up the network and have three\\nlayers.\\nThe first layer is called the input layer  and is the only layer exposed to external signals.\\nThe input layer transmits signals to the neurons in the next layer , which is called a hidden\\nlayer . The hidden layer extracts relevant features or patterns from the received signals.\\nThose features or patterns that are considered important are then directed to the output layer ,\\nwhich is the final layer of the network.', metadata={'source': 'Deep Learning.pdf', 'page': 3}),\n",
       " Document(page_content='Single Layers And Multi Layer Netwrok\\nIn Multi Layer net there are many number of hidden layer in between input and output\\nlayer ,but in single only one or not hidden layer .\\nWeight:', metadata={'source': 'Deep Learning.pdf', 'page': 4}),\n",
       " Document(page_content='Every input(x) to a neuron has an associated weight(w), which is assigned on the basis of its\\nrelative importance to other inputs.\\nThe way a neuron works is, if the weighted sum of inputs is greater than a specific threshold, it\\nwould give an output 1, otherwise an output 0. This is the mathematical model of a neuron,\\nalso known as the Perceptron.\\nEvery neural unit takes in a weighted sum of its inputs, with an additional term in the sum\\ncalled a Bias.\\nBias:\\nBias is a constant which is used to adjust the output along with the weighted sum of inputs, so\\nthat the model can best fit for the given data.\\nz=w.x+b\\nI defined\\nweighted sum z\\nweight vector w\\ninput vector x\\nbias value b.\\ny=a=f(z)\\nThe output(y) of the neuron is a function f of the weighted sum of inputs z. The function f is\\nnon linear and is called the Activation Function.\\nActivation Function: ¶\\nThe purpose of activation function is to introduce non-linearity into the output of neuron. It\\ntakes a single number , and performs some mathematical operation on it. There are several\\nactivation functions used in practice:\\n1. Sigmoid\\n2. Tanh\\n3. ReLU\\n4. Leaky relu\\n5. Softmax function\\nThese Are most widly use activation function that i covered in subsequent notebook.', metadata={'source': 'Deep Learning.pdf', 'page': 5}),\n",
       " Document(page_content='Forward And Backward Propogation\\nEvery Neural Network has 2 main parts:\\n       1. Feed Forward Propogation/Forward Propogation.  \\n       2. Backward Propogation/Back propogation.  \\n \\nFeed Forward Propogation:\\nAll weights in the network are randomly assigned. Assume the weights of the connections\\nfrom the inputs to that node are w1, w2 and w3.\\nHere I take an example to better understand of this two concept\\nLet take an example of if you study 35 Hour per day then you definitly Pass the exam with 67\\nMark. Now we apply this.  \\n \\nInput to the network = [35, 67]\\nDesired output from the network (target) = [1, 0] 1 means P ASS and 0 means Fail\\nThen output V from the node in consideration can be calculated as below (f is an activation\\nfunction such as sigmoid):\\nV = f (1*w1 + 35*w2 + 67*w3)\\nSimilarly , outputs from the other node in the hidden layer is also calculated. The outputs of the\\ntwo nodes in the hidden layer act as inputs to the two nodes in the output layer . This enables\\nus to calculate output probabilities from the two nodes in output layer .', metadata={'source': 'Deep Learning.pdf', 'page': 6}),\n",
       " Document(page_content='Suppose the output probabilities from the two nodes in the output layer are 0.4 and 0.6\\nrespectively (since the weights are randomly assigned, outputs will also be random). W e can\\nsee that the calculated probabilities (0.4 and 0.6) are very far from the desired probabilities (1\\nand 0 respectively), hence the network in above Figure is said to have an ‘Incorrect Output’.\\nAs it give output as fail but it not happen that one study 35 hr and secure 67.\\nhere W eight are randomly assigned so now we do Back Propagation and with W eight\\nUpdation.\\nBack Propagation and Weight Updation:\\nHere W e calculate the total error at the output nodes and propagate these errors back through\\nthe network using Backpropagation to calculate the gradients.\\nThen we use an optimization method such as Gradient Descent to ‘adjust’ all weights in the\\nnetwork with an aim of reducing the error at the output layer .\\nIf we now input the same example to the network again, the network should perform better\\nthan before since the weights have now been adjusted to minimize the error in prediction.', metadata={'source': 'Deep Learning.pdf', 'page': 7}),\n",
       " Document(page_content=\"As shown in above Figure, the errors at the output nodes now reduce to [0.2, -0.2] as\\ncompared to [0.6, -0.4] earlier . This means that our network has learnt to correctly classify our\\nfirst training example.\\nWe repeat this process with all other training examples in our dataset. Then, our network is\\nsaid to have learnt those examples.\\n1. Thanks To https://medium.com/@purnasaigudikandula/a-beginner-intro-to-\\nneural-networks-\\n543267bda3c8#:~:text=Neural%20networks%20are%20set%20of,the%20functioning%20of%\\n20human%20brian.&text=That's%20how%20similar%20the%20Neural,and%20outputs%20wha\\nt%20it%20is.\\n2. http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html\", metadata={'source': 'Deep Learning.pdf', 'page': 8}),\n",
       " Document(page_content='What is Perceptron?\\nPerceptron is a single layer neural network and a multi-layer perceptron is called Neural\\nNetworks.\\n \\n \\n1. Single-layered perceptron model  \\n2. Multi-layered perceptron model.\\n1. Single-layered perceptron model\\nIf you talk about the functioning of the single-layered perceptron model, its algorithm doesn’t\\nhave previous information, so initially , weights are allocated inconstantly , then the algorithm\\nadds up all the weighted inputs,\\nif the added value is more than some pre-determined value( or , threshold value) then single-\\nlayered perceptron is stated as activated and delivered output as +1.\\nIn simple words, multiple input values feed up to the perceptron model, model executes with\\ninput values, and if the estimated value is the same as the required output, then the model\\nperformance is found out to be satisfied, therefore weights demand no changes. In fact, if the\\nmodel doesn’t meet the required result then few changes are made up in weights to minimize\\nerrors.\\n2. Multi-layered perceptron model', metadata={'source': 'Deep Learning.pdf', 'page': 9}),\n",
       " Document(page_content='In the forward stage, activation functions are originated from the input layer to the output layer ,\\nand in the backward stage, the error between the actual observed value and demanded given\\nvalue is originated backward in the output layer for modifying weights and bias values.\\nIn simple terms, multi-layered perceptron can be treated as a network of numerous artificial\\nneurons overhead varied layers, the activation function is no longer linear , instead, non-linear\\nactivation functions such as Sigmoid functions, T anH, ReLU activation Functions, etc are\\ndeployed for execution.', metadata={'source': 'Deep Learning.pdf', 'page': 10}),\n",
       " Document(page_content='Activation Function\\nActivation function decides, whether a neuron should be activated or not by calculating\\nweighted sum and further adding bias with it. The purpose of the activation function is to\\nintroduce non-linearity into the output of a neuron.\\nExplanation :-\\nWe know , neural network has neurons that work in correspondence of weight, bias and their\\nrespective activation function. In a neural network, we would update the weights and biases of the\\nneurons on the basis of the error at the output. This process is known as back-propagation.\\nActivation functions make the back-propagation possible since the gradients are supplied along\\nwith the error to update the weights and biases.  \\nWhy do we need Non-linear activation functions :-  \\nA neural network without an activation function is essentially just a linear regression model. The\\nactivation function does the non-linear transformation to the input making it capable to learn and\\nperform more complex tasks.\\nThere are several type of Activation But here i dicuss some of them:\\n       1. Sigmoid (Binary Classification)  \\n       2. Tanh  \\n       3. Relu  \\n       4. Leaky Relu  \\n       5. Linear  \\n       6. Softmax (Use Multiclass Classification)\\n1. Sigmoid\\nThe Sigmoid Function curve looks like a S-shape.', metadata={'source': 'Deep Learning.pdf', 'page': 11}),\n",
       " Document(page_content='The main reason why we use sigmoid function is because it exists between (0 to 1).\\nTherefore, it is especially used for models where we have to predict the probability as an\\noutput.Since probability of anything exists only between the range of 0 and 1, sigmoid is the\\nright choice.\\nDerivative Of Sigmoid Function\\nSigmoid Function Derivative range from 0 to 0.25\\nUses :  Usually used in output layer  of a binary classification, where result is either 0 or 1, as value\\nfor sigmoid function lies between 0 and 1 only so, result can be predicted easily to be 1 if value is\\ngreater than 0.5 and 0 otherwise.\\n2. Tanh or hyperbolic tangent Activation Function\\ntanh is also like logistic sigmoid but better . The range of the tanh function is from (-1 to 1).\\ntanh is also sigmoidal (s - shaped).', metadata={'source': 'Deep Learning.pdf', 'page': 12}),\n",
       " Document(page_content='The advantage is that the negative inputs will be mapped strongly negative and the zero\\ninputs will be mapped near zero in the tanh graph.\\nThe activation that works almost always better than sigmoid function is T anh function also\\nknows as T angent Hyperbolic function. It’ s actually mathematically shifted version of the\\nsigmoid function. Both are similar and can be derived from each other .\\nDerivative Of T anh Function:-\\nSigmoid Function Derivative range from 0 to 1\\nUses :- Usually used in hidden layers  of a neural network as it’ s values lies between -1 to 1 hence\\nthe mean for the hidden layer comes out be 0 or very close to it, hence helps in centering the data\\nby bringing mean close to 0. This makes learning for the next layer much easier .', metadata={'source': 'Deep Learning.pdf', 'page': 13}),\n",
       " Document(page_content='Point :-\\n1. tanh and logistic sigmoid are the most popular activation functions in 90’ s but because of their\\nVanishing gradient problem and sometimes Exploding gradient problem (because of weights),\\nthey aren’t mostly used now .\\n2. These days Relu activation function is widely used. Even though, it sometimes gets into\\nvanishing gradient problem, variants of Relu help solving such cases.\\n3. tanh is preferred to sigmoid for faster convergence BUT again, this might change based on\\ndata. Data will also play an important role in deciding which activation function is best to\\nchoose.', metadata={'source': 'Deep Learning.pdf', 'page': 14}),\n",
       " Document(page_content='Vanishing Gradient Problem\\nVanishing gradient problem is a common problem that we face while training deep neural\\nnetworks.Gradients of neural networks are found during back propagation.\\nGenerally , adding more hidden layers will make the network able to learn more complex\\narbitrary functions, and thus do a better job in predicting future outcomes. This is where Deep\\nLearning is making a big dif ference.\\nNow during back-propagation i.e moving backward in the Network and calculating\\ngradients, it tends to get smaller and smaller as we keep on moving backward in the\\nNetwork . Below is Just a simple demonstration of V anishing Gradient Problem in single\\nlayer .\\nThis Happen because of we use sigmoid and tanh activation function in hidden layer . As\\nsigmoid and tanh deriative 0.25,1 respectively . so by calculating number of hidden layer the\\nderivative becomes 0 so avoid it we use RELU activation function in hidden layer .', metadata={'source': 'Deep Learning.pdf', 'page': 15}),\n",
       " Document(page_content='Exploding gradient Problem\\nWe have discussed about vanishing gradient problem.Now we will get in to exploding gradient\\nproblem.Earlier we discussed what happens when our gradient becomes very small.Now we\\nwill discuss what will happen if it gets large.\\nIn deep networks or recurrent neural networks, error gradients can accumulate during an\\nupdate and result in very large gradients.\\nThese in turn result in large updates to the network weights, and in turn, an unstable\\nnetwork.The explosion occurs through exponential growth by repeatedly multiplying gradients\\nthrough the network layers that have values larger than 1.0.This will ultimately led to an total\\nunstable network.\\n', metadata={'source': 'Deep Learning.pdf', 'page': 16}),\n",
       " Document(page_content='3. Relu\\nBy Using of Sigmoid And T anh function there is vanishing gradient problem occure so the\\nconvergence rate slow down.\\nTo overcome slighlty we use Relu Function. it not totally overcome this problem but here the\\nconvergence rate is faster than sigmoid and tanh.\\nValue Range :- [0, inf)\\nIts Nature non-linear , which means we can easily backpropagate the errors and have multiple\\nlayers of neurons being activated by the ReLU function.\\nPoint:\\n1. Vanishing Gradient Problem  occure due to multiple number of derivative.\\n2. As sigmoid derivative 0 to 0.25 so by multiple by this sigmoid derivative the result might\\nvanishing gradient as number of hidden layer increase.\\n3. But in Relu here its derivative 0 to 1 so here no problem of any vanishing gradient problem as\\nits derivative cant be 0.2,0.3 like.\\n4. But as its derivative can be 0 so here a problem aries that called Dead_Activation\\nUses :-  ReLu is less computationally expensive than tanh and sigmoid because it involves simpler\\nmathematical operations. At a time only a few neurons are activated making the network sparse\\nmaking it ef ficient and easy for computation.\\nWhat is Dead_Activation ?\\nWhen Derivative equal to 0 in Relu Then New W eight = Old W eight :', metadata={'source': 'Deep Learning.pdf', 'page': 17}),\n",
       " Document(page_content='which is not good for any model.Here W eight Cant be update. it occure when value of z is\\nnegative.This state called Dead Activation state. to overcome this we use Leaky Relu.\\nSee here no updation happen the value remain same and ‘nan’ for validation loss is an\\nunexpected very large or very small number . This is dead activation state to overcome this we\\nuse leaky relu\\n4. Leaky Relu\\nLeaky ReLU is an improved version of the ReLU function.\\nReLU function, the gradient is 0 for x < 0(-ve), which made the neurons die for activations in\\nthat region.\\nLeaky ReLU is defined to address this problem. Instead of defining the Relu function as 0 for x\\nless than 0, we define it as a small linear component of x.\\nLeaky ReLUs are one attempt to fix the Dying ReLU problem. Instead of the function being\\nzero when x < 0, a leaky ReLU will instead have a small negative slope (of 0.01, or so). That\\nis, the function computes:\\n', metadata={'source': 'Deep Learning.pdf', 'page': 18}),\n",
       " Document(page_content='Sigmoid,T anh,Relu,Leaky Relu\\n', metadata={'source': 'Deep Learning.pdf', 'page': 19}),\n",
       " Document(page_content='Softmax\\nSoftmax is used as the activation function for multi-class classification tasks, usually the last\\nlayer .\\nWe talked about its role transforming numbers (aka logits) into probabilities that sum to one.\\nLet’s not forget it is also an activation function which means it helps our model achieve non-\\nlinearity . Linear combinations of linear combinations will always be linear but adding activation\\nfunction helps gives our model ability to handle non-linear data.\\nOutput of other activation functions such as sigmoid does not necessarily sum to one. Having\\noutputs summing to one makes softmax function great for probability analysis.\\nThe function is great for classification problems, especially if you’re dealing with multi-class\\nclassification problems, as it will report back the “confidence score” for each class. Since we’re\\ndealing with probabilities here, the scores returned by the softmax function will add up to 1.\\nMathematical representation\\nWhere:\\nσ = softmax\\n=input vector\\n=standard exponential function for input vector\\nK = number of classes in the multi-class classifier\\n=standard exponential function for output vector\\nIt states that we need to apply a standard exponential function to each element of the output\\nlayer , and then normalize these values by dividing by the sum of all the exponentials. Doing so\\nensures the sum of all exponentiated values adds up to 1.\\nHere are the steps For Softmax:\\n1. Exponentiate every element of the output layer and sum the results\\n2. Take each element of the output layer , exponentiate it and divide by the sum obtained in step\\n1\\nExample Implementation', metadata={'source': 'Deep Learning.pdf', 'page': 20}),\n",
       " Document(page_content='To start, let’ s declare an array which imitates the output layer of a neural network:\\nIn [1]:\\nBy step 1 we need to exponentiate each of the elements of the output\\nlayer:\\nIn [2]:\\nAccording to step 2 calculate probabilities! W e can use Numpy to\\ndivide each element by exponentiated sum and store results in another\\narray\\nIn [3]:\\nHere see the output are formed. If we sum three then we get probability 1. after this you use\\nargmax function which return highest value index number . See here return 0 as 0 index\\nhave 0.65 value which is highest among three value.\\nWhen you use softmax in your dataset you should use argmax function to predict\\noutput.\\nFrom a probabilistic perspective, if the argmax() function returns 1 in the large value, it returns\\n0fortheother twoarray indexes,here itgiving fullweight toindex 0andnoweight toindex 1Out[1]: array([2. , 1. , 0.1])\\nOut[2]: array([7.3890561 , 2.71828183, 1.10517092])\\n[0.65900114 0.24243297 0.09856589]  \\n1.0 \\n0 ## as we know softmax used in output layer so here i take a outputlayer value\\nimport numpy as np\\noutput_layer  = np.array([2.0,1.0,0.1])\\noutput_layer\\nexponentiated  = np.exp(output_layer )\\nexponentiated\\nprobabilities  = exponentiated  / np.sum(exponentiated )\\nprint(probabilities )\\nprint(sum(probabilities ))\\nprint(np.argmax(probabilities ))', metadata={'source': 'Deep Learning.pdf', 'page': 21}),\n",
       " Document(page_content='0 for the other two array indexes,here it giving full weight to index 0 and no weight to index 1\\nand index 2 for the largest value in the list [0.65,0.24,0.09].\\nIn the Keras deep learning library with a three-class classification task, use of softmax in\\nthe output layer may look as follows:\\n \\nmodel.add(Dense(no.of output layer , activation=\\'softmax\\'))\\n- It apply when you have multiclass problem aries\\nThe Differences between Sigmoid and Softmax Activation\\nFunctions\\nIn [4]:\\nIn [5]:\\nThe key takeaway from this example is:--- Sigmoid---  \\n[0.38 0.77 0.48 0.92]  \\n2.54 \\n**************************************************  \\n---Softmax---  \\n[0.04 0.21 0.06 0.7 ]  \\n1.0 import numpy as np \\n### sigmoid function\\ndef sigmoid(x):\\n    s = 1 / (1 + np.exp(-x))\\n    return s\\n \\n## softmax function\\ndef softmax(x):\\n    exponentiated  = np.exp(x)\\n    probabilities  = exponentiated  / np.sum(exponentiated )\\n    return probabilities\\nx = np.array([-0.5, 1.2, -0.1, 2.4])\\na = sigmoid(x)\\nprint(\"--- Sigmoid---\" )\\nprint(a.round(2))\\nprint(sum(a).round(2))\\n \\nprint(50*\"*\")\\n \\noutput_layer  = np.array([-0.5, 1.2, -0.1, 2.4])\\nb = softmax(output_layer )\\nprint(\"---Softmax---\" )\\nprint(b.round(2))\\nprint(sum(b))', metadata={'source': 'Deep Learning.pdf', 'page': 22}),\n",
       " Document(page_content='Sigmoid:  probabilities produced by a Sigmoid are independent. Furthermore, they are not\\nconstrained to sum to one: 0.38 + 0.77 + 0.48 + 0.92 = 2.54. The reason for this is because\\nthe Sigmoid looks at each raw output value separately .\\nSoftmax:  the outputs are interrelated. The Softmax probabilities will always sum to one by\\ndesign: 0.04 + 0.21 + 0.06 + 0.7 = 1.00. In this case, if we want to increase the likelihood of\\none class, the other has to decrease by an equal amount.\\nSummary..\\nCharacteristics of a Sigmoid Activation Function:\\n1. Used for Binary Classification in the Logistic Regression model\\n2. The probabilities sum does not need to be 1\\n3. Used as an Activation Function while building a Neural Network\\nCharacteristics of a Softmax Activation Function\\n1. Used for Multi-classification in the Logistics Regression model\\n2. The probabilities sum will be 1\\n3. Used in the dif ferent layers of Neural Networks\\nActivation Function For Regression Problem\\nLinear Activation Function:-\\nEquation : f(x) = x \\nRange : (-infinity to infinity)\\nNo matter how many layers we have, if all are linear in nature, the final activation function of\\nlast layer is nothing but just a linear function of the input of first layer .\\nLinear activation function is used at just one place i.e. output layer .\\nIf we will dif ferentiate linear function to bring non-linearity , result will no more depend on input\\n“x” and function will become constant, it won’t introduce any ground-breaking behavior to our\\nalgorithm.', metadata={'source': 'Deep Learning.pdf', 'page': 23}),\n",
       " Document(page_content='algorithm.\\nUses:  Calculation of price of a house is a regression problem. House price may have any big/small\\nvalue, so we can apply linear activation at output layer . Even in this case neural net must have any\\nnon-linear function at hidden layers.', metadata={'source': 'Deep Learning.pdf', 'page': 24}),\n",
       " Document(page_content='Loss Functions\\nThe loss function is the function that computes the distance between the current output of the\\nalgorithm and the expected output. It’ s a method to evaluate how your algorithm models the\\ndata. It can be categorized into two groups. One for classification (discrete values, 0,1,2…)\\nand the other for regression (continuous values).\\nTYPES OF LOSS FUNCTION:\\n1. Regression Loss Functions\\n    Mean Absolute Error  \\n    Mean Squared Error  \\n    Root Mean Square error (RMSE)\\n2. Binary Classification Loss Functions\\n    Binary Cross-Entropy\\n3. Multi-Class Classification Loss Functions\\n    Multi-Class Cross-Entropy Loss  \\n    Sparse Multiclass Cross-Entropy Loss\\nRegression Losses\\nWe know all of this regression loss function but here i discuss a brief\\nMean Absolute Error\\nRegression metric which measures the average magnitude of errors in a group of predictions,\\nwithout considering their directions. In other words, it’ s a mean of absolute dif ferences among\\npredictions and expected results where all individual deviations have even importance.\\nwhere:\\ni — index of sample,', metadata={'source': 'Deep Learning.pdf', 'page': 25}),\n",
       " Document(page_content='ŷ — predicted value,\\ny — expected value,\\nm — number of samples in dataset.\\nSometimes it is possible to see the form of formula with swapped predicted value and expected\\nvalue, but it works the same.\\nMean Squared Error\\nOne of the most commonly used and firstly explained regression metrics. A verage squared\\ndifference between the predictions and expected results. In other words, an alteration of MAE\\nwhere instead of taking the absolute value of dif ferences, they are squared.\\nIn MAE, the partial error values were equal to the distances between points in the coordinate\\nsystem. Regarding MSE, each partial error is equivalent to the area of the square created out\\nof the geometrical distance between the measured points. All region areas are summed up\\nand averaged.\\nWhere\\ni — index of sample,\\nŷ — predicted value,\\ny — expected value,\\nm — number of samples in dataset.\\nRoot Mean Square error (RMSE)\\nRoot Mean Square error is the extension of MSE — measured as the average of square root\\nof sum of squared dif ferences between predictions and actual observations.\\n', metadata={'source': 'Deep Learning.pdf', 'page': 26}),\n",
       " Document(page_content='Classification Losses\\nBinary Classification Loss Functions\\nBinary Cross Entropy\\nAlso called Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss.\\nUnlike Softmax loss it is independent for each vector component (class), meaning that the loss\\ncomputed for every CNN output vector component is not af fected by other component values.\\nBinary cross entropy measures how far away from the true value (which is either 0 or 1) the\\nprediction is for each of the classes and then averages these class-wise errors to obtain the\\nfinal loss.\\nWe can define cross entropy as the dif ference between two probability distributions p and q,\\nwhere p is our true output and q is our estimate of this true output.\\nit Only use for binary classification problem\\nMulti-Class Classification Loss Functions\\nCategorical cross-entropy\\nUsed binary and multiclass problem, the label needs to be encoded as categorical, one-hot\\nencoding representation (for 3 classes: [0, 1, 0], [1,0,0]…)\\nIt is a loss function that is used for single label categorization. This is when only one category\\nis applicable for each data point. In other words, an example can belong to one class only .\\nUse categorical crossentropy in classification problems where only one result can be correct.\\nExample:  In the  MNIST   problem where you have images of the numbers 0,1, 2, 3, 4, 5, 6, 7, 8,\\nand 9. Categorical crossentropy gives the probability that an image of a number is, for\\nexample, a 4 or a 9.', metadata={'source': 'Deep Learning.pdf', 'page': 27}),\n",
       " Document(page_content='Categorical cross-entropy will compare the distribution of the predictions (the activations in the\\noutput layer , one for each class) with the true distribution, where the probability of the true\\nclass is set to 1 and 0 for the other classes. T o put it in a dif ferent way , the true class is\\nrepresented as a one-hot encoded vector , and the closer the model’ s outputs are to that\\nvector , the lower the loss.\\nSparse Categorical cross-entropy\\nUsed binary and multiclass problem (the label is an integer — 0 or 1 or … n, depends on the\\nnumber of labels)\\nAll Losses : https://keras.io/api/losses/  (https://keras.io/api/losses/)\\nSummary\\nThere are three kinds of classification tasks:\\n   1. Binary classification: two exclusive classes  \\n   2. Multi-class classification: more than two exclusive classes  \\n   3. Multi-label classification: just non-exclusive classes\\nHere, we can say\\n   1. In the case of (1), you need to use binary cross entropy.  \\n   2. In the case of (2), you need to use categorical cross entropy.  \\n   3.In the case of (3), you need to use binary cross entropy.', metadata={'source': 'Deep Learning.pdf', 'page': 28}),\n",
       " Document(page_content='Which Loss and Activation Functions should I use?\\nThe motive of the blog is to give you some ideas on the usage of “Activation Function” & “Loss\\nfunction” in dif ferent scenarios.\\nChoosing an activation function and loss function is directly dependent upon the output you\\nwant to predict. There are dif ferent cases and dif ferent outputs of a predictive model. Before I\\nintroduce you to such cases let see an introduction to the activation function and loss function.\\nThe activation function activates the neuron that is required for the desired output, converts\\nlinear input to non-linear output. If you are not aware of the dif ferent activation functions I\\nwould recommend you visit my activation pdf to get an in-depth explanation of dif ferent\\nactivation functions click here :\\nhttps://github.com/pratyusa98/ML_Algo_pdf/tree/main/01_Deep_Learning_PDF\\n(https://github.com/pratyusa98/ML_Algo_pdf/tree/main/01_Deep_Learning_PDF) .\\nLoss function helps you figure out the performance of your model in prediction, how good the\\nmodel is able to generalize. It computes the error for every training. Y ou can read more about\\nloss functions and how to reduce the loss\\nhttps://github.com/pratyusa98/ML_Algo_pdf/tree/main/01_Deep_Learning_PDF\\n(https://github.com/pratyusa98/ML_Algo_pdf/tree/main/01_Deep_Learning_PDF) ..\\nLet’s see the different cases:\\nCASE 1: When the output is a numerical value that you are\\ntrying to predict\\nEx:- Consider predicting the prices of houses provided with dif ferent features of the house. A\\nneural network structure where the final layer or the output later will consist of only one neuron\\nthat reverts the numerical value. For computing the accuracy score the predicted values are\\ncompared to true numeric values.\\nActivation Function to be used in Output layer such cases,', metadata={'source': 'Deep Learning.pdf', 'page': 29}),\n",
       " Document(page_content='         * Linear Activation - it gives output in a numeric form th\\nat is the demand for this case. Or  \\n         * ReLU Activation - This activation function gives you pos\\nitive numeric outputs as a result.  \\nLoss function to be used in such cases,\\n         * Mean Squared Error (MSE) - This loss function is respons\\nible to compute the average squared difference    between the true v\\nalues and the predicted values.\\nCASE 2: When the output you are trying to predict is\\nBinary\\nEx:- Consider a case where the aim is to predict whether a loan applicant will default or not. In\\nthese types of cases, the output layer consists of only one neuron that is responsible to result\\nin a value that is between 0 and 1 that can be also called probabilistic scores.\\nFor computing the accuracy of the prediction, it is again compared with the true labels. The\\ntrue value is 1 if the data belongs to that class or else it is 0.\\nActivation Function to be used in Output layer such cases,\\n            * Sigmoid Activation -  This activation function gives\\nthe output as 0 and 1.\\nLoss function to be used in such cases,\\n             * Binary Cross Entropy - The difference between the tw\\no probability distributions is given by binary  cross-entropy. (p,1-\\np) is the model distribution predicted by the model, to compare it w\\nith true distribution, the   binary cross-entropy is used.', metadata={'source': 'Deep Learning.pdf', 'page': 30}),\n",
       " Document(page_content='CASE 3: Predicting a single class from many classes\\nEx:- Consider a case where you are predicting the name of the fruit amongst 5 dif ferent fruits.\\nIn the case, the output layer will consist of only one neuron for every class and it will revert a\\nvalue between 0 and 1, the output is the probability distribution that results in 1 when all are\\nadded.\\nEach output is checked with its respective true value to get the accuracy . These values are\\none-hot-encoded which means if will be 1 for the correct class or else for others it would be\\nzero.\\nActivation Function to be used in Output layer such cases,\\n             * Softmax Activation -  This activation function gives  \\nthe output between 0 and 1 that are the probability scores which if\\nadded gives the result as 1.  \\nLoss function to be used in such cases,\\n             * Cross-Entropy - It computes the difference between t\\nwo probability distributions.  \\n             * (p1,p2,p3) is the model distribution that is predict\\ned by the model where p1+p2+p3=1. This is compared with the true dis\\ntribution using cross-entropy.\\nCASE 4: Predicting multiple labels from multiple\\nclass\\nEx:- Consider the case of predicting dif ferent objects in an image having multiple objects. This\\nis termed as multiclass classification. In these types of cases, the output layer consists of only\\none neuron that is responsible to result in a value that is between 0 and 1 that can be also', metadata={'source': 'Deep Learning.pdf', 'page': 31}),\n",
       " Document(page_content='called probabilistic scores.\\nFor computing the accuracy of the prediction, it is again compared with the true labels. The\\ntrue value is 1 if the data belongs to that class or else it is 0.\\nActivation Function to be used in Output layer such cases,\\n               * Sigmoid Activation -  This activation function giv\\nes the output as 0 and 1.\\nLoss function to be used in such cases,\\n               * Binary Cross Entropy - The difference between the\\ntwo probability distributions is given by binary cross-entropy. (p,\\n1-p) is the model distribution predicted by the model, to compare it  \\nwith true distribution, the binary cross-entropy is used.\\nAll Losses : https://keras.io/api/losses/  (https://keras.io/api/losses/)   \\nAll Activation : https://keras.io/api/layers/activations/  (https://keras.io/api/layers/activations/)\\nSummary\\nThis activation use only output layer and in hidden layer you can use Relu or Leaky Relu.\\nThe following table summarizes the above information to allow you to quickly find the final\\nlayer activation function and loss function that is appropriate to your use-case', metadata={'source': 'Deep Learning.pdf', 'page': 32}),\n",
       " Document(page_content='', metadata={'source': 'Deep Learning.pdf', 'page': 33}),\n",
       " Document(page_content='Weight Initialization\\nThe weight initialization technique you choose for your neural network can determine how\\nquickly the network converges or whether it converges at all. Although the initial values of\\nthese weights are just one parameter among many to tune, they are incredibly important.\\nTheir distribution af fects the gradients and, therefore, the ef fectiveness of training.\\nWhy is weight initialization important?¶\\nImproperly initialized weights can negatively af fect the training process by contributing to the\\nvanishing or exploding gradient problem.\\nWith the vanishing gradient problem, the weight update is minor and results in slower\\nconvergence — this makes the optimization of the loss function slow and in a worst case\\nscenario, may stop the network from converging altogether .\\nConversely , initializing with weights that are too large may result in exploding gradient values\\nduring forward propagation or back-propagation.\\n1. Zero initialization :\\nIf all the weights are initialized with 0, the derivative with respect to loss function is the same\\nfor every weight(w), thus all weights have the same value in subsequent iterations.\\nThis makes hidden units symmetric and continues for all the n iterations i.e. setting weights to\\n0 does not make it better than a linear model.\\nAn important thing to keep in mind is that biases have no ef fect what so ever when initialized\\nwith 0.\\nIt also gives problems like vanishing gradient problem.\\n2. initialization W ith -ve Number :\\nIf all weigth can be negative then it af fect Relu Activation Function.As in -ve Relu comes under\\ndead activation problem. so we cant use this technique.\\nWeights can’t be too high as gives problems like exploding Gradient problem(weights of the\\nmodel explode to infinity), which means that a large space is made available to search for\\nglobal minima hence convergence becomes slow .\\nTo prevent the gradients of the network’ s activations from vanishing or exploding, we need\\nto have following rules:\\n       1. The mean of the activations should be zero.  \\n       2. The variance of the activations should stay the same across e\\nvery layer.\\nIdea 1 : Normal or Naïve Initialization:', metadata={'source': 'Deep Learning.pdf', 'page': 34}),\n",
       " Document(page_content=\"In normal distribution weights can be a part of normal or gaussian distribution with mean as\\nzero and a unit standard deviation.\\nRandom initialization is done so that convergence is not to a false minima.\\nIn Keras it can be simply written as hyperparameter as - kernel_initializer='random_normal'\\nIdea 2: Uniform Initialization:\\nIn uniform initialization of weights , weights belong to a uniform distribution in range a,b with\\nvalues of a and b as below:\\nWhenever sigmoid activation function is used as , Uniform works well.\\nIn Keras it can be simply written as hyperparameter as - kernel_initializer='random_uniform'\\nIdea 3: Xavier/ Glorot Weight Initialization:\\nThe variance of weights in the case normal distribution was not taken care of which resulted in\\ntoo large or too small activation values which again led to exploding gradient and vanishing\\ngradient problems respectively , when back propagation was done.\\nIn order to overcome this problem Xavier Initialization was introduced. It keeps the variance\\nthe same across every layer . We will assume that our layer ’s activations are normally\\ndistributed around zero.\\nGlorot or Xavier had a belief that if they maintain variance of activations in all the layers going\\nforward and backward convergence will be fast as compared to using standard initialization\\nwhere gap was larger .\\nIt have T wo V arient\\n     a. Normal Distribution - kernel_initializer='glorot_normal'  \\n     b. Uniform Distribution - kernel_initializer='glorot_uniform'\\nPoint :  Works well with tanh , sigmoid activation functions.\\na. Normal Distribution:\\nIn Normal Distribution, weights belong to normal distribution where mean is zero and standard\\ndeviation is as below:\", metadata={'source': 'Deep Learning.pdf', 'page': 35}),\n",
       " Document(page_content=\"b. Uniform Distribution:\\nUniform Distribution , weights belong to uniform distribution in range of a and b defined as\\nbelow:\\nIdea 4: He-Initialization:\\nWhen using activation functions that were zero centered and have output range between-1,1\\nfor activation functions like tanh and softsign, activation outputs were having mean of 0 and\\nstandard deviation around 1 average wise.\\nBut if ReLu is used instead of tanh, it was observed that on average it has standard deviation\\nvery close to square root of 2 divided by input connections.\\nIt have T wo V arient\\n   a. Normal Distribution - kernel_initializer='he_normal'  \\n   b. Uniform Distribution - kernel_initializer='he_uniform'\\nPoint :  Works well with Relu And Leaky Relu activation functions.\\na. Normal Distribution:\\nIn He-Normal initialization method, weights belong to normal distribution where mean is zero\\nand standard deviation is as below:\", metadata={'source': 'Deep Learning.pdf', 'page': 36}),\n",
       " Document(page_content='b. Uniform Initialization :\\nIn He Uniform Initialization weights belong to uniform distribution in range as shown below:\\n', metadata={'source': 'Deep Learning.pdf', 'page': 37}),\n",
       " Document(page_content='Optimization Techniques\\nOptimization algorithms are responsible for reducing losses and provide most accurate results\\npossible.\\nThe weight is initialized using some initialization strategies and is updated with each epoch\\naccording to the equation. The best results are achieved using some optimization strategies or\\nalgorithms called Optimizer .\\nSome of the techniques that we will be discussing in this article is-\\n         * Gradient Descent  \\n         * Stochastic Gradient Descent (SGD)  \\n         * Mini-Batch Stochastic Gradient Descent (MB — SGD)  \\n         * SGD with Momentum  \\n         * Nesterov Accelerated Gradient (NAG)  \\n         * Adaptive Gradient (AdaGrad)  \\n         * AdaDelta  \\n         * RMSProp  \\n         * Adam\\n1. Gradient Descent or Batch Gradient Descent\\nA Gradient Descent is an iterative algorithm, that starts from a random point on the function\\nand traverses down its slope in steps until it reaches lowest point (global minima) of that\\nfunction.\\nThis algorithm is apt for cases where optimal points cannot be found by equating the slope of\\nthe function to 0. For the function to reach minimum value, the weights should be altered.\\nWith the help of back propagation, loss is transferred from one layer to another and “weights”\\nparameter are also modified depending on loss so that loss can be minimized.', metadata={'source': 'Deep Learning.pdf', 'page': 38}),\n",
       " Document(page_content='Point :\\n1. Use all training Sample for a forward pass and adjust the weights.\\n2. This makes it computationally intensive. 3. Another drawback is there are chances the iteration\\nvalues may get stuck at local minima or saddle point and never converge to minima. T o obtain the\\nbest solution, the must reach global minima. 4. Good For Small training data.\\nCost function: θ=θ−α ⋅∇ J(θ)\\nAdvantages:\\nEasy computation.\\nEasy to implement.\\nEasy to understand.\\nDisadvantages:\\nMay trap at local minima.\\nWeights are changed after calculating gradient on the whole dataset. So, if the dataset is too\\nlarge than this may take years to converge to the minima.\\nRequires large memory to calculate gradient on the whole dataset.\\n2. Stochastic Gradient Descent\\nStochastic Gradient Descent is an extension of Gradient Descent, where it overcomes some\\nof the disadvantages of Gradient Descent algorithm.\\nSGD tries to overcome the disadvantage of computationally intensive by computing the\\nderivative of one point at a time.\\nDue to this fact, SGD takes more number of iterations compared to GD to reach minimum and\\nalso contains some noise when compared to Gradient Descent.\\nAs SGD computes derivatives of only 1 point at a time, the time taken to complete one epoch\\nis large compared to Gradient Descent algorithm.\\nPoint :\\n1. Use One (Randomly Picked) Sample for a forward pass and adjust the weights.\\n2. Good when training set is very big and we dont want too much computation.\\ncost function θ=θ−α ⋅∇ J(θ;x(i);y(i)) , where {x(i) ,y(i)} are the training examples.\\nAdvantages:\\nFrequent updates of model parameters hence, converges in less time.\\nRequires less memory as no need to store values of loss functions.\\nMay get new minima’ s.\\nDisadvantages:\\nHigh variance(noisey) in model parameters.', metadata={'source': 'Deep Learning.pdf', 'page': 39}),\n",
       " Document(page_content='May shoot even after achieving global minima.\\nTo get the same convergence as gradient descent needs to slowly reduce the value of\\nlearning rate.\\n3. Mini Batch — Stochastic Gradient Descent\\nMB-SGD is an extension of SGD algorithm. It overcomes the time-consuming complexity of\\nSGD by taking a batch of points / subset of points from dataset to compute derivative.\\nIt’s best among all the variations of gradient descent algorithms. It is an improvement on both\\nSGD and standard gradient descent. It updates the model parameters after every batch. So,\\nthe dataset is divided into various batches and after every batch, the parameters are updated.\\nThis is a mixture of both stochastic and batch gradient descent.\\nThe training set is divided into multiple groups called batches. Each batch has a number of\\ntraining samples in it.\\nAt a time a single batch is passed through the network which computes the loss of every\\nsample in the batch and uses their average to update the parameters of the neural network.\\nFor example, say the training set has 100 training examples which is divided into 5 batches\\nwith each batch containing 20 training examples. This means that the equation in figure2 will\\nbe iterated over 5 times (number of batches).\\nPoint:\\n1. Use a Batch Of (Randomly Picked) Sample for a forward pass and adjust the weights.\\n2. It is observed that the derivative of loss function of MB-SGD is similar to the loss function of GD\\nafter some iterations. But the number iterations to achieve minima in MB-SGD is large compared\\nto GD and is computationally expensive. The update of weights in much noisier because the\\nderivative is not always towards minima.\\nθ=θ−α ⋅∇ J(θ; B(i)), where {B(i)} are the batches of training examples.\\nAdvantages:\\nFrequently updates the model parameters and also has less variance.\\nRequires medium amount of memory .\\nEasily fits in the memory\\nIt is computationally ef ficient\\nBenefit from vectorization\\nIf stuck in local minimums, some noisy steps can lead the way out of them\\nAverage of the training samples produces stable error gradients and convergence\\n!!!! This ensures the following advantages of both stochastic and batch gradient descent are used\\ndue to which Mini Batch Gradient Descent is most commonly used in practice.\\nSee How in this above three convergence Occure towards minima point', metadata={'source': 'Deep Learning.pdf', 'page': 40}),\n",
       " Document(page_content='Here W e see in SGD Due to frequent updates the steps taken towards the minima are very noisy .\\nThis can often lead the gradient descent into other directions. Also, due to noisy steps it may take\\nlonger to achieve convergence to the minima of the loss function. to reduce this we can use SGD\\nwith Momentum.\\n4. SGD with Momentum\\nMomentum was invented for reducing high variance in SGD and softens the convergence.\\nIt accelerates the convergence towards the relevant direction and reduces the fluctuation to\\nthe irrelevant direction. One more hyperparameter is used in this method known as\\nmomentum symbolized by ‘γ’(gamma).\\nIt is an adaptive optimization algorithm which exponentially uses weighted average gradients\\nover previous iterations to stabilize the convergence, resulting in quicker optimization.\\nThis is done by adding a fraction (gamma) to the previous iteration values.\\nEssentially the momentum term increase when the gradient points are in the same directions\\nand reduce when gradients fluctuate. As a result, the value of loss function converges faster\\nthan expected.\\nAdvantages:\\nReduces the oscillations and high variance of the parameters.\\nConverges faster than gradient descent.\\nDisadvantages:\\nOne more hyper-parameter is added which needs to be selected manually and accurately .', metadata={'source': 'Deep Learning.pdf', 'page': 41}),\n",
       " Document(page_content='5. Nesterov accelerated gradient(NAG)\\nMomentum may be a good method but if the momentum is too high the algorithm may miss\\nthe local minima and may continue to rise up. So, to resolve this issue the NAG algorithm was\\ndeveloped.\\nNesterov accelerated gradient (NAG) is a way to give momentum more precision.\\nThe idea of the NAG algorithm is very similar to SGD with momentum with a slight variant. In\\nthe case of SGD with momentum algorithm, the momentum and gradient are computed on\\nprevious updated weight.\\nBoth NAG and SGD with momentum algorithms work equally well and share the same\\nadvantages and disadvantages.\\n', metadata={'source': 'Deep Learning.pdf', 'page': 42}),\n",
       " Document(page_content='figure (a) :\\nIn figure (a), update 1 is positive i.e., the gradient is negative because as w_0 increases L\\ndecreases. Even update 2 is positive as well and you can see that the update is slightly larger\\nthan update 1 because of momentum.\\nBy now , you should be convinced that update 3 will be bigger than both update 1 and 2 simply\\nbecause of momentum and the positive update history .\\nUpdate 4 is where things get interesting. In SGD with Momentum case, due to the positive\\nhistory , the update overshoots and the descent recovers by doing negative updates.\\nfigure (b) :\\nBut in NAG’ s case, every update happens in two steps — first, a partial update, where we get\\nto the look_ahead point and then the final update (see the NAG update rule), see figure (b).\\nFirst 3 updates of NAG are pretty similar to the momentum-based method as both the updates\\n(partial and final) are positive in those cases. But the real dif ference becomes apparent during\\nupdate 4.\\nAs usual, each update happens in two stages, the partial update (4a) is positive, but the final\\nupdate (4b) would be negative as the calculated gradient at w_lookahead would be negative\\n(convince yourself by observing the graph).\\nThis negative final update slightly reduces the overall magnitude of the update, still resulting in\\nan overshoot but a smaller one when compared to the vanilla momentum-based gradient\\ndescent. And that my friend, is how NAG helps us in reducing the overshoots, i.e. making us\\ntake shorter U-turns.\\nAdvantages:\\nDoes not miss the local minima.\\nSlows if minima’ s are occurring.\\nDisadvantages:', metadata={'source': 'Deep Learning.pdf', 'page': 43}),\n",
       " Document(page_content='Still, the hyperparameter needs to be selected manually .\\nPoint :\\nBy using NAG technique, we are now able to adapt error function with the help of previous\\nand future values and thus eventually speed up the convergence. Now , in the next techniques\\nwe will try to adapt alter or vary the individual parameters depending on the importance factor\\nit plays in each case.', metadata={'source': 'Deep Learning.pdf', 'page': 44}),\n",
       " Document(page_content='6. Adaptive Gradient (AdaGrad)\\nAdaptive Gradient as the name suggests adopts the learning rate of parameters by updating it\\nat each iteration depending on the position it is present, i.e- by adapting slower learning rates\\nwhen features are occurring frequently and adapting higher learning rate when features are\\ninfrequent.\\nThe motivation behind Adagrad is to have dif ferent learning rates for each neuron of each\\nhidden layer for each iteration.\\nBut why do we need dif ferent learning rates?\\nData sets have two types of features:\\nDense features, e.g. House Price Data set (Large number of non-zero valued features), where\\nwe should perform smaller updates on such features; and\\nSparse Features, e.g. Bag of words (Large number of zero valued features), where we should\\nperform larger updates on such features.\\nIt has been found that Adagrad greatly improved the robustness of SGD, and is used for training\\nlarge-scale neural nets at Google.\\nη : initial Learning rate\\nϵ : smoothing term that avoids division by zero\\nw: W eight of parameters\\nIn SGD learning Rate same for all weight but in Adagrad this is dif ferent for all.\\nAdvantage:\\nNo need to update the learning rate manually as it changes adaptively with iterations.\\nIf we have some Sparse and Dense feature it automatically takes out what learning rate is\\nsuitable.\\nDisadvantage:', metadata={'source': 'Deep Learning.pdf', 'page': 45}),\n",
       " Document(page_content='As the number of iteration becomes very large learning rate decreases to a very small number\\nwhich leads to slow convergence.\\nComputationally expensive as a need to calculate the second order derivative.\\nAdadelta, RMSProp, and adam tries to resolve Adagrad’ s radically diminishing learning rates.\\n7. AdaDelta\\nIt is simply an extension of AdaGrad that seeks to reduce its monotonically decreasing\\nlearning rate.\\nInstead of summing all the past gradients, AdaDelta restricts the no. of summation values to a\\nlimit (w).\\nIn AdaDelta, the sum of past gradients (w) is defined as “Decaying A verage of all past\\nsquared gradients”. The current average at the iteration then depends only on the previous\\naverage and current gradient.\\nInstead of inef ficiently storing all previous squared gradients, we recursively define a decaying\\naverage of all past squared gradients. The running average at each time step then depends\\n(as a fraction γ , similarly to the Momentum term) only on the previous average and the\\ncurrent gradient.\\nAdvantages:\\nNow the learning rate does not decay and the training does not stop.\\nDisadvantages:\\nComputationally expensive.', metadata={'source': 'Deep Learning.pdf', 'page': 46}),\n",
       " Document(page_content='8. RMSProp\\nRMSProp is Root Mean Square Propagation. It was devised by Geof frey Hinton.\\nRMSProp tries to resolve Adagrad’ s radically diminishing learning rates by using a moving\\naverage of the squared gradient. It utilizes the magnitude of the recent gradient descents to\\nnormalize the gradient.\\nIn RMSProp learning rate gets adjusted automatically and it chooses a dif ferent learning rate\\nfor each parameter .\\nRMSProp divides the learning rate by the average of the exponential decay of squared\\ngradients\\nIts cost function same as Adadelta\\n9. Adam — Adaptive Moment Estimation\\nIt is a combination of RMSProp and Momentum.\\nThis method computes adaptive learning rate for each parameter .\\nIn addition to storing the previous decaying average of squared gradients, it also holds the\\naverage of past gradient similar to Momentum. Thus, Adam behaves like a heavy ball with\\nfriction which prefers flat minima in error surface.\\nAnother method that calculates the individual adaptive learning rate for each parameter from\\nestimates of first (Momentum) and second (RMSProp) moments of the gradients.\\n', metadata={'source': 'Deep Learning.pdf', 'page': 47}),\n",
       " Document(page_content='Advantages:\\nThe method is too fast and converges rapidly .\\nRectifies vanishing learning rate, high variance.\\nDisadvantages:\\nComputationally costly .', metadata={'source': 'Deep Learning.pdf', 'page': 48}),\n",
       " Document(page_content='Regularization Techniques\\nOne of the most common problem data science professionals face is to avoid overfitting. Have\\nyou come across a situation where your model performed exceptionally well on train data, but\\nwas not able to predict test data.\\nHave you seen this image before? As we move towards the right in this image, our model tries\\nto learn too well the details and the noise from the training data, which ultimately results in\\npoor performance on the unseen data.\\nIn other words, while going towards the right, the complexity of the model increases such that\\nthe training error reduces but the testing error doesn’t. This is shown in the image below .\\nIf you’ve built a neural network before, you know how complex they are. This makes them\\nmore prone to overfitting. Regularization is a technique which makes slight modifications to\\nthe learning algorithm such that the model generalizes better . This in turn improves the\\nmodel’ s performance on the unseen data as well.', metadata={'source': 'Deep Learning.pdf', 'page': 49}),\n",
       " Document(page_content='p\\nDifferent Regularization Techniques in Deep Learning\\n1. L1 & L2 regularization\\nL1 and L2 are the most common types of regularization. These update the general cost\\nfunction by adding another term known as the regularization term.\\n                     Cost function = Loss (say, binary cross entrop\\ny) + Regularization term\\nDue to the addition of this regularization term, the values of weight matrices decrease\\nbecause it assumes that a neural network with smaller weight matrices leads to simpler\\nmodels. Therefore, it will also reduce overfitting to quite an extent.\\nNote: Here the value 0.01 is the value of regularization parameter , i.e., lambda, which we\\nneed to optimize further . We can optimize it using the hyper parameter tuning\\nmethod.\\n2. Dropout\\nThis is the one of the most interesting types of regularization techniques. It also produces very\\ngood results and is consequently the most frequently used regularization technique in the field\\nof deep learning.\\nTo understand dropout, let’ s say our neural network structure is akin to the one shown below:## Below is the sample code to apply L2 regularization to a Dense layer.\\n \\nfrom keras import regularizers\\nmodel.add(Dense(64, input_dim=64,\\n                kernel_regularizer=regularizers.l2(0.01)\\n                \\n \\n## Below is the sample code to apply L1 regularization to a Dense layer.\\n \\nfrom keras import regularizers\\nmodel.add(Dense(64, input_dim=64,\\n                kernel_regularizer=regularizers.l1(0.01)', metadata={'source': 'Deep Learning.pdf', 'page': 50}),\n",
       " Document(page_content=\"So what does dropout do? At every iteration, it randomly selects some nodes and removes\\nthem along with all of their incoming and outgoing connections as shown below . So each\\niteration has a dif ferent set of nodes and this results in a dif ferent set of outputs. It can also be\\nthought of as an ensemble technique in machine learning.\\nPoint:-\\ndropout is usually preferred when we have a large neural network structure in order to\\nintroduce more randomness.\\nNote : As you can see, we have defined 0.25 as the probability of dropping. W e can tune it\\nfurther for better results using the Hyper parameter tuning method.\\n3. Data Augmentation (For Image Data)\\nThe simplest way to reduce overfitting is to increase the size of the training data. In machine\\nlearning, we were not able to increase the size of training data as the labeled data was too\\ncostly .\\nBut, now let’ s consider we are dealing with images. In this case, there are a few ways of\\nincreasing the size of the training data – rotating the image, flipping, scaling, shifting, etc. In\\nthe below image, some transformation has been done on the handwritten digits dataset.## In keras, we can implement dropout using the keras core layer. Below is the  \\npython code for it:\\n \\nfrom keras.layers.core import Dropout\\n \\nmodel = Sequential([\\n Dense(output_dim=hidden1_num_units, input_dim=input_num_units,  \\nactivation='relu'),\\n Dropout(0.25),\\n \\nDense(output_dim=output_num_units, input_dim=hidden5_num_units,  \\nactivation='softmax'),\\n ])\", metadata={'source': 'Deep Learning.pdf', 'page': 51}),\n",
       " Document(page_content='This technique is known as data augmentation. This usually provides a big leap in improving\\nthe accuracy of the model. It can be considered as a mandatory trick in order to improve our\\npredictions.\\nIn keras, we can perform all of these transformations using ImageDataGenerator . It has a big\\nlist of arguments which you you can use to pre-process your training data.\\n4. Batch Normalization\\nBatch normalization is a technique for improving the speed, performance, and stability of\\nartificial neural networks, also known as batch norm. The idea is to normalize the inputs of\\neach layer in such a way that, they have a mean activation output zero and a unit standard\\ndeviation.\\nThe reason for the ‘batch’ in the term Batch Normalization is because neural networks are\\nusually trained with a collated set of data at a time, this set or group of data is referred to as a\\nbatch. The operation within the BN technique occurs to an entire batch of input values as\\nopposed to a single input value.\\nWhy should we normalize the input?\\nLet say we have 2D data, X1, and X2. X1 feature has a very wider spread between 200 to\\n-200 whereas the X2 feature has a very narrow spread. The left graph shows the variance of\\nthe data which has dif ferent ranges. The right graph shows data lies between -2 to 2 and it’ s\\nnormally distributed with 0 mean and unit variance.## Below is the sample code to implement it.\\nfrom keras.preprocessing.image import ImageDataGenerator\\ndatagen = ImageDataGenerator(horizontal flip=True)\\ndatagen.fit(train)', metadata={'source': 'Deep Learning.pdf', 'page': 52}),\n",
       " Document(page_content='Essentially , scaling the inputs through normalization gives the error surface a more spherical\\nshape, where it would otherwise be a very high curvature ellipse. Having an error surface with\\nhigh curvature will mean that we take many steps that aren’t necessarily in the optimal\\ndirection.\\nWhen we scale the inputs, we reduce the curvature, which makes methods that ignore\\ncurvature like gradient descent work much better . When the error surface is circular or\\nspherical, the gradient points right at the minimum.\\nBenefits of Batch Normalization\\nInclusion of Batch Normalization technique in deep neural networks improves training time\\nBN enables the utilization of larger learning rates, this shortness the time of convergence\\nwhen training neural networks\\nReduces the common problem of vanishing gradients\\nCovariate shift within neural network is reduced\\nPoint: In Batch normalization just as we standardize the inputs, the same way we\\nstandardize the activation at all the layers so that, at each layer we have 0 mean and unit\\nstandard deviation.', metadata={'source': 'Deep Learning.pdf', 'page': 53}),\n",
       " Document(page_content=\"## In keras, we can implement BatchNormalization using the keras layer. Below  \\nis the python code for it:\\nmodel = Sequential([\\n Dense(output_dim=hidden1_num_units, input_dim=input_num_units,  \\nactivation='relu'),\\n keras.layers.BatchNormalization(),\\n \\nDense(output_dim=output_num_units, input_dim=hidden5_num_units,  \\nactivation='softmax'),\\n ])\", metadata={'source': 'Deep Learning.pdf', 'page': 54}),\n",
       " Document(page_content='In [1]:\\nBinary Classifier\\nIn [2]:\\nIn [3]:\\nIn [4]:\\nApply ANNOut[2]:\\nPregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction\\n0 2138 62 35 033.6 0.127\\n1 0 84 82 3112538.2 0.233\\n2 0145 0 0 044.2 0.630\\n3 0135 68 4225042.3 0.365\\n4 1139 62 4148040.7 0.536\\nOut[3]:\\n(2000, 9)import pandas as pd\\nfrom matplotlib  import pyplot as plt\\nimport numpy as np\\n%matplotlib  inline\\nfrom sklearn.model_selection  import train_test_split\\nfrom sklearn.preprocessing  import LabelEncoder\\nfrom sklearn.preprocessing  import StandardScaler\\nfrom sklearn.metrics import mean_absolute_error ,mean_squared_error\\nfrom sklearn.metrics import confusion_matrix  , classification_report ,accuracy_score\\nimport tensorflow  as tf\\nfrom tensorflow  import keras\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\ndf = pd.read_csv (\"kaggle_diabetes.csv\" )\\ndf.head()\\ndf.shape\\nX = df.drop(\\'Outcome\\' ,axis=1)\\ny = df[\\'Outcome\\' ]\\nX_train, X_test, y_train, y_test = train_test_split (X,y,test_size =0.2,random_state =5)', metadata={'source': 'Deep Learning.pdf', 'page': 55}),\n",
       " Document(page_content=\"In [5]:\\nIn [6]:\\nIn [7]:\\nIn [8]:\\nIn [9]:Epoch 1/150  \\n50/50 [==============================] - 1s 1ms/step - loss: 6.1302 - accu\\nracy: 0.4691  \\nEpoch 2/150  \\n50/50 [==============================] - 0s 1ms/step - loss: 1.5064 - accu\\nracy: 0.6145  \\nEpoch 3/150  \\n50/50 [==============================] - 0s 1ms/step - loss: 1.1917 - accu\\nracy: 0.6264  \\nEpoch 4/150  \\n50/50 [==============================] - 0s 1ms/step - loss: 0.9443 - accu\\nracy: 0.6452  \\nEpoch 5/150  \\n50/50 [==============================] - 0s 1ms/step - loss: 0.8631 - accu\\nracy: 0.6444  \\nEpoch 6/150  \\n50/50 [==============================] - 0s 2ms/step - loss: 0.8198 - accu\\nracy: 0.6555  \\nEpoch 7/150  \\n50/50[ ]01/tl07250\\n13/13 [==============================] - 0s 1ms/step - loss: 0.4997 - accura\\ncy: 0.7775  \\nOut[8]:\\n[0.4997430741786957, 0.7774999737739563]classifier  = Sequential ()\\n##input 1st layer\\nclassifier .add(Dense(16,activation ='relu',input_dim =8))\\n## second hidden layer\\nclassifier .add(Dense(8,activation ='relu'))\\n## output layer\\nclassifier .add(Dense(1,activation ='sigmoid' ))\\nclassifier .compile(optimizer ='adam',\\n              loss='binary_crossentropy' ,\\n              metrics=['accuracy' ])\\nclassifier .fit(X_train, y_train, epochs=150)\\nclassifier .evaluate (X_test, y_test)\\ny_pred = classifier .predict(X_test)\", metadata={'source': 'Deep Learning.pdf', 'page': 56}),\n",
       " Document(page_content=\"In [10]:\\nIn [11]:\\nIn [12]:\\nMulti Classification Using ANN\\nIn [13]:Out[10]:\\narray([[0.1072953 ],  \\n       [0.03356129],  \\n       [0.20200807],  \\n       [0.16860384],  \\n       [0.50361   ]], dtype=float32)\\n              precision    recall  f1-score   support  \\n \\n           0       0.79      0.92      0.85       269  \\n           1       0.74      0.49      0.59       131  \\n \\n    accuracy                           0.78       400  \\n   macro avg       0.77      0.70      0.72       400  \\nweighted avg       0.77      0.78      0.76       400  \\n \\nOut[13]:\\n(150, 5)yp = classifier .predict(X_test)\\nyp[:5]\\ny_pred = []\\nfor element in yp:\\n    if element > 0.5:\\n        y_pred.append(1)\\n    else:\\n        y_pred.append(0)\\nprint(classification_report (y_test,y_pred))\\ndata = pd.read_csv ('https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a\\ndata.shape\", metadata={'source': 'Deep Learning.pdf', 'page': 57}),\n",
       " Document(page_content=\"In [14]:\\nSplit in to X and y\\nIn [15]:\\nEncoding target variable Using Label or Dummy\\nimp1:- If you use dummy then in loss function you use categorical_crossentropy\\nimp2:- if you use label_encoding then in loss function you use sparse_categorical_crossentropy\\nOut[14]:\\nsepal_length sepal_width petal_length petal_width species\\n0 5.1 3.5 1.4 0.2setosa\\n1 4.9 3.0 1.4 0.2setosa\\n2 4.7 3.2 1.3 0.2setosa\\n3 4.6 3.1 1.5 0.2setosa\\n4 5.0 3.6 1.4 0.2setosadata.head()\\nX = data.drop('species' ,axis=1)\\ny = data['species' ]\", metadata={'source': 'Deep Learning.pdf', 'page': 58}),\n",
       " Document(page_content='In [16]:\\nIn [17]:\\nIn [18]:\\nIn [19]:\\nIn [20]:\\nApply ANN\\nIn [21]:\\nIn [22]:Out[16]:\\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  \\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  \\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  \\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  \\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,  \\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,  \\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\\nOut[20]:\\n(150, 4)## label\\nlb = LabelEncoder ()\\ny_enc = lb.fit_transform (y)\\ny_enc\\n# dummy\\n# y_dummy = pd.get_dummies(y).values\\nX_train,X_test,y_train,y_test = train_test_split (X,y_enc,test_size =0.25,random_state =4)\\nsc = StandardScaler ()\\nX_train_scaled  = sc.fit_transform (X_train)\\nX_test_scaled  = sc.transform (X_test)\\nX.shape\\nclassifier  = Sequential ()\\nclassifier .add(Dense(10,input_dim  = 4,activation  = \"relu\"))\\nclassifier .add(Dense(3,activation  = \"softmax\" ))\\n## if target dummy encoding use categorical_crossentropy if use label encoding use sparse_c\\nclassifier .compile(optimizer  = \\'adam\\' , loss = \\'sparse_categorical_crossentropy\\' ,  \\n                   metrics = [\\'accuracy\\' ] ) ', metadata={'source': 'Deep Learning.pdf', 'page': 59}),\n",
       " Document(page_content=\"In [23]:\\nIn [24]:\\nIn [25]:\\nIn [ ]:\\nIn [ ]:\\nRegression\\nIn [26]:Epoch 1/100  \\n4/4 [==============================] - 0s 2ms/step - loss: 0.8949 - accura\\ncy: 0.6033  \\nEpoch 2/100  \\n4/4 [==============================] - 0s 2ms/step - loss: 0.9165 - accura\\ncy: 0.5705  \\nEpoch 3/100  \\n4/4 [==============================] - 0s 2ms/step - loss: 0.8442 - accura\\ncy: 0.6268  \\nEpoch 4/100  \\n4/4 [==============================] - 0s 4ms/step - loss: 0.8412 - accura\\ncy: 0.6121  \\nEpoch 5/100  \\n4/4 [==============================] - 0s 4ms/step - loss: 0.8268 - accura\\ncy: 0.5933  \\nEpoch 6/100  \\n4/4 [==============================] - 0s 3ms/step - loss: 0.8125 - accura\\ncy: 0.6260  \\nEpoch 7/100  \\n4/4[ ]03/tl07945\\n              precision    recall  f1-score   support  \\n \\n           0       1.00      1.00      1.00        18  \\n           1       1.00      0.88      0.93         8  \\n           2       0.92      1.00      0.96        12  \\n \\n    accuracy                           0.97        38  \\n   macro avg       0.97      0.96      0.96        38  \\nweighted avg       0.98      0.97      0.97        38  \\n classifier .fit(X_train_scaled  , y_train ,epochs = 100) \\ny_pred = classifier .predict(X_test_scaled )\\n# y_test= np.argmax(y_test,axis=1) # when use dummy encoding in target\\ny_pred = np.argmax(y_pred,axis=1)\\nprint(classification_report (y_test,y_pred))\\ndf=pd.read_csv ('https://raw.githubusercontent.com/krishnaik06/Keras-Tuner/main/Real_Combine\", metadata={'source': 'Deep Learning.pdf', 'page': 60}),\n",
       " Document(page_content='In [27]:\\nIn [28]:\\nIn [29]:\\nIn [30]:\\nIn [31]:\\nIn [32]:\\nApply ANN\\nIn [33]:Out[27]:\\nT TM Tm SLP HVV V VM PM 2.5\\n07.49.84.81017.693.00.54.39.4219.720833\\n17.812.74.41018.587.00.64.411.1182.187500\\n26.713.42.41019.482.00.64.811.1154.037500\\n38.615.53.31018.772.00.88.120.6223.208333\\n412.420.94.41017.361.01.38.722.2200.645833\\nOut[32]:\\n(1092, 8)df.head()\\ndf.dropna(inplace=True)\\nX=df.drop(\\'PM 2.5\\' ,axis=1) ## independent features\\ny=df[\\'PM 2.5\\' ] ## dependent features\\nX_train, X_test, y_train, y_test = train_test_split (X, y, test_size =0.3, random_state =0)\\nsc = StandardScaler ()\\nX_train = sc.fit_transform (X_train)\\nX_test = sc.transform (X_test)\\nX.shape\\nclassifier  = Sequential ()\\nclassifier .add(Dense(10,input_dim  = 8,activation  = \"relu\"))\\n## second hidden layer\\nclassifier .add(Dense(8,activation =\\'relu\\'))\\nclassifier .add(Dense(1,activation  = \"linear\" ))', metadata={'source': 'Deep Learning.pdf', 'page': 61}),\n",
       " Document(page_content='In [34]:\\nIn [35]:\\nIn [36]:\\nIn [37]:\\nIn [38]:Epoch 1/100  \\n24/24 [==============================] - 1s 2ms/step - loss: 18824.7756 -  \\nmse: 18824.7756  \\nEpoch 2/100  \\n24/24 [==============================] - 0s 2ms/step - loss: 20844.0150 -  \\nmse: 20844.0150  \\nEpoch 3/100  \\n24/24 [==============================] - 0s 2ms/step - loss: 21034.4066 -  \\nmse: 21034.4066  \\nEpoch 4/100  \\n24/24 [==============================] - 0s 2ms/step - loss: 21785.1067 -  \\nmse: 21785.1067  \\nEpoch 5/100  \\n24/24 [==============================] - 0s 2ms/step - loss: 16766.3743 -  \\nmse: 16766.3743  \\nEpoch 6/100  \\n24/24 [==============================] - 0s 2ms/step - loss: 19805.6353 -  \\nmse: 19805.6353  \\nEpoch 7/100  \\n24/24[ ]0s3ms/step loss:189015041\\n11/11 [==============================] - 0s 2ms/step - loss: 3349.8369 - ms\\ne: 3349.8369  \\nOut[36]:\\n[3349.8369140625, 3349.8369140625]\\nMAE 40.52856917885261  \\nMSE 3349.837197332338  \\nRMSE 57.87777809602177  ## if target dummy encoding use categorical_crossentropy if use label encoding use sparse_c\\nclassifier .compile(optimizer  = \\'adam\\' , loss = \\'mse\\',  \\n                   metrics = [\\'mse\\'] ) \\nclassifier .fit(X_train , y_train ,epochs = 100) \\nclassifier .evaluate (X_test, y_test)\\ny_pred = classifier .predict(X_test)\\nprint(\"MAE\",mean_absolute_error (y_test,y_pred))\\nprint(\"MSE\",mean_squared_error (y_test,y_pred))\\nprint(\"RMSE\",mean_squared_error (y_test,y_pred,squared=False))', metadata={'source': 'Deep Learning.pdf', 'page': 62}),\n",
       " Document(page_content='Convolutional neural networks (CNN)\\nCnn are one of the most popular models used today . This neural network computational model uses a\\nvariation of multilayer perceptrons and contains one or more convolutional layers that can be either entirely\\nconnected or pooled.\\nThese convolutional layers create feature maps that record a region of image which is ultimately broken\\ninto rectangles and sent out for nonlinear processing.\\nLet us suppose this in the input matrix of 5×5 and a filter of matrix 3X3, for those who don’t know what a\\nfilter is a set of weights in a matrix applied on an image or a matrix to obtain the required features, please\\nsearch on convolution if this is your first time!\\nNote: W e always take the sum or average of all the values while doing a convolution.\\nSteps Involve in CNN', metadata={'source': 'Deep Learning.pdf', 'page': 63}),\n",
       " Document(page_content='1. Edge Detection (Convolution)\\nIn the previous article, we saw that the early layers of a neural network detect edges from an image.\\nDeeper layers might be able to detect the cause of the objects and even more deeper layers might detect\\nthe cause of complete objects (like a person’ s face).\\nIn this section, we will focus on how the edges can be detected from an image. Suppose we are given the\\nbelow image: As you can see, there are many vertical and horizontal edges in the image. The first thing to do is\\nto detect these edges:\\n', metadata={'source': 'Deep Learning.pdf', 'page': 64}),\n",
       " Document(page_content='So, we take the first 3 X 3 matrix from the 7 X 7 image and multiply it with the filter . Now , the first element\\nof the (n-k+1 x n-k+1) i.e (7-3+1 X 7-3+1) 5 X 5 output will be the sum of the element-wise product of these\\nvalues, i.e. 0 0+00+1 0+10+0 1+00+0 0+10+1*0 =0. T o calculate the second element of the 5 X 5 output, we\\nwill shift our filter one step towards the right and again get the sum of the element-wise product:\\n2. Pooling\\nA pooling layer is another building block of a CNN. Its function is to progressively reduce the spatial size of\\nthe representation to reduce the amount of parameters and computation in the network. Pooling layer\\noperates on each feature map independently . The most common approach used in pooling is max pooling.\\nTypes of Pooling Layers :-\\n1. Max Pooling  \\nMax pooling is a pooling operation that selects the maximum element from the region of the feature map\\ncovered by the filter . Thus, the output after max-pooling layer would be a feature map containing the most\\nprominent features of the previous feature map.\\n2. Average Pooling  \\nAverage pooling computes the average of the elements present in the region of feature map covered by the\\nfilter. Thus, while max pooling gives the most prominent feature in a particular patch of the feature map,\\naverage pooling gives the average of features present in a patch.', metadata={'source': 'Deep Learning.pdf', 'page': 65}),\n",
       " Document(page_content='More On Pooling https://www .geeksforgeeks.org/cnn-introduction-to-pooling-layer/\\n(https://www .geeksforgeeks.org/cnn-introduction-to-pooling-layer/)\\nNow Apply Pooling in our above Feature Map\\nProblem with Simple Convolution Layers\\nWhile applying convolutions we will not obtain the output dimensions the same as input we will lose data\\nover borders so we append a border of zeros and recalculate the convolution covering all the input values.\\n 1. Padding  \\n 2. Striding  \\n1. Padding', metadata={'source': 'Deep Learning.pdf', 'page': 66}),\n",
       " Document(page_content='See In without padding our input is 6x6 but output image goes down into 4x4 . so by using padding we got\\nthe same result.Padding is simply a process of adding layers of zeros to our input images so as to avoid\\nthe problems mentioned above.\\nSo padding prevents shrinking as, if p = number of layers of zeros added to the border of the image, then\\nour (n x n) image becomes (n + 2p) x (n + 2p) image after padding. So, applying convolution-operation\\n(with (f x f) filter) outputs (n + 2p – f + 1) x (n + 2p – f + 1) images. For example, adding one layer of\\npadding to an (8 x 8) image and using a (3 x 3) filter we would get an (8 x 8) output after performing\\nconvolution operation.', metadata={'source': 'Deep Learning.pdf', 'page': 67}),\n",
       " Document(page_content='2. Strides\\nIt uses to reduce the size of matrix. if we sfited by 1 then we called stride=1 and if we sfited by 2 means\\nstride = 2 so on.\\nPadding,Stride Put in One Equation', metadata={'source': 'Deep Learning.pdf', 'page': 68}),\n",
       " Document(page_content='Step3 : Flattening\\nFlattening is converting the data into a 1-dimensional array for inputting it to the next layer . We flatten the\\noutput of the convolutional layers to create a single long feature vector . And it is connected to the final\\nclassification model, which is called a fully-connected layer .\\nStep 4\\nComplete CNN in one V iew\\nhere in last step we use full connection network\\n', metadata={'source': 'Deep Learning.pdf', 'page': 69}),\n",
       " Document(page_content=\"This is a simple CNN Network\\nIn [1]:\\nIn [2]:\\nIn [3]:\\nIn [4]:Found 198 images belonging to 2 classes.  \\nFound 100 images belonging to 2 classes.  # Importing the libraries\\nimport tensorflow  as tf\\nfrom tensorflow .keras.preprocessing .image import ImageDataGenerator\\n#data sugmentation\\n# Preprocessing the Training set\\ntrain_datagen  = ImageDataGenerator (rescale=1./255,\\n                                      rotation_range =40,\\n                                      width_shift_range =0.2,\\n                                      height_shift_range =0.2,\\n                                      shear_range =0.2,\\n                                      zoom_range =0.2,\\n                                      horizontal_flip =True,\\n                                      fill_mode ='nearest' )\\ntraining_set  = train_datagen .flow_from_directory ('image_data/training' ,\\n                                                 target_size  = (64, 64),\\n                                                 batch_size  = 32,\\n                                                 class_mode  = 'binary' )\\n# Preprocessing the Test set\\ntest_datagen  = ImageDataGenerator (rescale = 1./255)\\ntest_set  = test_datagen .flow_from_directory ('image_data/validation' ,\\n                                            target_size  = (64, 64),\\n                                            batch_size  = 32,\\n                                            class_mode  = 'binary' )\\n## showing some image from training\\nimport matplotlib .pyplot as plt\\ndef plotImages (images_arr ):\\n    fig, axes = plt.subplots (1, 5, figsize=(20, 20))\\n    axes = axes.flatten()\\n    for img, ax in zip(images_arr , axes):\\n        ax.imshow(img)\\n    plt.tight_layout ()\\n    plt.show()\", metadata={'source': 'Deep Learning.pdf', 'page': 70}),\n",
       " Document(page_content='In [5]:\\nModel Build Use Only CNN\\nIn [6]:\\nIn [7]:\\nIn [8]:\\nimages = [training_set [0][0][0] for i in range(5)]\\nplotImages (images)\\nfrom tensorflow .keras.layers import Conv2D\\n# Part 2 - Building the CNN\\n# Initialising the CNN\\ncnn = tf.keras.models.Sequential ()\\n# Step 1 - # Adding a first convolutional layer\\ncnn.add(tf.keras.layers.Conv2D(filters=32,padding=\"same\",kernel_size =3, activation =\\'relu\\', \\n## step 2 - #apply maxpool\\ncnn.add(tf.keras.layers.MaxPool2D (pool_size =2, strides=2)) ## Apply pooing stride\\n# Adding a second convolutional layer\\ncnn.add(tf.keras.layers.Conv2D(filters=32,padding=\\'same\\',kernel_size =3, activation =\\'relu\\'))\\ncnn.add(tf.keras.layers.MaxPool2D (pool_size =2, strides=2))\\n# Step 3 - Flattening\\ncnn.add(tf.keras.layers.Flatten())\\n# Step 4 - Full Connection\\ncnn.add(tf.keras.layers.Dense(units=128, activation =\\'relu\\'))\\ntf.keras.layers.Dropout(0.5)\\n# Step 5 - Output Layer\\ncnn.add(tf.keras.layers.Dense(units=1, activation =\\'sigmoid\\' ))\\n# Part 3 - Training the CNN\\n# Compiling the CNN\\ncnn.compile(optimizer  = \\'adam\\', loss = \\'binary_crossentropy\\' , metrics = [\\'accuracy\\' ])', metadata={'source': 'Deep Learning.pdf', 'page': 71}),\n",
       " Document(page_content=\"In [9]:\\nSave And Load Model\\nIn [10]:\\nIn [11]:\\nIn [12]:Epoch 1/2  \\n7/7 [==============================] - 4s 410ms/step - loss: 0.7529 - accura\\ncy: 0.4600 - val_loss: 0.6988 - val_accuracy: 0.5000  \\nEpoch 2/2  \\n7/7 [==============================] - 2s 308ms/step - loss: 0.6898 - accura\\ncy: 0.5349 - val_loss: 0.6932 - val_accuracy: 0.5100  \\nOut[12]:\\narray([[0.5059088]], dtype=float32)# Training the CNN on the Training set and evaluating it on the Test set\\nhistory = cnn.fit(x = training_set , validation_data  = test_set , epochs = 2)\\n#save model\\nfrom tensorflow .keras.models import load_model\\ncnn.save('model_rcat_dog.h5' )\\nfrom tensorflow .keras.models import load_model\\n# load model\\nmodel = load_model ('model_rcat_dog.h5' )\\n# Part 4 - Making a single prediction\\nimport numpy as np\\nfrom tensorflow .keras.preprocessing  import image\\ntest_image  = image.load_img ('image_data/test/3285.jpg' , target_size  = (64,64))\\ntest_image  = image.img_to_array (test_image )\\ntest_image =test_image /255\\ntest_image  = np.expand_dims (test_image , axis = 0)\\nresult = cnn.predict(test_image )\\nresult \", metadata={'source': 'Deep Learning.pdf', 'page': 72}),\n",
       " Document(page_content='In [13]:\\nThe image classified is dog  \\nOut[13]:\\nif result[0]<=0.5:\\n    print(\"The image classified is cat\" )\\nelse:\\n    print(\"The image classified is dog\" )\\n    \\nfrom IPython.display import Image\\nImage(filename =\\'image_data/test/3285.jpg\\' ,height=\\'200\\',width=\\'200\\') ', metadata={'source': 'Deep Learning.pdf', 'page': 73}),\n",
       " Document(page_content='Basic Introduction\\nLeNet-5, from the paper Gradient-Based Learning Applied to Document Recognition, is a very ef ficient\\nconvolutional neural network for handwritten character recognition.\\nPaper: Gradient-Based Learning Applied to Document Recognition\\n(http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\\nAuthors : Yann LeCun, Léon Bottou, Y oshua Bengio, and Patrick Haf fner\\nPublished in : Proceedings of the IEEE (1998)\\nStructure of the LeNet network\\nLeNet5 is a small network, it contains the basic modules of deep learning: convolutional layer , pooling layer ,\\nand full link layer . It is the basis of other deep learning models. Here we analyze LeNet5 in depth. At the same\\ntime, through example analysis, deepen the understanding of the convolutional layer and pooling layer .\\nLeNet-5 T otal seven layer , does not comprise an input, each containing a trainable parameters; each layer has\\na plurality of the Map the Feature , a characteristic of each of the input FeatureMap extracted by means of a\\nconvolution filter , and then each FeatureMap There are multiple neurons.\\nDetailed explanation of each layer parameter:', metadata={'source': 'Deep Learning.pdf', 'page': 74}),\n",
       " Document(page_content=\"INPUT Layer\\nThe first is the data INPUT layer . The size of the input image is uniformly normalized to 32 * 32.\\nNote: This layer does not count as the network structure of LeNet-5. T raditionally , the input layer\\nis not considered as one of the network hierarchy .\\nC1 layer-convolutional layer\\nInput picture : 32 * 32\\nConvolution kernel size : 5 * 5\\nConvolution kernel types : 6\\nOutput featuremap size : 28 * 28 (32-5 + 1) = 28\\nNumber of neurons : 28 28 6\\nTrainable parameters : (5 5 + 1) 6 (5 * 5 = 25 unit parameters and one bias parameter per filter ,\\na total of 6 filters)\\nNumber of connections : (5 5 + 1) 6 28 28 = 122304\\nDetailed description:\\n1. The first convolution operation is performed on the input image (using 6 convolution kernels of size 5 5) to\\nobtain 6 C1 feature maps (6 feature maps of size 28 28, 32-5 + 1 = 28).\\n2. Let's take a look at how many parameters are needed. The size of the convolution kernel is 5 5, and there\\nare 6 (5 * 5 + 1) = 156 parameters in total, where +1 indicates that a kernel has a bias.\\n3. For the convolutional layer C1, each pixel in C1 is connected to 5 5 pixels and 1 bias in the input image, so\\nthere are 156 28 * 28 = 122304 connections in total. There are 122,304 connections, but we only need to\\nlearn 156 parameters, mainly through weight sharing.\\nS2 layer-pooling layer (downsampling layer)\\nInput : 28 * 28\\nSampling area : 2 * 2\\nSampling method : 4 inputs are added, multiplied by a trainable parameter , plus a trainable\\noffset. Results via sigmoid\\nSampling type : 6\\nOutput featureMap size : 14 * 14 (28/2)\\nNumber of neurons : 14 14 6\\nTrainable parameters : 2 * 6 (the weight of the sum + the of fset)\\nNumber of connections : (2 2 + 1) 6 14 14\\nThe size of each feature map in S2 is 1/4 of the size of the feature map in C1.\", metadata={'source': 'Deep Learning.pdf', 'page': 75}),\n",
       " Document(page_content='Detailed description:\\nThe pooling operation is followed immediately after the first convolution. Pooling is performed using 2 2 kernels,\\nand S2, 6 feature maps of 14 14 (28/2 = 14) are obtained.\\nThe pooling layer of S2 is the sum of the pixels in the 2 * 2 area in C1 multiplied by a weight coef ficient plus an\\noffset, and then the result is mapped again.\\nSo each pooling core has two training parameters, so there are 2x6 = 12 training parameters, but there are\\n5x14x14x6 = 5880 connections.\\nC3 layer-convolutional layer\\nInput : all 6 or several feature map combinations in S2\\nConvolution kernel size : 5 * 5\\nConvolution kernel type : 16\\nOutput featureMap size : 10 * 10 (14-5 + 1) = 10\\nEach feature map in C3 is connected to all 6 or several feature maps in S2, indicating that the\\nfeature map of this layer is a dif ferent combination of the feature maps extracted from the\\nprevious layer .\\nOne way is that the first 6 feature maps of C3 take 3 adjacent feature map subsets in S2 as\\ninput. The next 6 feature maps take 4 subsets of neighboring feature maps in S2 as input. The\\nnext three take the non-adjacent 4 feature map subsets as input. The last one takes all the\\nfeature maps in S2 as input.\\nThe trainable parameters are : 6 (3 5 5 + 1) + 6 (4 5 5 + 1) + 3 (4 5 5 + 1) + 1 (6 5 5 +1) = 1516\\nNumber of connections : 10 10 1516 = 151600\\nDetailed description:\\nAfter the first pooling, the second convolution, the output of the second convolution is C3, 16 10x10 feature\\nmaps, and the size of the convolution kernel is 5 5. We know that S2 has 6 14 14 feature maps, how to get 16\\nfeature maps from 6 feature maps? Here are the 16 feature maps calculated by the special combination of the\\nfeature maps of S2. details as follows:\\nThe first 6 feature maps of C3 (corresponding to the 6th column of the first red box in the figure above) are\\nconnected to the 3 feature maps connected to the S2 layer (the first red box in the above figure), and the next 6\\nfeature maps are connected to the S2 layer The 4 feature maps are connected (the second red box in the figure\\nabove), the next 3 feature maps are connected with the 4 feature maps that are not connected at the S2 layer ,\\nand the last is connected with all the feature maps at the S2 layer . The convolution kernel size is still 5 5, so\\nthere are 6 (3 5 5 + 1) + 6 (4 5 5 + 1) + 3 (4 5 5 + 1) +1 (6 5 5 + 1) = 1516 parameters. The image size is 10 10,\\nso there are 151600 connections.', metadata={'source': 'Deep Learning.pdf', 'page': 76}),\n",
       " Document(page_content='The convolution structure of C3 and the first 3 graphs in S2 is shown below:\\nS4 layer-pooling layer (downsampling layer)\\nInput : 10 * 10\\nSampling area : 2 * 2\\nSampling method : 4 inputs are added, multiplied by a trainable parameter , plus a trainable\\noffset. Results via sigmoid\\nSampling type : 16\\nOutput featureMap size : 5 * 5 (10/2)\\nNumber of neurons : 5 5 16 = 400\\nTrainable parameters : 2 * 16 = 32 (the weight of the sum + the of fset)\\nNumber of connections : 16 (2 2 + 1) 5 5 = 2000\\nThe size of each feature map in S4 is 1/4 of the size of the feature map in C3\\nDetailed description:', metadata={'source': 'Deep Learning.pdf', 'page': 77}),\n",
       " Document(page_content='S4 is the pooling layer , the window size is still 2 * 2, a total of 16 feature maps, and the 16 10x10 maps of the\\nC3 layer are pooled in units of 2x2 to obtain 16 5x5 feature maps. This layer has a total of 32 training\\nparameters of 2x16, 5x5x5x16 = 2000 connections.\\nThe connection is similar to the S2 layer .\\nC5 layer-convolution layer\\nInput : All 16 unit feature maps of the S4 layer (all connected to s4)\\nConvolution kernel size : 5 * 5\\nConvolution kernel type : 120\\nOutput featureMap size : 1 * 1 (5-5 + 1)\\nTrainable parameters / connection : 120 (16 5 * 5 + 1) = 48120\\nDetailed description:\\nThe C5 layer is a convolutional layer . Since the size of the 16 images of the S4 layer is 5x5, which is the same\\nas the size of the convolution kernel, the size of the image formed after convolution is 1x1. This results in 120\\nconvolution results. Each is connected to the 16 maps on the previous level. So there are (5x5x16 + 1) x120 =\\n48120 parameters, and there are also 48120 connections. The network structure of the C5 layer is as follows:\\nF6 layer-fully connected layer\\nInput : c5 120-dimensional vector\\nCalculation method : calculate the dot product between the input vector and the weight vector ,\\nplus an of fset, and the result is output through the sigmoid function.\\nTrainable parameters : 84 * (120 + 1) = 10164\\nDetailed description:', metadata={'source': 'Deep Learning.pdf', 'page': 78}),\n",
       " Document(page_content='Layer 6 is a fully connected layer . The F6 layer has 84 nodes, corresponding to a 7x12 bitmap, -1 means white,\\n1 means black, so the black and white of the bitmap of each symbol corresponds to a code. The training\\nparameters and number of connections for this layer are (120 + 1) x84 = 10164. The ASCII encoding diagram is\\nas follows:\\nThe connection method of the F6 layer is as follows:\\nOutput layer-fully connected layer\\nThe output layer is also a fully connected layer , with a total of 10 nodes, which respectively represent the\\nnumbers 0 to 9, and if the value of node i is 0, the result of network recognition is the number i. A radial basis\\nfunction (RBF) network connection is used. Assuming x is the input of the previous layer and y is the output of\\nthe RBF , the calculation of the RBF output is:\\nThe value of the above formula w_ij is determined by the bitmap encoding of i, where i ranges from 0 to 9, and j\\nranges from 0 to 7 * 12-1. The closer the value of the RBF output is to 0, the closer it is to i, that is, the closer to\\nthe ASCII encoding figure of i, it means that the recognition result input by the current network is the character i.\\nThis layer has 84x10 = 840 parameters and connections.', metadata={'source': 'Deep Learning.pdf', 'page': 79}),\n",
       " Document(page_content='Summary\\nLeNet-5 is a very ef ficient convolutional neural network for handwritten character recognition.\\nConvolutional neural networks can make good use of the structural information of images.\\nThe convolutional layer has fewer parameters, which is also determined by the main characteristics of the\\nconvolutional layer , that is, local connection and shared weights.\\nCode Implementation\\nIn [4]:\\nIn [2]:import keras\\nfrom keras.datasets  import mnist\\nfrom keras.layers import Conv2D, MaxPooling2D ,AveragePooling2D\\nfrom keras.layers import Dense, Flatten\\nfrom keras.models import Sequential\\n# Loading the dataset and perform splitting\\n(x_train, y_train), (x_test, y_test) = mnist.load_data ()\\n# Peforming reshaping operation\\nx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)', metadata={'source': 'Deep Learning.pdf', 'page': 80}),\n",
       " Document(page_content=\"In [3]:\\nLeNet Model\\nIn [8]:\\nOR# Normalization\\nx_train = x_train / 255\\nx_test = x_test / 255\\n \\n# One Hot Encoding\\ny_train = keras.utils.to_categorical (y_train, 10)\\ny_test = keras.utils.to_categorical (y_test, 10)\\n# Building the Model Architecture\\nmodel = Sequential ()\\n# Select 6 feature convolution kernels with a size of 5 * 5 (without offset), and get 66 fe\\n# That is, the number of neurons has been reduced from 10241024 to 28 ∗  28 = 784 28 ∗  28 = \\n# Parameters between input layer and C1 layer: 6 ∗  (5 ∗  5 + 1)\\nmodel.add(Conv2D(6, kernel_size =(5, 5), activation ='tanh', input_shape =(28, 28, 1)))\\n# The input of this layer is the output of the first layer, which is a 28 * 28 * 6 node mat\\n# The size of the filter used in this layer is 2 * 2, and the step length and width are bot\\nmodel.add(MaxPooling2D (pool_size =(2, 2)))\\n# The input matrix size of this layer is 14 * 14 * 6, the filter size used is 5 * 5, and th\\n# The output matrix size of this layer is 10 * 10 * 16. This layer has 5 * 5 * 6 * 16 + 16 \\nmodel.add(Conv2D(16, kernel_size =(5, 5), activation ='tanh'))\\n# The input matrix size of this layer is 10 * 10 * 16. The size of the filter used in this \\nmodel.add(MaxPooling2D (pool_size =(2, 2)))\\n# The input matrix size of this layer is 5 * 5 * 16. This layer is called a convolution lay\\n# So it is not different from the fully connected layer. If the nodes in the 5 * 5 * 16 mat\\n# The number of output nodes in this layer is 120, with a total of 5 * 5 * 16 * 120 + 120 =\\nmodel.add(Flatten())\\nmodel.add(Dense(120, activation ='tanh'))\\n# The number of input nodes in this layer is 120 and the number of output nodes is 84. The \\nmodel.add(Dense(84, activation ='tanh'))\\n# The number of input nodes in this layer is 84 and the number of output nodes is 10. The t\\nmodel.add(Dense(10, activation ='softmax' ))\", metadata={'source': 'Deep Learning.pdf', 'page': 81}),\n",
       " Document(page_content='In [ ]:\\nIn [9]:\\nIn [10]:Model: \"sequential_1\"  \\n_________________________________________________________________  \\nLayer (type)                 Output Shape              Param #    \\n=================================================================  \\nconv2d_2 (Conv2D)            (None, 24, 24, 6)         156        \\n_________________________________________________________________  \\nmax_pooling2d_2 (MaxPooling2 (None, 12, 12, 6)         0          \\n_________________________________________________________________  \\nconv2d_3 (Conv2D)            (None, 8, 8, 16)          2416       \\n_________________________________________________________________  \\nmax_pooling2d_3 (MaxPooling2 (None, 4, 4, 16)          0          \\n_________________________________________________________________  \\nflatten_1 (Flatten)          (None, 256)               0          \\n_________________________________________________________________  \\ndense_3 (Dense)              (None, 120)               30840      \\n_________________________________________________________________  \\ndense_4 (Dense)              (None, 84)                10164      \\n_________________________________________________________________  \\ndense_5 (Dense)              (None, 10)                850        \\n=================================================================  \\nTotal params: 44,426  \\nTrainable params: 44,426  \\nNon-trainable params: 0  \\n_________________________________________________________________  \\n469/469 [==============================] - 22s 46ms/step - loss: 0.6072 - ac\\ncuracy: 0.8285 - val_loss: 0.0840 - val_accuracy: 0.9737  \\nOut[10]:\\n<tensorflow.python.keras.callbacks.History at 0x195a9244748>model = keras.Sequential ()\\n \\nmodel.add(Conv2D(filters=6, kernel_size =(5, 5), activation =\\'tanh\\', input_shape =(32,32,1)))\\nmodel.add(AveragePooling2D (2,2))\\n \\nmodel.add(Conv2D(filters=16, kernel_size =(5, 5), activation =\\'tanh\\'))\\nmodel.add(AveragePooling2D ())\\n \\nmodel.add(Flatten())\\n \\nmodel.add(Dense(units=120, activation =\\'tanh\\'))\\n \\nmodel.add(Dense(units=84, activation =\\'tanh\\'))\\n \\nmodel.add(Dense(units=10, activation  = \\'softmax\\' ))\\nmodel.summary()\\nmodel.compile(loss=keras.metrics.categorical_crossentropy , optimizer =keras.optimizers .Adam(\\nmodel.fit(x_train, y_train, batch_size =128, epochs=1, verbose=1, validation_data =(x_test, y', metadata={'source': 'Deep Learning.pdf', 'page': 82}),\n",
       " Document(page_content=\"In [11]:\\n313/313 [==============================] - 2s 7ms/step - loss: 0.0840 - accu\\nracy: 0.9737  \\nTest Loss: 0.08401492983102798  \\nTest accuracy: 0.9736999869346619  score = model.evaluate (x_test, y_test)\\nprint('Test Loss:' , score[0])\\nprint('Test accuracy:' , score[1])\", metadata={'source': 'Deep Learning.pdf', 'page': 83}),\n",
       " Document(page_content='ALEXNET\\nThe most important features of the AlexNet paper are:\\nAs the model had to train 60 million parameters (which is quite a lot), it was prone to overfitting. According\\nto the paper , the usage of Dropout and Data Augmentation significantly helped in reducing overfitting. The\\nfirst and second fully connected layers in the architecture thus used a dropout of 0.5 for the purpose.\\nArtificially increasing the number of images through data augmentation helped in the expansion of the\\ndataset dynamically during runtime, which helped the model generalize better .\\nAnother distinct factor was using the ReLU activation function instead of tanh or sigmoid, which resulted in\\nfaster training times (a decrease in training time by 6 times). Deep Learning Networks usually employ\\nReLU non-linearity to achieve faster training times as the others start saturating when they hit higher\\nactivation values.\\nAlexNet is a Classic type of Convolutional Neural Network, and it came into existence after the 2012\\nImageNet challenge. The network architecture is given below :\\nModel Explanation :\\nThe Input to this model have the dimensions 227x227x3 follwed by a Convolutional Layer with 96 filters of\\n11x11 dimensions and having a ‘same’ padding and a stride of 4. The resulting output dimensions are\\ngiven as :', metadata={'source': 'Deep Learning.pdf', 'page': 84}),\n",
       " Document(page_content='       floor(((n + 2*padding - filter)/stride) + 1 ) * floor(((n + 2*padding —  \\nfilter)/stride) + 1)  \\nNote : This formula is for square input with height = width = n\\nExplaining the first Layer with input 227x227x3 and Convolutional layer with 96 filters of 1 1x11 , ‘valid’\\npadding and stride = 4 , output dims will be\\n= floor(((227 + 0–1 1)/4) + 1) * floor(((227 + 0–1 1)/4) + 1)\\n= floor((216/4) + 1) * floor((216/4) + 1)\\n= floor(54 + 1) * floor(54 + 1)\\n= 55 * 55\\nSince number of filters = 96 , thus output of first Layer is : 55x55x96\\nContinuing we have the MaxPooling layer (3, 3) with the stride of 2,making the output size decrease to\\n27x27x96, followed by another Convolutional Layer with 256, (5,5) filters and ‘same’ padding, that is, the\\noutput height and width are retained as the previous layer thus output from this layer is 27x27x256.\\nNext we have the MaxPooling again ,reducing the size to 13x13x256. Another Convolutional Operation\\nwith 384, (3,3) filters having same padding is applied twice giving the output as 13x13x384, followed by\\nanother Convulutional Layer with 256 , (3,3) filters and same padding resulting in 13x13x256 output.\\nThis is MaxPooled and dimensions are reduced to 6x6x256. Further the layer is Flatten out and 2 Fully\\nConnected Layers with 4096 units each are made which is further connected to 1000 units softmax layer .\\nThe network is used for classifying much large number of classes as per our requirement. However in our\\ncase, we will make the output softmax layer with 6 units as we ahve to classify into 6 classes. The softmax\\nlayer gives us the probablities for each class to which an Input Image might belong.\\nSize /\\nOperationFilter Depth Stride Padding Number of Parameters Forward Computation\\n3 227 227\\nConv1 + Relu 11 11 96 4 (11113 + 1) 96=34944 (11113 + 1) 96 55 55=105705600\\n96 55 55\\nMax Pooling 3 3 2\\n96 27 27\\nNorm\\nConv2 + Relu 5 5 256 1 2 (5 5 96 + 1) 256=614656 (5 5 96 + 1) 256 27 27=448084224\\n256 27 27\\nMax Pooling 3 3 2\\n256 13 13\\nNorm\\nConv3 + Relu 3 3 384 1 1 (3 3 256 + 1) 384=885120 (3 3 256 + 1) 384 13 13=149585280\\n384 13 13\\nConv4 + Relu 3 3 384 1 1 (3 3 384 + 1) 384=1327488 (3 3 384 + 1) 384 13 13=224345472\\n384 13 13\\nConv5 + Relu 3 3 256 1 1 (3 3 384 + 1) 256=884992 (3 3 384 + 1) 256 13 13=149563648\\n256 13 13', metadata={'source': 'Deep Learning.pdf', 'page': 85}),\n",
       " Document(page_content='Size /\\nOperationFilter Depth Stride Padding Number of Parameters Forward Computation\\nMax Pooling 3 3 2\\n256 6 6\\nDropout (rate\\n0.5)\\nFC6 + Relu 256 6 6 4096=37748736 256 6 6 4096=37748736\\n4096\\nDropout (rate\\n0.5)\\nFC7 + Relu 4096 4096=16777216 4096 4096=16777216\\n4096\\nFC8 + Relu 4096 1000=4096000 4096 1000=4096000\\n1000 classes\\nOverall 62369152=62.3 million 1135906176=1.1 billion\\nConv VS FCConv:3.7million (6%) , FC: 58.6\\nmillion (94% )Conv: 1.08 billion (95%) , FC: 58.6\\nmillion (5%)\\nWhy does AlexNet achieve better results?\\n1. Relu activation function is used.\\nRelu function: f (x) = max (0, x)\\nReLU-based deep convolutional networks are trained several times faster than tanh and sigmoid- based\\nnetworks. The following figure shows the number of iterations for a four-layer convolutional network based on\\nCIFAR-10 that reached 25% training error in tanh and ReLU:\\n', metadata={'source': 'Deep Learning.pdf', 'page': 86}),\n",
       " Document(page_content='1. Standardization ( Local Response Normalization )\\nAfter using ReLU f (x) = max (0, x), you will find that the value after the activation function has no range like the\\ntanh and sigmoid functions, so a normalization will usually be done after ReLU, and the LRU is a steady\\nproposal (Not sure here, it should be proposed?) One method in neuroscience is called \"Lateral inhibition\",\\nwhich talks about the ef fect of active neurons on its surrounding neurons.\\n1. Dropout\\nDropout is also a concept often said, which can ef fectively prevent overfitting of neural networks. Compared to\\nthe general linear model, a regular method is used to prevent the model from overfitting. In the neural network,\\nDropout is implemented by modifying the structure of the neural network itself. For a certain layer of neurons,\\nrandomly delete some neurons with a defined probability , while keeping the individuals of the input layer and\\noutput layer neurons unchanged, and then update the parameters according to the learning method of the\\nneural network. In the next iteration, rerandom Remove some neurons until the end of training.\\n1. Enhanced Data ( Data Augmentation )\\nIn deep learning, when the amount of data is not large enough, there are generally 4 solutions:', metadata={'source': 'Deep Learning.pdf', 'page': 87}),\n",
       " Document(page_content='Data augmentation- artificially increase the size of the training set-create a batch of \"new\" data\\nfrom existing data by means of translation, flipping, noise\\nRegularization——The relatively small amount of data will cause the model to overfit, making\\nthe training error small and the test error particularly large. By adding a regular term after the\\nLoss Function , the overfitting can be suppressed. The disadvantage is that a need is\\nintroduced Manually adjusted hyper-parameter .\\nDropout- also a regularization method. But dif ferent from the above, it is achieved by randomly\\nsetting the output of some neurons to zero\\nUnsupervised Pre-training- use Auto-Encoder or RBM\\'s convolution form to do unsupervised\\npre-training layer by layer , and finally add a classification layer to do supervised Fine-T uning\\nIn [1]:\\n#Importing library\\nimport keras\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Activation , Dropout, Flatten, Conv2D, MaxPooling2D\\nfrom keras.layers.normalization  import BatchNormalization\\nimport numpy as np\\n \\nnp.random.seed(1000)\\n ', metadata={'source': 'Deep Learning.pdf', 'page': 88}),\n",
       " Document(page_content=\"In [6]:\\nmodel = Sequential () \\n \\n# 1st Convolutional Layer \\nmodel.add(Conv2D(filters = 96, input_shape  = (224, 224, 3), kernel_size  = (11, 11), strides\\nmodel.add(Activation ('relu')) \\n# Max-Pooling \\nmodel.add(MaxPooling2D (pool_size  = (2, 2), strides = (2, 2), padding = 'valid')) \\n# Batch Normalisation \\nmodel.add(BatchNormalization ()) \\n \\n# 2nd Convolutional Layer \\nmodel.add(Conv2D(filters = 256, kernel_size  = (11, 11), strides = (1, 1), padding = 'valid'\\nmodel.add(Activation ('relu')) \\n# Max-Pooling \\nmodel.add(MaxPooling2D (pool_size  = (2, 2), strides = (2, 2), padding = 'valid')) \\n# Batch Normalisation \\nmodel.add(BatchNormalization ()) \\n \\n# 3rd Convolutional Layer \\nmodel.add(Conv2D(filters = 384, kernel_size  = (3, 3), strides = (1, 1), padding = 'valid'))\\nmodel.add(Activation ('relu')) \\n# Batch Normalisation \\nmodel.add(BatchNormalization ()) \\n \\n# 4th Convolutional Layer \\nmodel.add(Conv2D(filters = 384, kernel_size  = (3, 3), strides = (1, 1), padding = 'valid'))\\nmodel.add(Activation ('relu')) \\n# Batch Normalisation \\nmodel.add(BatchNormalization ()) \\n \\n# 5th Convolutional Layer \\nmodel.add(Conv2D(filters = 256, kernel_size  = (3, 3), strides = (1, 1), padding = 'valid'))\\nmodel.add(Activation ('relu')) \\n# Max-Pooling \\nmodel.add(MaxPooling2D (pool_size  = (2, 2), strides = (2, 2),padding = 'valid')) \\n# Batch Normalisation \\nmodel.add(BatchNormalization ()) \\n \\n# Flattening \\nmodel.add(Flatten()) \\n \\n# 1st Dense Layer \\nmodel.add(Dense(4096, input_shape  = (224*224*3, ))) \\nmodel.add(Activation ('relu')) \\n# Add Dropout to prevent overfitting \\nmodel.add(Dropout(0.4)) \\n# Batch Normalisation \\nmodel.add(BatchNormalization ()) \\n \\n# 2nd Dense Layer \\nmodel.add(Dense(4096)) \\nmodel.add(Activation ('relu')) \\n# Add Dropout \\nmodel.add(Dropout(0.4)) \\n# Batch Normalisation \\nmodel.add(BatchNormalization ()) \\n \\n# Output Softmax Layer \\nmodel.add(Dense(10)) \", metadata={'source': 'Deep Learning.pdf', 'page': 89}),\n",
       " Document(page_content='In [7]:\\nIn [ ]:Model: \"sequential\"  \\n_________________________________________________________________  \\nLayer (type)                 Output Shape              Param #    \\n=================================================================  \\nconv2d (Conv2D)              (None, 54, 54, 96)        34944      \\n_________________________________________________________________  \\nactivation (Activation)      (None, 54, 54, 96)        0          \\n_________________________________________________________________  \\nmax_pooling2d (MaxPooling2D) (None, 27, 27, 96)        0          \\n_________________________________________________________________  \\nbatch_normalization (BatchNo (None, 27, 27, 96)        384        \\n_________________________________________________________________  \\nconv2d_1 (Conv2D)            (None, 17, 17, 256)       2973952    \\n_________________________________________________________________  \\nactivation_1 (Activation)    (None, 17, 17, 256)       0          \\n_________________________________________________________________  \\nmax_pooling2d_1 (MaxPooling2 (None, 8, 8, 256)         0          \\n_________________________________________________________________  \\nbatch_normalization_1 (Batch (None, 8, 8, 256)         1024       model.add(Activation (\\'softmax\\' )) \\n \\n#Model Summary\\nmodel.summary()\\n ', metadata={'source': 'Deep Learning.pdf', 'page': 90}),\n",
       " Document(page_content='VGG16\\nVGG16 is a convolution neural net (CNN ) architecture which was used to win ILSVR(Imagenet)\\ncompetition in 2014.\\nIt is considered to be one of the excellent vision model architecture till date. Most unique thing about\\nVGG16 is that instead of having a large number of hyper-parameter they focused on having convolution\\nlayers of 3x3 filter with a stride 1 and always used same padding and maxpool layer of 2x2 filter of stride 2.\\nIt follows this arrangement of convolution and max pool layers consistently throughout the whole\\narchitecture. In the end it has 2 FC(fully connected layers) followed by a softmax for output. The 16 in\\nVGG16 refers to it has 16 layers that have weights. This network is a pretty large network and it has about\\n138 million (approx) parameters.\\nThe following are the layers of the model:\\nConvolutional Layers = 13\\nPooling Layers = 5\\nDense Layers = 3\\nLet us explore the layers in detail:\\n1. Input:  Image of dimensions (224, 224, 3).', metadata={'source': 'Deep Learning.pdf', 'page': 91}),\n",
       " Document(page_content='pg (, ,)\\n2. Convolution Layer Conv1:\\n- Conv1-1: 64 filters  \\n- Conv1-2: 64 filters and Max Pooling  \\n- Image dimensions: (224, 224)  \\n3. Convolution layer Conv2: Now, we increase the filters to 128\\n- Input Image dimensions: (112,112)  \\n- Conv2-1: 128 filters  \\n- Conv2-2: 128 filters and Max Pooling  \\n4. Convolution Layer Conv3:  Again, double the filters to 256, and now add another convolution layer\\n- Input Image dimensions: (56,56)  \\n- Conv3-1: 256 filters  \\n- Conv3-2: 256 filters  \\n- Conv3-3: 256 filters and Max Pooling  \\n5. Convolution Layer Conv4:  Similar to Conv3, but now with 512 filters\\n- Input Image dimensions: (28, 28)  \\n- Conv4-1: 512 filters  \\n- Conv4-2: 512 filters  \\n- Conv4-3: 512 filters and Max Pooling  \\n6. Convolution Layer Conv5:  Same as Conv4\\n- Input Image dimensions: (14, 14)  \\n- Conv5-1: 512 filters  \\n- Conv5-2: 512 filters  \\n- Conv5-3: 512 filters and Max Pooling  \\n- The output dimensions here are (7, 7). At this point, we flatten the output\\nof this layer to generate a feature vector  \\n7. Fully Connected/Dense FC1:  4096 nodes, generating a feature vector of size(1, 4096)\\n8. Fully ConnectedDense FC2:  4096 nodes generating a feature vector of size(1, 4096)\\n9. Fully Connected /Dense FC3:  4096 nodes, generating 1000 channels for 1000 classes. This is then\\npassed on to a Softmax activation function\\n10. Output layer\\nVGG16 contains 16 layers and VGG19 contains 19 layers. A series of VGGs are exactly the\\nsame in the last three fully connected layers. The overall structure includes 5 sets of\\nconvolutional layers, followed by a MaxPool. The dif ference is that more and more cascaded\\nconvolutional layers are included in the five sets of convolutional layers .', metadata={'source': 'Deep Learning.pdf', 'page': 92}),\n",
       " Document(page_content='Each convolutional layer in AlexNet contains only one convolution, and the size of the\\nconvolution kernel is 7 7 ,. In VGGNet, each convolution layer contains 2 to 4 convolution\\noperations. The size of the convolution kernel is 3 3, the convolution step size is 1, the pooling\\nkernel is 2 * 2, and the step size is 2. The most obvious improvement of VGGNet is to reduce\\nthe size of the convolution kernel and increase the number of convolution layers.\\nUsing multiple convolution layers with smaller convolution kernels instead of a larger\\nconvolution layer with convolution kernels can reduce parameters on the one hand, and the\\nauthor believes that it is equivalent to more non-linear mapping, which increases the Fit\\nexpression ability .', metadata={'source': 'Deep Learning.pdf', 'page': 93}),\n",
       " Document(page_content='Two consecutive 3 3 convolutions are equivalent to a 5 5 receptive field, and three are\\nequivalent to 7 7. The advantages of using three 3 3 convolutions instead of one 7 7\\nconvolution are twofold : one, including three ReLu layers instead of one , makes the decision\\nfunction more discriminative; and two, reducing parameters . For example, the input and output\\nare all C channels. 3 convolutional layers using 3 3 require 3 (3 3 C C) = 27 C C, and 1\\nconvolutional layer using 7 7 requires 7 7 C C = 49C C. This can be seen as applying a kind of\\nregularization to the 7 7 convolution, so that it is decomposed into three 3 3 convolutions.\\nThe 1 1 convolution layer is mainly to increase the non-linearity of the decision function without\\naffecting the receptive field of the convolution layer . Although the 1 1 convolution operation is\\nlinear , ReLu adds non-linearity .\\nSome basic questions\\nQ1: Why can 3 3x3 convolutions replace 7x7 convolutions?\\nAnswer 1\\n3 3x3 convolutions, using 3 non-linear activation functions, increasing non-linear expression capabilities,\\nmaking the segmentation plane more separable Reduce the number of parameters. For the convolution kernel\\nof C channels, 7x7 contains parameters , and the number of 3 3x3 parameters is greatly reduced.\\nQ2: The role of 1x1 convolution kernel\\nAnswer 2\\nIncrease the nonlinearity of the model without af fecting the receptive field 1x1 winding machine is equivalent to\\nlinear transformation, and the non-linear activation function plays a non-linear role\\nQ3: The effect of network depth on results (in the same year , Google also independently released the\\nnetwork GoogleNet with a depth of 22 layers)\\nAnswer 3', metadata={'source': 'Deep Learning.pdf', 'page': 94}),\n",
       " Document(page_content='VGG and GoogleNet models are deep Small convolution VGG only uses 3x3, while GoogleNet uses 1x1, 3x3,\\n5x5, the model is more complicated (the model began to use a large convolution kernel to reduce the\\ncalculation of the subsequent machine layer)\\nImplement On Keras\\nIn [3]:\\nIn [4]:#Importing library\\nimport keras\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Activation , Dropout, Flatten, Conv2D, MaxPooling2D ,MaxPool2\\nfrom keras.layers.normalization  import BatchNormalization\\nimport numpy as np\\n \\nnp.random.seed(1000)\\n \\nmodel = Sequential ()\\nmodel.add(Conv2D(input_shape =(224,224,3),filters=64,kernel_size =(3,3),padding=\"same\", activ\\nmodel.add(Conv2D(filters=64,kernel_size =(3,3),padding=\"same\", activation =\"relu\"))\\nmodel.add(MaxPool2D (pool_size =(2,2),strides=(2,2)))\\n \\n \\nmodel.add(Conv2D(filters=128, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(Conv2D(filters=128, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(MaxPool2D (pool_size =(2,2),strides=(2,2)))\\n \\n \\nmodel.add(Conv2D(filters=256, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(Conv2D(filters=256, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(Conv2D(filters=256, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(MaxPool2D (pool_size =(2,2),strides=(2,2)))\\n \\n \\nmodel.add(Conv2D(filters=512, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(Conv2D(filters=512, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(Conv2D(filters=512, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(MaxPool2D (pool_size =(2,2),strides=(2,2)))\\n \\n \\nmodel.add(Conv2D(filters=512, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(Conv2D(filters=512, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(Conv2D(filters=512, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(MaxPool2D (pool_size =(2,2),strides=(2,2)))\\n \\nmodel.add(Flatten())\\nmodel.add(Dense(units=4096,activation =\"relu\"))\\nmodel.add(Dense(units=4096,activation =\"relu\"))\\nmodel.add(Dense(units=2, activation =\"softmax\" ))', metadata={'source': 'Deep Learning.pdf', 'page': 95}),\n",
       " Document(page_content='In [6]:\\nHere I have started with initialising the model by specifying that the model is a sequential model. AfterThe output of this will be the summary of the model which I just created.  \\nModel: \"sequential_1\"  \\n_________________________________________________________________  \\nLayer (type)                 Output Shape              Param #    \\n=================================================================  \\nconv2d_2 (Conv2D)            (None, 224, 224, 64)      1792       \\n_________________________________________________________________  \\nconv2d_3 (Conv2D)            (None, 224, 224, 64)      36928      \\n_________________________________________________________________  \\nmax_pooling2d (MaxPooling2D) (None, 112, 112, 64)      0          \\n_________________________________________________________________  \\nconv2d_4 (Conv2D)            (None, 112, 112, 128)     73856      \\n_________________________________________________________________  \\nconv2d_5 (Conv2D)            (None, 112, 112, 128)     147584     \\n_________________________________________________________________  \\nmax_pooling2d_1 (MaxPooling2 (None, 56, 56, 128)       0          \\n_________________________________________________________________  \\nconv2d_6 (Conv2D)            (None, 56, 56, 256)       295168     \\n_________________________________________________________________  \\nconv2d_7 (Conv2D)            (None, 56, 56, 256)       590080     \\n_________________________________________________________________  \\nconv2d_8 (Conv2D)            (None, 56, 56, 256)       590080     \\n_________________________________________________________________  \\nmax_pooling2d_2 (MaxPooling2 (None, 28, 28, 256)       0          \\n_________________________________________________________________  \\nconv2d_9 (Conv2D)            (None, 28, 28, 512)       1180160    \\n_________________________________________________________________  \\nconv2d_10 (Conv2D)           (None, 28, 28, 512)       2359808    \\n_________________________________________________________________  \\nconv2d_11 (Conv2D)           (None, 28, 28, 512)       2359808    \\n_________________________________________________________________  \\nmax_pooling2d_3 (MaxPooling2 (None, 14, 14, 512)       0          \\n_________________________________________________________________  \\nconv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808    \\n_________________________________________________________________  \\nconv2d_13 (Conv2D)           (None, 14, 14, 512)       2359808    \\n_________________________________________________________________  \\nconv2d_14 (Conv2D)           (None, 14, 14, 512)       2359808    \\n_________________________________________________________________  \\nmax_pooling2d_4 (MaxPooling2 (None, 7, 7, 512)         0          \\n_________________________________________________________________  \\nflatten (Flatten)            (None, 25088)             0          \\n_________________________________________________________________  \\ndense (Dense)                (None, 4096)              102764544  \\n_________________________________________________________________  \\ndense_1 (Dense)              (None, 4096)              16781312   \\n_________________________________________________________________  \\ndense_2 (Dense)              (None, 2)                 8194       \\n=================================================================  \\nTotal params: 134,268,738  \\nTrainable params: 134,268,738  \\nNon-trainable params: 0  \\n_________________________________________________________________  print(\"The output of this will be the summary of the model which I just created.\" )\\nmodel.summary()', metadata={'source': 'Deep Learning.pdf', 'page': 96}),\n",
       " Document(page_content='initialising the model I add\\n→ 2 x convolution layer of 64 channel of 3x3 kernal and same padding\\n→ 1 x maxpool layer of 2x2 pool size and stride 2x2\\n→ 2 x convolution layer of 128 channel of 3x3 kernal and same padding\\n→ 1 x maxpool layer of 2x2 pool size and stride 2x2\\n→ 3 x convolution layer of 256 channel of 3x3 kernal and same padding\\n→ 1 x maxpool layer of 2x2 pool size and stride 2x2\\n→ 3 x convolution layer of 512 channel of 3x3 kernal and same padding\\n→ 1 x maxpool layer of 2x2 pool size and stride 2x2\\n→ 3 x convolution layer of 512 channel of 3x3 kernal and same padding\\n→ 1 x maxpool layer of 2x2 pool size and stride 2x2\\nI also add relu(Rectified Linear Unit) activation to each layers so that all the negative values are not passed to\\nthe next layer .\\nAfter creating all the convolution I pass the data to the dense layer so for that I flatten the vector which comes\\nout of the convolutions and add\\n→ 1 x Dense layer of 4096 units\\n→ 1 x Dense layer of 4096 units\\n→ 1 x Dense Softmax layer of 2 units\\nI will use RELU activation for both the dense layer of 4096 units so that I stop forwarding negative values\\nthrough the network. I use a 2 unit dense layer in the end with softmax activation as I have 2 classes to predict\\nfrom in the end which are dog and cat. The softmax layer will output the value between 0 and 1 based on the\\nconfidence of the model that which class the images belongs to.\\nAfter the creation of softmax layer the model is finally prepared.\\nIn [ ]:\\n ', metadata={'source': 'Deep Learning.pdf', 'page': 97}),\n",
       " Document(page_content='Introduction\\nResNet is a network structure proposed by the He Kaiming, Sun Jian and others of Microsoft Research\\nAsia in 2015, and won the first place in the ILSVRC-2015 classification task. At the same time, it won the\\nfirst place in ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation\\ntasks. It was a sensation at the time.\\nResNet, also known as residual neural network, refers to the idea of   adding residual learning to the traditional\\nconvolutional neural network, which solves the problem of gradient dispersion and accuracy degradation\\n(training set) in deep networks, so that the network can get more and more The deeper , both the accuracy and\\nthe speed are controlled.\\nDeep Residual Learning for Image Recognition Original link : ResNet Paper\\n(https://arxiv .org/pdf/1512.03385.pdf)\\nThe problem caused by increasing depth\\nThe first problem brought by increasing depth is the problem of gradient explosion / dissipation . This is\\nbecause as the number of layers increases, the gradient of backpropagation in the network will become\\nunstable with continuous multiplication, and become particularly large or special. small. Among them , the\\nproblem of gradient dissipation often occurs .\\nIn order to overcome gradient dissipation, many solutions have been devised, such as using BatchNorm,\\nreplacing the activation function with ReLu, using Xaiver initialization, etc. It can be said that gradient\\ndissipation has been well solved\\nAnother problem of increasing depth is the problem of network degradation, that is, as the depth increases,\\nthe performance of the network will become worse and worse, which is directly reflected in the decrease in\\naccuracy on the training set. The residual network article solves this problem. And after this problem is\\nsolved, the depth of the network has increased by several orders of magnitude.\\nDegradation of deep network\\nWith network depth increasing, accuracy gets saturated (which might be unsurprising) and then\\ndegrades rapidly . Unexpectedly , such degradation is not caused by overfitting, and adding more\\nlayers to a favored deep model leads to higher training error .', metadata={'source': 'Deep Learning.pdf', 'page': 98}),\n",
       " Document(page_content='The above figure is the error rate of the training set classified by the network on the CIF AR10-data set with\\nthe increase of the network depth . It can be seen that if we directly stack the convolutional layers, as the\\nnumber of layers increases, the error rate increases significantly . The trend is that the deepest 56-layer\\nnetwork has the worst accuracy . W e verified it on the VGG network. For the CIF AR-10 dataset, it took 5\\nminutes on the 18-layer VGG network to get the full network training. The 80% accuracy rate was\\nachieved, and the 34-layer VGG model took 8 minutes to get the 72% accuracy rate. The problem of\\nnetwork degradation does exist.\\nThe decrease in the training set error rate indicates that the problem of degradation is not caused by\\noverfitting. The specific reason is that it is left for further study . The author\\'s other paper \"Identity Mappings\\nin Deep Residual Networks\" proved the occurrence of degradation. It is because the optimization\\nperformance is not good, which indicates that the deeper the network, the more dif ficult the reverse\\ngradient is to conduct.\\nDeep Residual Networks\\nFrom 10 to 100 layers\\nWe can imagine that when we simply stack the network directly to a particularly long length, the internal\\ncharacteristics of the network have reached the best situation in one of the layers. At this time, the remaining\\nlayers should not make any changes to the characteristics and learn automatically . The form of identity\\nmapping . That is to say , for a particularly deep deep network, the solution space of the shallow form of the\\nnetwork should be a subset of the solution space of the deep network, in other words, a network deeper than\\nthe shallow network will not have at least W orse ef fect, but this is not true because of network degradation.\\nThen, we settle for the second best. In the case of network degradation, if we do not add depth, we can improve\\nthe accuracy . Can we at least make the deep network achieve the same performance as the shallow network,\\nthat is, let the layers behind the deep network achieve at least The role of identity mapping . Based on this idea,\\nthe author proposes a residual module to help the network achieve identity mapping.', metadata={'source': 'Deep Learning.pdf', 'page': 99}),\n",
       " Document(page_content='To understand ResNet, we must first understand what kind of problems will occur when the  \\nnetwork becomes deeper.\\nThe first problem brought by increasing the network depth is the disappearance and explosion of the\\ngradient.\\nThis problem was successfully solved after Szegedy proposed the BN (Batch Normalization)  structure. The\\nBN layer can normalize the output of each layer . The size can still be kept stable after the reverse layer transfer ,\\nand it will not be too small or too large.\\nIs it easy to converge after adding BN and then increasing the depth?\\nThe answer is still negative . The author mentioned the second problem- the degradation problem : when the\\nlevel reaches a certain level, the accuracy will saturate and then decline rapidly . This decline is not caused by\\nthe disappearance of the gradient. It is not caused by overfit, but because the network is so complicated that it\\nis difficult to achieve the ideal error rate by unconstrained stocking training alone.\\nThe degradation problem is not a problem of the network structure itself, but is caused by the current insuf ficient\\ntraining methods. The currently widely used training methods, whether it is SGD, AdaGrad, or RMSProp,\\ncannot reach the theoretically optimal convergence result after the network depth becomes larger .\\nWe can also prove that as long as there is an ideal training method, deeper networks will definitely perform\\nbetter than shallow networks.\\nThe proof process is also very simple : Suppose that several layers are added behind a network A  \\nto form a new network B. If the added level is just an identity mapping of the output of  \\nA, that is, the output of A is after the level of B becomes the output of B, there is no  \\nchange, so the error rates of network A and network B are equal, which proves that the  \\ndeepened network will not be worse than the network before deepening.\\nHe Kaiming proposed a residual structure to implement the above identity mapping (Below Figure): In addition\\nto the normal convolution layer output, the entire module has a branch directly connecting the input to the\\noutput. The output and the output of the convolution do The final output is obtained by arithmetic addition. The\\nformula is H (x) = F (x) + x, x is the input, F (x) is the output of the convolution branch, and H (x) is the output of\\nthe entire structure. It can be shown that if all parameters in the F (x) branch are 0, H (x) is an identity mapping.\\nThe residual structure artificially creates an identity map, which can make the entire structure converge in the\\ndirection of the identity map, ensuring that the final error rate will not become worse because the depth\\nbecomes larger . If a network can achieve the desired result by simply setting the parameter values by hand,\\nthen this structure can easily converge to the result through training. This is a rule that is unsuccessful when\\ndesigning complex networks. Recall that in order to restore the original distribution after BN processing, the\\nformula y = rx + delta is used. When r is manually set to standard deviation and delta is the mean, y is the\\ndistribution before BN processing. This is the use of this Rules.', metadata={'source': 'Deep Learning.pdf', 'page': 100}),\n",
       " Document(page_content='What does residual learning mean?\\nThe idea of residual learning is the above picture, which can be understood as a block, defined as follows:\\nThe residual learning block contains two branches or two mappings:\\n1. Identity mapping refers to the curved curve on the right side of the figure above. As its name implies,\\nidentity mapping refers to its own mapping, which is x itself;\\n1. F(x) Residual mapping refers to another branch, that is, part. This part is called residual mapping (y -x) .\\nWhat role does the residual module play in back propagation?\\nThe residual module will significantly reduce the parameter value in the module, so that the parameters in\\nthe network have a more sensitive response ability to the loss of reverse conduction, although the\\nfundamental It does not solve the problem that the loss of backhaul is too small, but it reduces the\\nparameters. Relatively speaking, it increases the ef fect of backhaul loss and also generates a certain\\nregularization ef fect.\\nSecondly , because there are branches of the identity mapping in the forward process, the gradient\\nconduction in the back-propagation process also has more simple paths , and the gradient can be\\ntransmitted to the previous module after only one relu.\\nThe so-called backpropagation is that the network outputs a value, and then compares it with the real value\\nto an error loss. At the same time, the loss is changed to change the parameter . The returned loss depends\\non the original loss and gradient. Since the purpose is to change the parameter , The problem is that if the\\nintensity of changing the parameter is too small, the value of the parameter can be reduced, so that the\\nloss of the intensity of changing the parameter is relatively greater .\\nTherefore, the most important role of the residual module is to change the way of forward and backward\\ninformation transmission, thereby greatly promoting the optimization of the network.\\nUsing the four criteria proposed by Inceptionv3, we will use them again to improve the residual module.\\nUsing criterion 3, the dimensionality reduction before spatial aggregation will not cause information loss, so\\nthe same method is also used here, adding 1 * 1 convolution The kernel is used to increase the non-\\nlinearity and reduce the depth of the output to reduce the computational cost. Y ou get the form of a residual\\nmodule that becomes a bottleneck. The figure above shows the basic form on the left and the bottleneck\\nform on the right.', metadata={'source': 'Deep Learning.pdf', 'page': 101}),\n",
       " Document(page_content='To sum up, the shortcut module will help the features in the network perform identity mapping in the\\nforward process, and help conduct gradients in the reverse process, so that deeper models can be\\nsuccessfully trained.\\nWhy can the residual learning solve the problem of \"the accuracy of the network deepening\\ndeclines\"?\\nFor a neural network model, if the model is optimal, then training can easily optimize the residual mapping to 0,\\nand only identity mapping is left at this time. No matter how you increase the depth, the network will always be\\nin an optimal state in theory . Because it is equivalent to all the subsequent added networks to carry information\\ntransmission along the identity mapping (self), it can be understood that the number of layers behind the\\noptimal network is discarded (without the ability to extract features), and it does not actually play a role. . In this\\nway, the performance of the network will not decrease with increasing depth.\\nThe author used two types of data, ImageNet  and CIFAR, to prove the ef fectiveness of ResNet:\\nThe first is ImageNet. The authors compared the training ef fect of ResNet structure and traditional structure with\\nthe same number of layers. The left side of Figure is a VGG-19 network with a traditional structure (each\\nfollowed by BN), the middle is a 34-layer network with a traditional structure (each followed by BN), and the\\nright side is 34 layers ResNet (the solid line indicates a direct connection, and the dashed line indicates a\\ndimensional change using 1x1 convolution to match the number of features of the input and output). Figure 3\\nshows the results after training these types of networks.\\nThe data on the left shows that the 34-layer network (red line) with the traditional structure has a higher error\\nrate than the VGG-19 (blue-green line). Because the BN structure is added to each layer Therefore, the high\\nerror is not caused by the gradient disappearing after the level is increased, but by the degradation problem; the\\nResNet structure on the right side of Figure 3 shows that the 34-layer network (red line) has a higher error rate\\nthan the 18-layer network (blue-green line). Low , this is because the ResNet structure has overcome the\\ndegradation problem. In addition, the final error rate of the ResNet 18-layer network on the right is similar to the\\nerror rate of the traditional 18-layer network on the left. This is because the 18-layer network is simpler and can\\nconverge to a more ideal result even without the ResNet structure.', metadata={'source': 'Deep Learning.pdf', 'page': 102}),\n",
       " Document(page_content='', metadata={'source': 'Deep Learning.pdf', 'page': 103}),\n",
       " Document(page_content='The ResNet structure like the left side of Fig. 4 is only used for shallow ResNet networks. If there are many\\nnetwork layers, the dimensions near the output end of the network will be very large. Still using the structure on\\nthe left side of Fig. 4 will cause a huge amount of calculation. For deeper networks, we all use the bottleneck\\nstructure on the right side of Figure 4, first using a 1x1 convolution for dimensionality reduction, then 3x3\\nconvolution, and finally using 1x1 dimensionality to restore the original dimension.\\nIn practice, considering the cost of the calculation, the residual block is calculated and optimized, that is, the\\ntwo 3x3 convolution layers are replaced with 1x1 + 3x3 + 1x1 , as shown below . The middle 3x3 convolutional\\nlayer in the new structure first reduces the calculation under one dimensionality-reduced 1x1 convolutional layer\\n, and then restores it under another 1x1 convolutional layer , both maintaining accuracy and reducing the\\namount of calculation .\\nThis is equivalent to reducing the amount of parameters for the same number of layers , so it can be extended\\nto deeper models. So the author proposed ResNet with 50, 101 , and 152 layers , and not only did not have\\ndegradation problems, the error rate was greatly reduced, and the computational complexity was also kept at a\\nvery low level .', metadata={'source': 'Deep Learning.pdf', 'page': 104}),\n",
       " Document(page_content='At this time, the error rate of ResNet has already dropped other networks a few streets, but it does not seem to\\nbe satisfied. Therefore, a more abnormal 1202 layer network has been built. For such a deep network,\\noptimization is still not dif ficult, but it appears The problem of overfitting is quite normal. The author also said\\nthat the 1202 layer model will be further improved in the future.\\nThere are two main types of blocks are used in a ResNet, depending mainly on\\nwhether the input/output dimensions are the same or different.\\n1. Identity Block\\nThe identity block is the standard block used in ResNets and corresponds to the case where the input activation\\nhas the same dimension as the output activation.\\n2. Convolutional Block\\nWe can use this type of block when the input and output dimensions don’t match up. The dif ference with the\\nidentity block is that there is a CONV2D layer in the shortcut path.\\nResNet-50\\nThe ResNet-50 model consists of 5 stages each with a convolution and Identity block. Each convolution block\\nhas 3 convolution layers and each identity block also has 3 convolution layers. The ResNet-50 has over 23\\nmillion trainable parameters.\\nDiffrent V ariants  : -', metadata={'source': 'Deep Learning.pdf', 'page': 105}),\n",
       " Document(page_content='Below is the transcript of resnet, winning the championship at ImageNet2015\\nCode Implement\\nIn [1]:\\nimport cv2\\nimport numpy as np\\nimport os\\nfrom keras.preprocessing .image import ImageDataGenerator\\nfrom keras import backend as K\\nimport keras\\nfrom keras.models import Sequential , Model,load_model\\nfrom keras.optimizers  import SGD\\nfrom keras.callbacks  import EarlyStopping ,ModelCheckpoint\\nfrom keras.layers import Input, Add, Dense, Activation , ZeroPadding2D , BatchNormalization , \\nfrom keras.preprocessing  import image\\nfrom keras.initializers  import glorot_uniform', metadata={'source': 'Deep Learning.pdf', 'page': 106}),\n",
       " Document(page_content=\"In [2]:\\nIn [3]:def identity_block (X, f, filters, stage, block):\\n   \\n    conv_name_base  = 'res' + str(stage) + block + '_branch'\\n    bn_name_base  = 'bn' + str(stage) + block + '_branch'\\n    F1, F2, F3 = filters\\n \\n    X_shortcut  = X\\n   \\n    X = Conv2D(filters=F1, kernel_size =(1, 1), strides=(1, 1), padding='valid', name=conv_n\\n    X = BatchNormalization (axis=3, name=bn_name_base  + '2a')(X)\\n    X = Activation ('relu')(X)\\n \\n    X = Conv2D(filters=F2, kernel_size =(f, f), strides=(1, 1), padding='same', name=conv_na\\n    X = BatchNormalization (axis=3, name=bn_name_base  + '2b')(X)\\n    X = Activation ('relu')(X)\\n \\n    X = Conv2D(filters=F3, kernel_size =(1, 1), strides=(1, 1), padding='valid', name=conv_n\\n    X = BatchNormalization (axis=3, name=bn_name_base  + '2c')(X)\\n \\n    X = Add()([X, X_shortcut ])# SKIP Connection\\n    X = Activation ('relu')(X)\\n \\n    return X\\ndef convolutional_block (X, f, filters, stage, block, s=2):\\n   \\n    conv_name_base  = 'res' + str(stage) + block + '_branch'\\n    bn_name_base  = 'bn' + str(stage) + block + '_branch'\\n \\n    F1, F2, F3 = filters\\n \\n    X_shortcut  = X\\n \\n    X = Conv2D(filters=F1, kernel_size =(1, 1), strides=(s, s), padding='valid', name=conv_n\\n    X = BatchNormalization (axis=3, name=bn_name_base  + '2a')(X)\\n    X = Activation ('relu')(X)\\n \\n    X = Conv2D(filters=F2, kernel_size =(f, f), strides=(1, 1), padding='same', name=conv_na\\n    X = BatchNormalization (axis=3, name=bn_name_base  + '2b')(X)\\n    X = Activation ('relu')(X)\\n \\n    X = Conv2D(filters=F3, kernel_size =(1, 1), strides=(1, 1), padding='valid', name=conv_n\\n    X = BatchNormalization (axis=3, name=bn_name_base  + '2c')(X)\\n \\n    X_shortcut  = Conv2D(filters=F3, kernel_size =(1, 1), strides=(s, s), padding='valid', na\\n    X_shortcut  = BatchNormalization (axis=3, name=bn_name_base  + '1')(X_shortcut )\\n \\n    X = Add()([X, X_shortcut ])\\n    X = Activation ('relu')(X)\\n \\n    return X\", metadata={'source': 'Deep Learning.pdf', 'page': 107}),\n",
       " Document(page_content=\"In [4]:\\nIn [6]:\\nIn [7]:\\nIn [8]:def ResNet50 (input_shape =(224, 224, 3)):\\n \\n    X_input = Input(input_shape )\\n \\n    X = ZeroPadding2D ((3, 3))(X_input)\\n \\n    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer =glorot_uniform (\\n    X = BatchNormalization (axis=3, name='bn_conv1' )(X)\\n    X = Activation ('relu')(X)\\n    X = MaxPooling2D ((3, 3), strides=(2, 2))(X)\\n \\n    X = convolutional_block (X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\\n    X = identity_block (X, 3, [64, 64, 256], stage=2, block='b')\\n    X = identity_block (X, 3, [64, 64, 256], stage=2, block='c')\\n \\n \\n    X = convolutional_block (X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\\n    X = identity_block (X, 3, [128, 128, 512], stage=3, block='b')\\n    X = identity_block (X, 3, [128, 128, 512], stage=3, block='c')\\n    X = identity_block (X, 3, [128, 128, 512], stage=3, block='d')\\n \\n    X = convolutional_block (X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\\n    X = identity_block (X, 3, [256, 256, 1024], stage=4, block='b')\\n    X = identity_block (X, 3, [256, 256, 1024], stage=4, block='c')\\n    X = identity_block (X, 3, [256, 256, 1024], stage=4, block='d')\\n    X = identity_block (X, 3, [256, 256, 1024], stage=4, block='e')\\n    X = identity_block (X, 3, [256, 256, 1024], stage=4, block='f')\\n \\n    X = X = convolutional_block (X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\\n    X = identity_block (X, 3, [512, 512, 2048], stage=5, block='b')\\n    X = identity_block (X, 3, [512, 512, 2048], stage=5, block='c')\\n \\n    X = AveragePooling2D (pool_size =(2, 2), padding='same')(X)\\n    \\n    model = Model(inputs=X_input, outputs=X, name='ResNet50' )\\n \\n    return model\\nbase_model  = ResNet50 (input_shape =(224, 224, 3))\\nheadModel  = base_model .output\\nheadModel  = Flatten()(headModel )\\nheadModel =Dense(256, activation ='relu', name='fc1',kernel_initializer =glorot_uniform (seed=0\\nheadModel =Dense(128, activation ='relu', name='fc2',kernel_initializer =glorot_uniform (seed=0\\nheadModel  = Dense( 1,activation ='sigmoid' , name='fc3',kernel_initializer =glorot_uniform (see\\nmodel = Model(inputs=base_model .input, outputs=headModel )\", metadata={'source': 'Deep Learning.pdf', 'page': 108}),\n",
       " Document(page_content='In [9]:\\nIn [ ]:Model: \"model\"  \\n__________________________________________________________________________\\n________________________  \\nLayer (type)                    Output Shape         Param #     Connected  \\nto                      \\n==========================================================================\\n========================  \\ninput_1 (InputLayer)            [(None, 224, 224, 3) 0                     \\n__________________________________________________________________________\\n________________________  \\nzero_padding2d (ZeroPadding2D)  (None, 230, 230, 3)  0           input_1\\n[0][0]                     \\n__________________________________________________________________________\\n________________________  \\nconv1 (Conv2D)                  (None, 112, 112, 64) 9472        zero_padd\\ning2d[0][0]              \\n__________________________________________________________________________\\n________________________  \\nbn_conv1 (BatchNormalization)   (None, 112, 112, 64) 256         conv1[0]\\n[0]model.summary()\\n ', metadata={'source': 'Deep Learning.pdf', 'page': 109}),\n",
       " Document(page_content='Introduction\\nInception net achieved a milestone in CNN classifiers when previous models were just going deeper to\\nimprove the performance and accuracy but compromising the computational cost. The Inception network,\\non the other hand, is heavily engineered.\\nIt uses a lot of tricks to push performance, both in terms of speed and accuracy . It is the winner of the\\nImageNet Large Scale V isual Recognition Competition in 2014, an image classification competition, which\\nhas a significant improvement over ZFNet (The winner in 2013), AlexNet (The winner in 2012) and has\\nrelatively lower error rate compared with the VGGNet (1st runner-up in 2014).\\nThe major issues faced by deeper CNN models such as VGGNet were:\\n1. Although, previous networks such as VGG achieved a remarkable accuracy on the ImageNet dataset,\\ndeploying these kinds of models is highly computationally expensive because of the deep architecture.\\n2. Very deep networks are susceptible to overfitting. It is also hard to pass gradient updates through the entire\\nnetwork.\\nMotivation for Inception Network\\nInstead of deciding whether to use a 1x1 convolution, or a 3x3 or a 5x5 Convolution, or whether to use a\\nPooling layer - Why not use all of them?\\nIn the example above, all the filters are applied to the input to generate a stacked outpu, whioch contains the\\noutput of each filter stacked on top of each other . The Padding is kept at ‘same’ to ensure that the output from\\nall the filters are of the same size.\\nDisadvantage: Huge memory cost\\nSolving the problem of memory cost\\nFor example, the computational cost of th 5x5 filer in the above diagram:', metadata={'source': 'Deep Learning.pdf', 'page': 110}),\n",
       " Document(page_content='Input: 28x28x192\\nFilter: Conv 5x5x192, same, 32\\nOutput: 28x28x32\\nTotal number of calculations = (28 28 32) (5 5 * 192 ) = 120 Million !!\\nUsing 1x1 Convolution to reduce computation cost\\nA 1x1 convolution is added before the 5x5 cvonvolution -= Also called a bottleneck layer\\nTotal number of calculations = [(28 28 16) (1 1 192)] + [(28 28 32) (5 5 16)] = 12.4 Million !! (earlier the cost was\\n120 Million)', metadata={'source': 'Deep Learning.pdf', 'page': 111}),\n",
       " Document(page_content='The popular versions are as follows:\\nInception v1.\\nInception v2 and Inception v3.\\nInception v4 and Inception-ResNet.\\nInception V1\\nThis architecture has 22 layers in total! Using the dimension-reduced inception module, a neural network\\narchitecture is constructed. This is popularly known as GoogLeNet (Inception v1).\\nGoogLeNet has 9 such inception modules fitted linearly . It is 22 layers deep (27, including the pooling\\nlayers). At the end of the architecture, fully connected layers were replaced by a global average pooling\\nwhich calculates the average of every feature map. This indeed dramatically declines the total number of\\nparameters.\\nThe above are the explainof V1\\nProblems of Inception V1 architecture:', metadata={'source': 'Deep Learning.pdf', 'page': 112}),\n",
       " Document(page_content='Inception V1 have sometimes use convolutions such as 5*5 that causes the input dimensions to decrease\\nby a large margin. This causes the neural network some accuracy decrease. The reason behind that the\\nneural network is susceptible to information loss if the input dimension decreases too drastically .\\nFurthermore, there is also complexity decrease when we use bigger convolutions like 5×5 as compared to\\n3×3.W e can go further in terms of factorization i.e. that we can divide a 3×3 convolution into an asymmetric\\nconvolution of 1×3 then followed by 3×1 convolution. This is equivalent to sliding a two-layer network with\\nthe same receptive field as in a 3×3 convolution but 33% more cheaper than 3×3. This factorization does\\nnot work well for early layers when input dimensions are big but only when the input size mxm (m is\\nbetween 12 and 20). According to the Inception V1 architecture, the auxiliary classifier improves the\\nconvergence of the network. They argue that it can help reduce the ef fect of the vanishing gradient problem\\nin deep network by pushing the useful gradient to earlier layers (to reduce the loss). But, the authors of this\\npaper found that this classifier didn’t improve the convergence very much early in the training.\\nInception V2 And Inception V3\\nInception-v2(2015)\\nArchitectural Changes in Inception V2:\\nIn the Inception V2 architecture. The 5×5 convolution is replaced by the two 3×3 convolutions. This also\\ndecreases computational time and thus increase computational speed because a 5×5 convolution is 2.78 more\\nexpensive than 3×3 convolution. So, Using two 3×3 layers instead of 5×5 increases the performance of\\narchitecture.\\nThis architecture also converts nXn factorization into 1xn and nx1 factorization. As we discuss above that a 3×3\\nconvolution can be converted into 1×3 then followed by 3×1 convolution which is 33% cheaper in terms of\\ncomputational complexity as compared to 3×3.', metadata={'source': 'Deep Learning.pdf', 'page': 113}),\n",
       " Document(page_content='To deal with the problem of the representational bottleneck, the feature banks of the module were expanded\\ninstead of making it deeper . This would prevent the loss of information that causes when we make it deeper .\\nThe above three principles were used to build three dif ferent types of inception modules (Let’ s call them\\nmodules A,B and C in the order they were introduced. These names are introduced for clarity , and not the\\nofficial names). The architecture is as follows:', metadata={'source': 'Deep Learning.pdf', 'page': 114}),\n",
       " Document(page_content='Algorithm advantages:\\n1. Improved learning rate  : In the BN model, a higher learning rate is used to accelerate training\\nconvergence, but it will not cause other ef fects. Because if the scale of each layer is dif ferent, then the\\nlearning rate required by each layer is dif ferent. The scale of the same layer dimension often also needs\\ndifferent learning rates. Usually , the minimum learning is required to ensure the loss function to decrease,\\nbut The BN layer keeps the scale of each layer and dimension consistent, so you can directly use a higher\\nlearning rate for optimization.\\n1. Remove the dropout layer  : The BN layer makes full use of the goals of the dropout layer . Remove the\\ndropout layer from the BN-Inception model, but no overfitting will occur .\\n1. Decrease the attenuation coefficient of L2 weight  : Although the L2 loss controls the overfitting of the\\nInception model, the loss of weight has been reduced by five times in the BN-Inception model.\\n1. Accelerate the decay of the learning rate  : When training the Inception model, we let the learning rate\\ndecrease exponentially . Because our network is faster than Inception, we will increase the speed of\\nreducing the learning rate by 6 times.\\n1. Remove the local response layer  : Although this layer has a certain role, but after the BN layer is added,\\nthis layer is not necessary .', metadata={'source': 'Deep Learning.pdf', 'page': 115}),\n",
       " Document(page_content='1. Scramble training samples more thoroughly  : We scramble training samples, which can prevent the\\nsame samples from appearing in a mini-batch. This can improve the accuracy of the validation set by 1%,\\nwhich is the advantage of the BN layer as a regular term. In our method, random selection is more ef fective\\nwhen the model sees dif ferent samples each time.\\n1. To reduce image distortion : Because BN network training is faster and observes each training sample\\nless often, we want the model to see a more realistic image instead of a distorted image.\\nInception-v3-2015\\nThis architecture focuses, how to use the convolution kernel two or more smaller size of the convolution kernel\\nto replace, but also the introduction of asymmetrical layers i.e. a convolution dimensional convolution  has\\nalso been proposed for pooling layer Some remedies that can cause loss of spatial information; there are ideas\\nsuch as label-smoothing , BN-ahxiliary  .\\nExperiments were performed on inputs with dif ferent resolutions . The results show that although low-resolution\\ninputs require more time to train, the accuracy and high-resolution achieved are not much dif ferent.\\nThe computational cost is reduced while improving the accuracy of the network.\\nGeneral Design Principles\\nWe will describe some design principles that have been proposed through extensive experiments with dif ferent\\narchitectural designs for convolutional networks. At this point, full use of the following principles can be\\nguessed, and some additional experiments in the future will be necessary to estimate their accuracy and\\neffectiveness.\\n1. Prevent bottlenecks in characterization  . The so-called bottleneck of feature description is that a large\\nproportion of features are compressed in the middle layer (such as using a pooling operation). This\\noperation will cause the loss of feature space information and the loss of features. Although the operation\\nof pooling in CNN is important, there are some methods that can be used to avoid this loss as much as\\npossible (I note: later hole convolution operations ).\\n2. The higher the dimensionality of the feature, the faster the training converges  . That is, the\\nindependence of features has a great relationship with the speed of model convergence. The more\\nindependent features, the more thoroughly the input feature information is decomposed. It is easier to\\nconverge if the correlation is strong. Hebbin principle : fire together , wire together .\\n3. Reduce the amount of calculation through dimensionality reduction  . In v1, the feature is first reduced\\nby 1x1 convolutional dimensionality reduction. There is a certain correlation between dif ferent dimensions.\\nDimension reduction can be understood as a lossless or low-loss compression. Even if the dimensions are\\nreduced, the correlation can still be used to restore its original information.\\n4. Balance the depth and width of the network  . Only by increasing the depth and width of the network in\\nthe same proportion can the performance of the model be maximized.\\nInception V3 is similar to and contains all the features of Inception V2 with following changes/additions:\\n   1. Use of RMSprop optimizer.  \\n   2. Batch Normalization in the fully connected layer of Auxiliary classifier.  \\n   3. Use of 7×7 factorized Convolution  \\n   4. Label Smoothing Regularization: It is a method to regularize the classifier  \\nby estimating the effect of label-dropout during training. It prevents the classif\\nier to predict too confidently a class. The addition of label smoothing gives 0.2%  \\nimprovement from the error rate.  ', metadata={'source': 'Deep Learning.pdf', 'page': 116}),\n",
       " Document(page_content='Inception-v4-2016\\nInception V4 was introduced in combination with Inception-ResNet by thee researchers a Google in 2016.\\nThe main aim of the paper was to reduce the complexity of Inception V3 model which give the state-of-the-\\nart accuracy on ILSVRC 2015 challenge. This paper also explores the possibility of using residual networks\\non Inception model.\\nIntroduction\\nResidual conn works well when training very deep networks. Because the Inception network architecture can be\\nvery deep, it is reasonable to use residual conn instead of concat.\\nCompared with v3, Inception-v4 has more unified simplified structure and more inception modules.\\nThe big picture of Inception-v4:', metadata={'source': 'Deep Learning.pdf', 'page': 117}),\n",
       " Document(page_content='Fig9 is an overall picture, and Fig3,4,5,6,7,8 are all local structures. For the specific structure of each module,\\nsee the end of the article.\\nResidual Inception Blocks\\nFor the residual version in the Inception network, we use an Inception module that consumes less than the\\noriginal Inception. The convolution kernel (followed by 1x1) of each Inception module is used to modify the\\ndimension, which can compensate the reduction of the Inception dimension to some extent.\\nOne is named Inception-ResNet-v1 , which is consistent with the calculation cost of Inception-v3. One is\\nnamed Inception-ResNet-v2 , which is consistent with the calculation cost of Inception-v4.\\nFigure 15 shows the structure of both. However , Inception-v4 is actually slower in practice, probably because it\\nhas more layers.', metadata={'source': 'Deep Learning.pdf', 'page': 118}),\n",
       " Document(page_content='Another small technique is that we use the BN layer in the header of the traditional layer in the Inception-\\nResNet module, but not in the header of the summations. ** There is reason to believe that the BN layer is\\neffective. But in order to add more Inception modules, we made a compromise between the two.\\nInception-ResNet-v1\\nInception-ResNet-v2\\nScaling of the Residuals\\nThis paper finds that when the number of convolution kernels exceeds 1,000 , the residual variants will start to\\nshow instability , and the network will die in the early stages of training, which means that the last layer before\\nthe average pooling layer is in the V ery few iterations start with just a zero value . This situation cannot be', metadata={'source': 'Deep Learning.pdf', 'page': 119}),\n",
       " Document(page_content=\"prevented by reducing the learning rate or by adding a BN layer . Hekaiming's ResNet article also mentions this\\nphenomenon.\\nThis article finds that scale can stabilize the training process before adding the residual module to the activation\\nlayer . This article sets the scale coef ficient between 0.1 and 0.3.\\nIn order to prevent the occurrence of unstable training of deep residual networks, He suggested in the article\\nthat it is divided into two stages of training. The first stage is called warm-up (preheating) , that is, training the\\nmodel with a very low learning first. In the second stage, a higher learning rate is used. And this article finds\\nthat if the convolution sum is very high, even a learning rate of 0.00001 cannot solve this training instability\\nproblem, and the high learning rate will also destroy the ef fect. But this article considers scale residuals to be\\nmore reliable than warm-up.\\nEven if scal is not strictly necessary , it has no ef fect on the final accuracy , but it can stabilize the training\\nprocess.\\nConclusion\\nInception-ResNet-v1 : a network architecture combining inception module and resnet module with similar\\ncalculation cost to Inception-v3;\\nInception-ResNet-v2 : A more expensive but better performing network architecture.\\nInception-v4 : A pure inception module, without residual connections, but with performance similar to Inception-\\nResNet-v2.\", metadata={'source': 'Deep Learning.pdf', 'page': 120}),\n",
       " Document(page_content='Q. Why do we use an RNN instead of a simple neural network?\\nWe use Recurrent Neural Networks mostly in sequential data.W e use RNN over standard neural networks due\\nto the following reasons :\\nIn case of sequential data, the inputs and outputs can be of dif ferent lengths. For e.g. , in sentiment\\nanalysis, we map the input sentences to one number describing the sentiment of the text.\\nStandard neural network does not share features learnt across dif ferent positions of text. For e.g. , in\\nnamed entity recognition(identifying names of person in sentences), suppose we identify Henry occurs in\\nfirst position as name, we would want the algorithm to use this information in case Henry occurs later again\\nin the sentence. W e want things learnt in one part to generalize in others parts in sequence data.\\nThe parameters required for handling text will be very large in case of Standard neural networks. RNN\\nrequires much less parameters to learn.\\nRecurrent Neural Network\\nRecurrent Neural Network(RNN) are a type of Neural Network where the output from previous step are fed\\nas input to the current step.\\nIn traditional neural networks, all the inputs and outputs are independent of each other , but in cases like\\nwhen it is required to predict the next word of a sentence, the previous words are required and hence there\\nis a need to remember the previous words.\\nThus RNN came into existence, which solved this issue with the help of a Hidden Layer . The main and\\nmost important feature of RNN is Hidden state, which remembers some information about a sequence.\\nRNN have a “memory” which remembers all information about what has been calculated. It uses the same\\nparameters for each input as it performs the same task on all the inputs or hidden layers to produce the\\noutput.\\nThis reduces the complexity of parameters, unlike other neural networks.\\n', metadata={'source': 'Deep Learning.pdf', 'page': 121}),\n",
       " Document(page_content='Types of RNN\\nOne-to-one:\\nThis is also called Plain Neural networks. It deals with a fixed size of the input to the fixed size of output, where\\nthey are independent of previous information/output.\\nExample: Image classification.\\nOne-to-Many:\\nIt deals with a fixed size of information as input that gives a sequence of data as output.\\nExample: Image Captioning takes the image as input and outputs a sentence of words.\\nMany-to-One:\\nIt takes a sequence of information as input and outputs a fixed size of the output.\\nExample: sentiment analysis where any sentence is classified as expressing the positive or negative sentiment.\\nMany-to-Many:\\nIt takes a Sequence of information as input and processes the recurrently outputs as a Sequence of data.\\nExample: Machine T ranslation, where the RNN reads any sentence in English and then outputs the sentence in\\nFrench.\\nBidirectional Many-to-Many:\\nSynced sequence input and output. Notice that in every case are no pre-specified constraints on the lengths\\nsequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.\\nExample: V ideo classification where we wish to label every frame of the video.', metadata={'source': 'Deep Learning.pdf', 'page': 122}),\n",
       " Document(page_content=\"S.no CNN RNN\\n1 CNN  stands for Convolutional Neural Network .RNN  stands for Recurrent Neural\\nNetwork .\\n2 CNN is considered to be more potent than RNN.RNN includes less feature\\ncompatibility when compared to CNN.\\n3 CNN is ideal for images and video processing.RNN is ideal for text and speech\\nAnalysis.\\n4 It is suitable for spatial data like images.RNN is used for temporal data, also\\ncalled sequential data.\\n5 The network takes fixed-size inputs and generates fixed size outputs.RNN can handle arbitrary input/\\noutput lengths.\\n6CNN is a type of feed-forward artificial neural network with variations of\\nmultilayer perceptron's designed to use minimal amounts of preprocessing.RNN, unlike feed-forward neural\\nnetworks- can use their internal\\nmemory to process arbitrary\\nsequences of inputs.\\n7CNN's use of connectivity patterns between the neurons. CNN is af fected by the\\norganization of the animal visual cortex , whose individual neurons are\\narranged in such a way that they can respond to overlapping regions in the\\nvisual field.Recurrent neural networks use time-\\nseries information- what a user spoke\\nlast would impact what he will speak\\nnext.\", metadata={'source': 'Deep Learning.pdf', 'page': 123}),\n",
       " Document(page_content='Q. What is the Problem W ith Simple RNN ?\\nAns:- Lets take Many-Many (Same Length)\\nHere yi4 value depends a lot on xi4 and 03 and less depends on xi1 and 01 which is a limitation of this\\nsimple RNN because in real world application yi4 may depends more on xi1 and 01 and less xi4 and 03.\\nthis is called long term dependency .\\nSimple RNN Cant handle long term dependency .\\nlong term dependency - Later output depends a lot on earlier input i.e. ( yi4 -> xi1 )\\nSo that small change in yi4 should create similar change in weight of xi1 result vanish the gradient. in\\nsimple RNN with Sigmoid and tanh later output nodes of network are less sensitive to the input this is\\nhappen due to vanishing gradient problem.', metadata={'source': 'Deep Learning.pdf', 'page': 124}),\n",
       " Document(page_content='If a sequence is long enough, they’ll have a hard time carrying information from earlier time steps to later\\nones. So if you are trying to process a paragraph of text to do predictions, RNN’ s may leave out important\\ninformation from the beginning.\\nDuring back propagation, recurrent neural networks suf fer from the vanishing gradient problem. Gradients\\nare values used to update a neural networks weights. The vanishing gradient problem is when the gradient\\nshrinks as it back propagates through time. If a gradient value becomes extremely small, it doesn’t\\ncontribute too much learning.\\nSo in recurrent neural networks, layers that get a small gradient update stops learning. Those are usually\\nthe earlier layers. So because these layers don’t learn, RNN’ s can forget what it seen in longer sequences,\\nthus having a short-term memory . If you want to know more about the mechanics of recurrent neural\\nnetworks in general, you can read my previous post here.\\nTo overcome this we use LSTM (Long Short T erm Memory) And GRU (Gated Recurrent Unit)\\nLSTM (Long Short Term Memory)\\nIt takes care both long and short term dependency .\\n1. Input gate- It discover which value from input should be used to modify the memory . Sigmoid function\\ndecides which values to let through 0 or 1. And tanh function gives weightage to the values which are\\npassed, deciding their level of importance ranging from -1 to 1.  \\n \\n \\n2. Forget gate- It discover the details to be discarded from the block. A sigmoid function decides it. It looks at\\nthe previous state (ht-1) and the content input (Xt) and outputs a number between 0(omit this) and 1(keep\\nthis) for each number in the cell state Ct-1.  \\n \\n \\n3. Output gate- The input and the memory of the block are used to decide the output. Sigmoid function\\ndecides which values to let through 0 or 1. And tanh function decides which values to let through 0, 1. And\\ntanh function gives weightage to the values which are passed, deciding their level of importance ranging\\nfrom -1 to 1 and multiplied with an output of sigmoid.', metadata={'source': 'Deep Learning.pdf', 'page': 125}),\n",
       " Document(page_content='It represents a full RNN cell that takes the current input of the sequence xi, and outputs the current hidden\\nstate, hi, passing this to the next RNN cell for our input sequence. The inside of an LSTM cell is a lot more\\ncomplicated than a traditional RNN cell, while the conventional RNN cell has a single \"internal layer\" acting\\non the current state (ht-1) and input (xt).\\nStep-by-Step LSTM Walk Through\\nIt takes care both long and short term dependency .\\nStep-1 (forget gate layer)', metadata={'source': 'Deep Learning.pdf', 'page': 126}),\n",
       " Document(page_content='The first step in our LSTM is to decide what information we’re going to throw away from the cell state. This\\ndecision is made by a sigmoid layer called the “forget gate layer .”\\nIt looks at ht−1 and xt, and outputs a number between 0 and 1 for each number in the cell state Ct−1. A 1\\nrepresents “completely keep this” while a 0 represents “completely get rid of this.”\\nStep-2 (input gate layer)\\nIn this step is to decide what new information we’re going to store in the cell state.\\nThis has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update.\\nNext, a tanh layer creates a vector of new candidate values, C~t, that could be added to the state. In the\\nnext step, we’ll combine these two to create an update to the state.\\nStep-3 (input gate layer)\\nIt’s now time to update the old cell state, Ct−1, into the new cell state Ct. The previous steps already\\ndecided what to do, we just need to actually do it.\\nWe multiply the old state by ft, forgetting the things we decided to forget earlier . Then we add it ∗ C~t. This is\\nthe new candidate values, scaled by how much we decided to update each state value.', metadata={'source': 'Deep Learning.pdf', 'page': 127}),\n",
       " Document(page_content='Step-4 (output gate layer)\\nFinally , we need to decide what we’re going to output.\\nThis output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which\\ndecides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push\\nthe values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output\\nthe parts we decided to.\\nGRU (Gated Recurrent Units)\\nSo now we know how an LSTM work, let’ s briefly look at the GRU. The GRU is the newer generation of\\nRecurrent Neural networks and is pretty similar to an LSTM. GRU’ s got rid of the cell state and used the\\nhidden state to transfer information. It also only has two gates, a reset gate and update gate.', metadata={'source': 'Deep Learning.pdf', 'page': 128}),\n",
       " Document(page_content='1. Update Gate - The update gate acts similar to the forget and input gate of an LSTM. It decides what\\ninformation to throw away and what new information to add.\\n2. Reset Gate- The reset gate is another gate is used to decide how much past information to forget.\\nGRU’ s has fewer tensor operations; therefore, they are a little speedier to train then LSTM’ s. There isn’t a\\nclear winner which one is better . Researchers and engineers usually try both to determine which one works\\nbetter for their use case.\\n', metadata={'source': 'Deep Learning.pdf', 'page': 129}),\n",
       " Document(page_content='', metadata={'source': 'Deep Learning.pdf', 'page': 130})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Deep Learning Introduction\\nDeep learning is a branch of machine learning which is completely based on artificial neural\\nnetworks, as neural network is going to mimic the human brain so deep learning is also a kind\\nof mimic of human brain.\\nit is an artificial intelligence (AI) function that imitates the workings of the human brain in\\nprocessing data and creating patterns for use in decision making.\\nKEY T AKEA WAYS\\nDeep learning is an AI function that mimics the workings of the human brain in processing\\ndata for use in detecting objects, recognizing speech, translating languages, and making\\ndecisions.\\nDeep learning AI is able to learn without human supervision, drawing from data that is both\\nunstructured and unlabeled.\\nDeep learning, a form of machine learning, can be used to help detect fraud or money\\nlaundering, among other functions.\\nKey Difference between Machine Learning and Deep\\nLearning :', metadata={'source': 'Deep Learning.pdf', 'page': 0}),\n",
       " Document(page_content='Types of Deep Learning Algorithms That I Coverd In Notebook\\n1. Multilayer Perceptrons (MLPs)\\n2. Convolutional Neural Networks (CNNs)\\n3. Recurrent Neural Networks (RNNs)\\n4. Long Short T erm Memory Networks (LSTMs)\\n5. Generative Adversarial Networks (GANs)\\n6. Restricted Boltzmann Machines( RBMs)\\n7. Autoencoders\\n8. Self Organizing Maps (SOMs)\\nThere are so many techniques but in my note book i will focus on this 8 topic.', metadata={'source': 'Deep Learning.pdf', 'page': 1}),\n",
       " Document(page_content='Neural Networks\\nBefore Deep Dive in to deep learning first see some important terminology regarding this\\nQ1. What are Neural networks?\\nNeural networks are set of algorithms inspired by the functioning of human brian. Generally\\nwhen you open your eyes, what you see is called data and is processed by the Nuerons(data\\nprocessing cells) in your brain, and recognises what is around you. That’ s how similar the\\nNeural Networks works. They takes a large set of data, process the data(draws out the\\npatterns from data), and outputs what it is.  \\n \\nA neural network is composed of layers, which is a collection of neurons, with connections\\nbetween dif ferent layers. These layers transform data by first calculating the weighted sum of\\ninputs and then normalizing it using the activation functions assigned to the neurons.\\nThe leftmost layer in a Neural Network is called the input layer , and the rightmost layer is', metadata={'source': 'Deep Learning.pdf', 'page': 2}),\n",
       " Document(page_content='inputs and then normalizing it using the activation functions assigned to the neurons.\\nThe leftmost layer in a Neural Network is called the input layer , and the rightmost layer is\\ncalled the output layer . The layers between the input and the output, are called the hidden\\nlayers. Any Neural Network has 1 input layer and 1 output layer .\\nThe number of hidden layers dif fer between dif ferent networks depending on the complexity of\\nthe problem. Also, each hidden layer can have its own activation function.\\n \\n \\nHere 3 terms Comes in picture 1. Neuron , 2. W eights , 3. Bias , 4. Actiation_Function', metadata={'source': 'Deep Learning.pdf', 'page': 2}),\n",
       " Document(page_content='1. Neuron\\nLike in a human brain, the basic building block of a Neural Network is a Neuron. Its\\nfunctionality is similar to a human brain, i.e, it takes in some inputs and fires an output. Each\\nneuron is a small computing unit that takes a set of real valued numbers as input, performs\\nsome computation on them, and produces a single output value.\\nThe basic unit of computation in a neural network is the neuron, often called as a node or unit.\\nIt receives input from some other nodes, or from an external source and computes an output.\\nEach input has an associated weight (w), which is assigned on the basis of its relative\\nimportance to other inputs. The node applies a activation function f (defined below) to the\\nweighted sum of its inputs as in figure below .\\nThe above network have:\\nnumerical inputs X1 and X2\\nweights w1 and w2 associated with those inputs\\nb (called the Bias) associated with it.\\nThe Left side Picture is Neuron Of Human Brain ,The Right Side is Artificial Neuron', metadata={'source': 'Deep Learning.pdf', 'page': 3})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Deep Learning Introduction\\nDeep learning is a branch of machine learning which is completely based on artificial neural\\nnetworks, as neural network is going to mimic the human brain so deep learning is also a kind\\nof mimic of human brain.\\nit is an artificial intelligence (AI) function that imitates the workings of the human brain in\\nprocessing data and creating patterns for use in decision making.\\nKEY T AKEA WAYS\\nDeep learning is an AI function that mimics the workings of the human brain in processing\\ndata for use in detecting objects, recognizing speech, translating languages, and making\\ndecisions.\\nDeep learning AI is able to learn without human supervision, drawing from data that is both\\nunstructured and unlabeled.\\nDeep learning, a form of machine learning, can be used to help detect fraud or money\\nlaundering, among other functions.\\nKey Difference between Machine Learning and Deep\\nLearning :', metadata={'source': 'Deep Learning.pdf', 'page': 0}),\n",
       " Document(page_content='Types of Deep Learning Algorithms That I Coverd In Notebook\\n1. Multilayer Perceptrons (MLPs)\\n2. Convolutional Neural Networks (CNNs)\\n3. Recurrent Neural Networks (RNNs)\\n4. Long Short T erm Memory Networks (LSTMs)\\n5. Generative Adversarial Networks (GANs)\\n6. Restricted Boltzmann Machines( RBMs)\\n7. Autoencoders\\n8. Self Organizing Maps (SOMs)\\nThere are so many techniques but in my note book i will focus on this 8 topic.', metadata={'source': 'Deep Learning.pdf', 'page': 1}),\n",
       " Document(page_content='Neural Networks\\nBefore Deep Dive in to deep learning first see some important terminology regarding this\\nQ1. What are Neural networks?\\nNeural networks are set of algorithms inspired by the functioning of human brian. Generally\\nwhen you open your eyes, what you see is called data and is processed by the Nuerons(data\\nprocessing cells) in your brain, and recognises what is around you. That’ s how similar the\\nNeural Networks works. They takes a large set of data, process the data(draws out the\\npatterns from data), and outputs what it is.  \\n \\nA neural network is composed of layers, which is a collection of neurons, with connections\\nbetween dif ferent layers. These layers transform data by first calculating the weighted sum of\\ninputs and then normalizing it using the activation functions assigned to the neurons.\\nThe leftmost layer in a Neural Network is called the input layer , and the rightmost layer is', metadata={'source': 'Deep Learning.pdf', 'page': 2}),\n",
       " Document(page_content='inputs and then normalizing it using the activation functions assigned to the neurons.\\nThe leftmost layer in a Neural Network is called the input layer , and the rightmost layer is\\ncalled the output layer . The layers between the input and the output, are called the hidden\\nlayers. Any Neural Network has 1 input layer and 1 output layer .\\nThe number of hidden layers dif fer between dif ferent networks depending on the complexity of\\nthe problem. Also, each hidden layer can have its own activation function.\\n \\n \\nHere 3 terms Comes in picture 1. Neuron , 2. W eights , 3. Bias , 4. Actiation_Function', metadata={'source': 'Deep Learning.pdf', 'page': 2}),\n",
       " Document(page_content='1. Neuron\\nLike in a human brain, the basic building block of a Neural Network is a Neuron. Its\\nfunctionality is similar to a human brain, i.e, it takes in some inputs and fires an output. Each\\nneuron is a small computing unit that takes a set of real valued numbers as input, performs\\nsome computation on them, and produces a single output value.\\nThe basic unit of computation in a neural network is the neuron, often called as a node or unit.\\nIt receives input from some other nodes, or from an external source and computes an output.\\nEach input has an associated weight (w), which is assigned on the basis of its relative\\nimportance to other inputs. The node applies a activation function f (defined below) to the\\nweighted sum of its inputs as in figure below .\\nThe above network have:\\nnumerical inputs X1 and X2\\nweights w1 and w2 associated with those inputs\\nb (called the Bias) associated with it.\\nThe Left side Picture is Neuron Of Human Brain ,The Right Side is Artificial Neuron', metadata={'source': 'Deep Learning.pdf', 'page': 3}),\n",
       " Document(page_content='numerical inputs X1 and X2\\nweights w1 and w2 associated with those inputs\\nb (called the Bias) associated with it.\\nThe Left side Picture is Neuron Of Human Brain ,The Right Side is Artificial Neuron  \\nBiological Neuron W ork:\\nInformation from other neurons, in the form of electrical impulses, enters the dendrites  at\\nconnection points called synapses . The information flows from the dendrites to the cell where\\nit is processed. The output signal, a train of impulses, is then sent down the axon  to the\\nsynapse of other neurons.  \\n \\nArtificial Neuron W ork:\\nThe arrangements and connections of the neurons made up the network and have three\\nlayers.\\nThe first layer is called the input layer  and is the only layer exposed to external signals.\\nThe input layer transmits signals to the neurons in the next layer , which is called a hidden\\nlayer . The hidden layer extracts relevant features or patterns from the received signals.', metadata={'source': 'Deep Learning.pdf', 'page': 3}),\n",
       " Document(page_content='The input layer transmits signals to the neurons in the next layer , which is called a hidden\\nlayer . The hidden layer extracts relevant features or patterns from the received signals.\\nThose features or patterns that are considered important are then directed to the output layer ,\\nwhich is the final layer of the network.', metadata={'source': 'Deep Learning.pdf', 'page': 3}),\n",
       " Document(page_content='Single Layers And Multi Layer Netwrok\\nIn Multi Layer net there are many number of hidden layer in between input and output\\nlayer ,but in single only one or not hidden layer .\\nWeight:', metadata={'source': 'Deep Learning.pdf', 'page': 4}),\n",
       " Document(page_content='Every input(x) to a neuron has an associated weight(w), which is assigned on the basis of its\\nrelative importance to other inputs.\\nThe way a neuron works is, if the weighted sum of inputs is greater than a specific threshold, it\\nwould give an output 1, otherwise an output 0. This is the mathematical model of a neuron,\\nalso known as the Perceptron.\\nEvery neural unit takes in a weighted sum of its inputs, with an additional term in the sum\\ncalled a Bias.\\nBias:\\nBias is a constant which is used to adjust the output along with the weighted sum of inputs, so\\nthat the model can best fit for the given data.\\nz=w.x+b\\nI defined\\nweighted sum z\\nweight vector w\\ninput vector x\\nbias value b.\\ny=a=f(z)\\nThe output(y) of the neuron is a function f of the weighted sum of inputs z. The function f is\\nnon linear and is called the Activation Function.\\nActivation Function: ¶\\nThe purpose of activation function is to introduce non-linearity into the output of neuron. It', metadata={'source': 'Deep Learning.pdf', 'page': 5}),\n",
       " Document(page_content='non linear and is called the Activation Function.\\nActivation Function: ¶\\nThe purpose of activation function is to introduce non-linearity into the output of neuron. It\\ntakes a single number , and performs some mathematical operation on it. There are several\\nactivation functions used in practice:\\n1. Sigmoid\\n2. Tanh\\n3. ReLU\\n4. Leaky relu\\n5. Softmax function\\nThese Are most widly use activation function that i covered in subsequent notebook.', metadata={'source': 'Deep Learning.pdf', 'page': 5}),\n",
       " Document(page_content='Forward And Backward Propogation\\nEvery Neural Network has 2 main parts:\\n       1. Feed Forward Propogation/Forward Propogation.  \\n       2. Backward Propogation/Back propogation.  \\n \\nFeed Forward Propogation:\\nAll weights in the network are randomly assigned. Assume the weights of the connections\\nfrom the inputs to that node are w1, w2 and w3.\\nHere I take an example to better understand of this two concept\\nLet take an example of if you study 35 Hour per day then you definitly Pass the exam with 67\\nMark. Now we apply this.  \\n \\nInput to the network = [35, 67]\\nDesired output from the network (target) = [1, 0] 1 means P ASS and 0 means Fail\\nThen output V from the node in consideration can be calculated as below (f is an activation\\nfunction such as sigmoid):\\nV = f (1*w1 + 35*w2 + 67*w3)\\nSimilarly , outputs from the other node in the hidden layer is also calculated. The outputs of the\\ntwo nodes in the hidden layer act as inputs to the two nodes in the output layer . This enables', metadata={'source': 'Deep Learning.pdf', 'page': 6}),\n",
       " Document(page_content='Similarly , outputs from the other node in the hidden layer is also calculated. The outputs of the\\ntwo nodes in the hidden layer act as inputs to the two nodes in the output layer . This enables\\nus to calculate output probabilities from the two nodes in output layer .', metadata={'source': 'Deep Learning.pdf', 'page': 6}),\n",
       " Document(page_content='Suppose the output probabilities from the two nodes in the output layer are 0.4 and 0.6\\nrespectively (since the weights are randomly assigned, outputs will also be random). W e can\\nsee that the calculated probabilities (0.4 and 0.6) are very far from the desired probabilities (1\\nand 0 respectively), hence the network in above Figure is said to have an ‘Incorrect Output’.\\nAs it give output as fail but it not happen that one study 35 hr and secure 67.\\nhere W eight are randomly assigned so now we do Back Propagation and with W eight\\nUpdation.\\nBack Propagation and Weight Updation:\\nHere W e calculate the total error at the output nodes and propagate these errors back through\\nthe network using Backpropagation to calculate the gradients.\\nThen we use an optimization method such as Gradient Descent to ‘adjust’ all weights in the\\nnetwork with an aim of reducing the error at the output layer .\\nIf we now input the same example to the network again, the network should perform better', metadata={'source': 'Deep Learning.pdf', 'page': 7}),\n",
       " Document(page_content='network with an aim of reducing the error at the output layer .\\nIf we now input the same example to the network again, the network should perform better\\nthan before since the weights have now been adjusted to minimize the error in prediction.', metadata={'source': 'Deep Learning.pdf', 'page': 7}),\n",
       " Document(page_content=\"As shown in above Figure, the errors at the output nodes now reduce to [0.2, -0.2] as\\ncompared to [0.6, -0.4] earlier . This means that our network has learnt to correctly classify our\\nfirst training example.\\nWe repeat this process with all other training examples in our dataset. Then, our network is\\nsaid to have learnt those examples.\\n1. Thanks To https://medium.com/@purnasaigudikandula/a-beginner-intro-to-\\nneural-networks-\\n543267bda3c8#:~:text=Neural%20networks%20are%20set%20of,the%20functioning%20of%\\n20human%20brian.&text=That's%20how%20similar%20the%20Neural,and%20outputs%20wha\\nt%20it%20is.\\n2. http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html\", metadata={'source': 'Deep Learning.pdf', 'page': 8}),\n",
       " Document(page_content='What is Perceptron?\\nPerceptron is a single layer neural network and a multi-layer perceptron is called Neural\\nNetworks.\\n \\n \\n1. Single-layered perceptron model  \\n2. Multi-layered perceptron model.\\n1. Single-layered perceptron model\\nIf you talk about the functioning of the single-layered perceptron model, its algorithm doesn’t\\nhave previous information, so initially , weights are allocated inconstantly , then the algorithm\\nadds up all the weighted inputs,\\nif the added value is more than some pre-determined value( or , threshold value) then single-\\nlayered perceptron is stated as activated and delivered output as +1.\\nIn simple words, multiple input values feed up to the perceptron model, model executes with\\ninput values, and if the estimated value is the same as the required output, then the model\\nperformance is found out to be satisfied, therefore weights demand no changes. In fact, if the\\nmodel doesn’t meet the required result then few changes are made up in weights to minimize\\nerrors.', metadata={'source': 'Deep Learning.pdf', 'page': 9}),\n",
       " Document(page_content='performance is found out to be satisfied, therefore weights demand no changes. In fact, if the\\nmodel doesn’t meet the required result then few changes are made up in weights to minimize\\nerrors.\\n2. Multi-layered perceptron model', metadata={'source': 'Deep Learning.pdf', 'page': 9}),\n",
       " Document(page_content='In the forward stage, activation functions are originated from the input layer to the output layer ,\\nand in the backward stage, the error between the actual observed value and demanded given\\nvalue is originated backward in the output layer for modifying weights and bias values.\\nIn simple terms, multi-layered perceptron can be treated as a network of numerous artificial\\nneurons overhead varied layers, the activation function is no longer linear , instead, non-linear\\nactivation functions such as Sigmoid functions, T anH, ReLU activation Functions, etc are\\ndeployed for execution.', metadata={'source': 'Deep Learning.pdf', 'page': 10}),\n",
       " Document(page_content='Activation Function\\nActivation function decides, whether a neuron should be activated or not by calculating\\nweighted sum and further adding bias with it. The purpose of the activation function is to\\nintroduce non-linearity into the output of a neuron.\\nExplanation :-\\nWe know , neural network has neurons that work in correspondence of weight, bias and their\\nrespective activation function. In a neural network, we would update the weights and biases of the\\nneurons on the basis of the error at the output. This process is known as back-propagation.\\nActivation functions make the back-propagation possible since the gradients are supplied along\\nwith the error to update the weights and biases.  \\nWhy do we need Non-linear activation functions :-  \\nA neural network without an activation function is essentially just a linear regression model. The\\nactivation function does the non-linear transformation to the input making it capable to learn and\\nperform more complex tasks.', metadata={'source': 'Deep Learning.pdf', 'page': 11}),\n",
       " Document(page_content='activation function does the non-linear transformation to the input making it capable to learn and\\nperform more complex tasks.\\nThere are several type of Activation But here i dicuss some of them:\\n       1. Sigmoid (Binary Classification)  \\n       2. Tanh  \\n       3. Relu  \\n       4. Leaky Relu  \\n       5. Linear  \\n       6. Softmax (Use Multiclass Classification)\\n1. Sigmoid\\nThe Sigmoid Function curve looks like a S-shape.', metadata={'source': 'Deep Learning.pdf', 'page': 11}),\n",
       " Document(page_content='The main reason why we use sigmoid function is because it exists between (0 to 1).\\nTherefore, it is especially used for models where we have to predict the probability as an\\noutput.Since probability of anything exists only between the range of 0 and 1, sigmoid is the\\nright choice.\\nDerivative Of Sigmoid Function\\nSigmoid Function Derivative range from 0 to 0.25\\nUses :  Usually used in output layer  of a binary classification, where result is either 0 or 1, as value\\nfor sigmoid function lies between 0 and 1 only so, result can be predicted easily to be 1 if value is\\ngreater than 0.5 and 0 otherwise.\\n2. Tanh or hyperbolic tangent Activation Function\\ntanh is also like logistic sigmoid but better . The range of the tanh function is from (-1 to 1).\\ntanh is also sigmoidal (s - shaped).', metadata={'source': 'Deep Learning.pdf', 'page': 12}),\n",
       " Document(page_content='The advantage is that the negative inputs will be mapped strongly negative and the zero\\ninputs will be mapped near zero in the tanh graph.\\nThe activation that works almost always better than sigmoid function is T anh function also\\nknows as T angent Hyperbolic function. It’ s actually mathematically shifted version of the\\nsigmoid function. Both are similar and can be derived from each other .\\nDerivative Of T anh Function:-\\nSigmoid Function Derivative range from 0 to 1\\nUses :- Usually used in hidden layers  of a neural network as it’ s values lies between -1 to 1 hence\\nthe mean for the hidden layer comes out be 0 or very close to it, hence helps in centering the data\\nby bringing mean close to 0. This makes learning for the next layer much easier .', metadata={'source': 'Deep Learning.pdf', 'page': 13}),\n",
       " Document(page_content='Point :-\\n1. tanh and logistic sigmoid are the most popular activation functions in 90’ s but because of their\\nVanishing gradient problem and sometimes Exploding gradient problem (because of weights),\\nthey aren’t mostly used now .\\n2. These days Relu activation function is widely used. Even though, it sometimes gets into\\nvanishing gradient problem, variants of Relu help solving such cases.\\n3. tanh is preferred to sigmoid for faster convergence BUT again, this might change based on\\ndata. Data will also play an important role in deciding which activation function is best to\\nchoose.', metadata={'source': 'Deep Learning.pdf', 'page': 14}),\n",
       " Document(page_content='Vanishing Gradient Problem\\nVanishing gradient problem is a common problem that we face while training deep neural\\nnetworks.Gradients of neural networks are found during back propagation.\\nGenerally , adding more hidden layers will make the network able to learn more complex\\narbitrary functions, and thus do a better job in predicting future outcomes. This is where Deep\\nLearning is making a big dif ference.\\nNow during back-propagation i.e moving backward in the Network and calculating\\ngradients, it tends to get smaller and smaller as we keep on moving backward in the\\nNetwork . Below is Just a simple demonstration of V anishing Gradient Problem in single\\nlayer .\\nThis Happen because of we use sigmoid and tanh activation function in hidden layer . As\\nsigmoid and tanh deriative 0.25,1 respectively . so by calculating number of hidden layer the\\nderivative becomes 0 so avoid it we use RELU activation function in hidden layer .', metadata={'source': 'Deep Learning.pdf', 'page': 15}),\n",
       " Document(page_content='Exploding gradient Problem\\nWe have discussed about vanishing gradient problem.Now we will get in to exploding gradient\\nproblem.Earlier we discussed what happens when our gradient becomes very small.Now we\\nwill discuss what will happen if it gets large.\\nIn deep networks or recurrent neural networks, error gradients can accumulate during an\\nupdate and result in very large gradients.\\nThese in turn result in large updates to the network weights, and in turn, an unstable\\nnetwork.The explosion occurs through exponential growth by repeatedly multiplying gradients\\nthrough the network layers that have values larger than 1.0.This will ultimately led to an total\\nunstable network.', metadata={'source': 'Deep Learning.pdf', 'page': 16}),\n",
       " Document(page_content='3. Relu\\nBy Using of Sigmoid And T anh function there is vanishing gradient problem occure so the\\nconvergence rate slow down.\\nTo overcome slighlty we use Relu Function. it not totally overcome this problem but here the\\nconvergence rate is faster than sigmoid and tanh.\\nValue Range :- [0, inf)\\nIts Nature non-linear , which means we can easily backpropagate the errors and have multiple\\nlayers of neurons being activated by the ReLU function.\\nPoint:\\n1. Vanishing Gradient Problem  occure due to multiple number of derivative.\\n2. As sigmoid derivative 0 to 0.25 so by multiple by this sigmoid derivative the result might\\nvanishing gradient as number of hidden layer increase.\\n3. But in Relu here its derivative 0 to 1 so here no problem of any vanishing gradient problem as\\nits derivative cant be 0.2,0.3 like.\\n4. But as its derivative can be 0 so here a problem aries that called Dead_Activation\\nUses :-  ReLu is less computationally expensive than tanh and sigmoid because it involves simpler', metadata={'source': 'Deep Learning.pdf', 'page': 17}),\n",
       " Document(page_content='4. But as its derivative can be 0 so here a problem aries that called Dead_Activation\\nUses :-  ReLu is less computationally expensive than tanh and sigmoid because it involves simpler\\nmathematical operations. At a time only a few neurons are activated making the network sparse\\nmaking it ef ficient and easy for computation.\\nWhat is Dead_Activation ?\\nWhen Derivative equal to 0 in Relu Then New W eight = Old W eight :', metadata={'source': 'Deep Learning.pdf', 'page': 17}),\n",
       " Document(page_content='which is not good for any model.Here W eight Cant be update. it occure when value of z is\\nnegative.This state called Dead Activation state. to overcome this we use Leaky Relu.\\nSee here no updation happen the value remain same and ‘nan’ for validation loss is an\\nunexpected very large or very small number . This is dead activation state to overcome this we\\nuse leaky relu\\n4. Leaky Relu\\nLeaky ReLU is an improved version of the ReLU function.\\nReLU function, the gradient is 0 for x < 0(-ve), which made the neurons die for activations in\\nthat region.\\nLeaky ReLU is defined to address this problem. Instead of defining the Relu function as 0 for x\\nless than 0, we define it as a small linear component of x.\\nLeaky ReLUs are one attempt to fix the Dying ReLU problem. Instead of the function being\\nzero when x < 0, a leaky ReLU will instead have a small negative slope (of 0.01, or so). That\\nis, the function computes:', metadata={'source': 'Deep Learning.pdf', 'page': 18}),\n",
       " Document(page_content='Sigmoid,T anh,Relu,Leaky Relu', metadata={'source': 'Deep Learning.pdf', 'page': 19}),\n",
       " Document(page_content='Softmax\\nSoftmax is used as the activation function for multi-class classification tasks, usually the last\\nlayer .\\nWe talked about its role transforming numbers (aka logits) into probabilities that sum to one.\\nLet’s not forget it is also an activation function which means it helps our model achieve non-\\nlinearity . Linear combinations of linear combinations will always be linear but adding activation\\nfunction helps gives our model ability to handle non-linear data.\\nOutput of other activation functions such as sigmoid does not necessarily sum to one. Having\\noutputs summing to one makes softmax function great for probability analysis.\\nThe function is great for classification problems, especially if you’re dealing with multi-class\\nclassification problems, as it will report back the “confidence score” for each class. Since we’re\\ndealing with probabilities here, the scores returned by the softmax function will add up to 1.\\nMathematical representation\\nWhere:\\nσ = softmax\\n=input vector', metadata={'source': 'Deep Learning.pdf', 'page': 20}),\n",
       " Document(page_content='dealing with probabilities here, the scores returned by the softmax function will add up to 1.\\nMathematical representation\\nWhere:\\nσ = softmax\\n=input vector\\n=standard exponential function for input vector\\nK = number of classes in the multi-class classifier\\n=standard exponential function for output vector\\nIt states that we need to apply a standard exponential function to each element of the output\\nlayer , and then normalize these values by dividing by the sum of all the exponentials. Doing so\\nensures the sum of all exponentiated values adds up to 1.\\nHere are the steps For Softmax:\\n1. Exponentiate every element of the output layer and sum the results\\n2. Take each element of the output layer , exponentiate it and divide by the sum obtained in step\\n1\\nExample Implementation', metadata={'source': 'Deep Learning.pdf', 'page': 20}),\n",
       " Document(page_content='To start, let’ s declare an array which imitates the output layer of a neural network:\\nIn [1]:\\nBy step 1 we need to exponentiate each of the elements of the output\\nlayer:\\nIn [2]:\\nAccording to step 2 calculate probabilities! W e can use Numpy to\\ndivide each element by exponentiated sum and store results in another\\narray\\nIn [3]:\\nHere see the output are formed. If we sum three then we get probability 1. after this you use\\nargmax function which return highest value index number . See here return 0 as 0 index\\nhave 0.65 value which is highest among three value.\\nWhen you use softmax in your dataset you should use argmax function to predict\\noutput.\\nFrom a probabilistic perspective, if the argmax() function returns 1 in the large value, it returns\\n0fortheother twoarray indexes,here itgiving fullweight toindex 0andnoweight toindex 1Out[1]: array([2. , 1. , 0.1])\\nOut[2]: array([7.3890561 , 2.71828183, 1.10517092])\\n[0.65900114 0.24243297 0.09856589]  \\n1.0', metadata={'source': 'Deep Learning.pdf', 'page': 21}),\n",
       " Document(page_content='Out[2]: array([7.3890561 , 2.71828183, 1.10517092])\\n[0.65900114 0.24243297 0.09856589]  \\n1.0 \\n0 ## as we know softmax used in output layer so here i take a outputlayer value\\nimport numpy as np\\noutput_layer  = np.array([2.0,1.0,0.1])\\noutput_layer\\nexponentiated  = np.exp(output_layer )\\nexponentiated\\nprobabilities  = exponentiated  / np.sum(exponentiated )\\nprint(probabilities )\\nprint(sum(probabilities ))\\nprint(np.argmax(probabilities ))', metadata={'source': 'Deep Learning.pdf', 'page': 21}),\n",
       " Document(page_content='0 for the other two array indexes,here it giving full weight to index 0 and no weight to index 1\\nand index 2 for the largest value in the list [0.65,0.24,0.09].\\nIn the Keras deep learning library with a three-class classification task, use of softmax in\\nthe output layer may look as follows:\\n \\nmodel.add(Dense(no.of output layer , activation=\\'softmax\\'))\\n- It apply when you have multiclass problem aries\\nThe Differences between Sigmoid and Softmax Activation\\nFunctions\\nIn [4]:\\nIn [5]:\\nThe key takeaway from this example is:--- Sigmoid---  \\n[0.38 0.77 0.48 0.92]  \\n2.54 \\n**************************************************  \\n---Softmax---  \\n[0.04 0.21 0.06 0.7 ]  \\n1.0 import numpy as np \\n### sigmoid function\\ndef sigmoid(x):\\n    s = 1 / (1 + np.exp(-x))\\n    return s\\n \\n## softmax function\\ndef softmax(x):\\n    exponentiated  = np.exp(x)\\n    probabilities  = exponentiated  / np.sum(exponentiated )\\n    return probabilities\\nx = np.array([-0.5, 1.2, -0.1, 2.4])\\na = sigmoid(x)\\nprint(\"--- Sigmoid---\" )', metadata={'source': 'Deep Learning.pdf', 'page': 22}),\n",
       " Document(page_content='exponentiated  = np.exp(x)\\n    probabilities  = exponentiated  / np.sum(exponentiated )\\n    return probabilities\\nx = np.array([-0.5, 1.2, -0.1, 2.4])\\na = sigmoid(x)\\nprint(\"--- Sigmoid---\" )\\nprint(a.round(2))\\nprint(sum(a).round(2))\\n \\nprint(50*\"*\")\\n \\noutput_layer  = np.array([-0.5, 1.2, -0.1, 2.4])\\nb = softmax(output_layer )\\nprint(\"---Softmax---\" )\\nprint(b.round(2))\\nprint(sum(b))', metadata={'source': 'Deep Learning.pdf', 'page': 22}),\n",
       " Document(page_content='Sigmoid:  probabilities produced by a Sigmoid are independent. Furthermore, they are not\\nconstrained to sum to one: 0.38 + 0.77 + 0.48 + 0.92 = 2.54. The reason for this is because\\nthe Sigmoid looks at each raw output value separately .\\nSoftmax:  the outputs are interrelated. The Softmax probabilities will always sum to one by\\ndesign: 0.04 + 0.21 + 0.06 + 0.7 = 1.00. In this case, if we want to increase the likelihood of\\none class, the other has to decrease by an equal amount.\\nSummary..\\nCharacteristics of a Sigmoid Activation Function:\\n1. Used for Binary Classification in the Logistic Regression model\\n2. The probabilities sum does not need to be 1\\n3. Used as an Activation Function while building a Neural Network\\nCharacteristics of a Softmax Activation Function\\n1. Used for Multi-classification in the Logistics Regression model\\n2. The probabilities sum will be 1\\n3. Used in the dif ferent layers of Neural Networks\\nActivation Function For Regression Problem\\nLinear Activation Function:-', metadata={'source': 'Deep Learning.pdf', 'page': 23}),\n",
       " Document(page_content='2. The probabilities sum will be 1\\n3. Used in the dif ferent layers of Neural Networks\\nActivation Function For Regression Problem\\nLinear Activation Function:-\\nEquation : f(x) = x \\nRange : (-infinity to infinity)\\nNo matter how many layers we have, if all are linear in nature, the final activation function of\\nlast layer is nothing but just a linear function of the input of first layer .\\nLinear activation function is used at just one place i.e. output layer .\\nIf we will dif ferentiate linear function to bring non-linearity , result will no more depend on input\\n“x” and function will become constant, it won’t introduce any ground-breaking behavior to our\\nalgorithm.', metadata={'source': 'Deep Learning.pdf', 'page': 23}),\n",
       " Document(page_content='algorithm.\\nUses:  Calculation of price of a house is a regression problem. House price may have any big/small\\nvalue, so we can apply linear activation at output layer . Even in this case neural net must have any\\nnon-linear function at hidden layers.', metadata={'source': 'Deep Learning.pdf', 'page': 24}),\n",
       " Document(page_content='Loss Functions\\nThe loss function is the function that computes the distance between the current output of the\\nalgorithm and the expected output. It’ s a method to evaluate how your algorithm models the\\ndata. It can be categorized into two groups. One for classification (discrete values, 0,1,2…)\\nand the other for regression (continuous values).\\nTYPES OF LOSS FUNCTION:\\n1. Regression Loss Functions\\n    Mean Absolute Error  \\n    Mean Squared Error  \\n    Root Mean Square error (RMSE)\\n2. Binary Classification Loss Functions\\n    Binary Cross-Entropy\\n3. Multi-Class Classification Loss Functions\\n    Multi-Class Cross-Entropy Loss  \\n    Sparse Multiclass Cross-Entropy Loss\\nRegression Losses\\nWe know all of this regression loss function but here i discuss a brief\\nMean Absolute Error\\nRegression metric which measures the average magnitude of errors in a group of predictions,\\nwithout considering their directions. In other words, it’ s a mean of absolute dif ferences among', metadata={'source': 'Deep Learning.pdf', 'page': 25}),\n",
       " Document(page_content='Regression metric which measures the average magnitude of errors in a group of predictions,\\nwithout considering their directions. In other words, it’ s a mean of absolute dif ferences among\\npredictions and expected results where all individual deviations have even importance.\\nwhere:\\ni — index of sample,', metadata={'source': 'Deep Learning.pdf', 'page': 25}),\n",
       " Document(page_content='ŷ — predicted value,\\ny — expected value,\\nm — number of samples in dataset.\\nSometimes it is possible to see the form of formula with swapped predicted value and expected\\nvalue, but it works the same.\\nMean Squared Error\\nOne of the most commonly used and firstly explained regression metrics. A verage squared\\ndifference between the predictions and expected results. In other words, an alteration of MAE\\nwhere instead of taking the absolute value of dif ferences, they are squared.\\nIn MAE, the partial error values were equal to the distances between points in the coordinate\\nsystem. Regarding MSE, each partial error is equivalent to the area of the square created out\\nof the geometrical distance between the measured points. All region areas are summed up\\nand averaged.\\nWhere\\ni — index of sample,\\nŷ — predicted value,\\ny — expected value,\\nm — number of samples in dataset.\\nRoot Mean Square error (RMSE)\\nRoot Mean Square error is the extension of MSE — measured as the average of square root', metadata={'source': 'Deep Learning.pdf', 'page': 26}),\n",
       " Document(page_content='ŷ — predicted value,\\ny — expected value,\\nm — number of samples in dataset.\\nRoot Mean Square error (RMSE)\\nRoot Mean Square error is the extension of MSE — measured as the average of square root\\nof sum of squared dif ferences between predictions and actual observations.', metadata={'source': 'Deep Learning.pdf', 'page': 26}),\n",
       " Document(page_content='Classification Losses\\nBinary Classification Loss Functions\\nBinary Cross Entropy\\nAlso called Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss.\\nUnlike Softmax loss it is independent for each vector component (class), meaning that the loss\\ncomputed for every CNN output vector component is not af fected by other component values.\\nBinary cross entropy measures how far away from the true value (which is either 0 or 1) the\\nprediction is for each of the classes and then averages these class-wise errors to obtain the\\nfinal loss.\\nWe can define cross entropy as the dif ference between two probability distributions p and q,\\nwhere p is our true output and q is our estimate of this true output.\\nit Only use for binary classification problem\\nMulti-Class Classification Loss Functions\\nCategorical cross-entropy\\nUsed binary and multiclass problem, the label needs to be encoded as categorical, one-hot\\nencoding representation (for 3 classes: [0, 1, 0], [1,0,0]…)', metadata={'source': 'Deep Learning.pdf', 'page': 27}),\n",
       " Document(page_content='Categorical cross-entropy\\nUsed binary and multiclass problem, the label needs to be encoded as categorical, one-hot\\nencoding representation (for 3 classes: [0, 1, 0], [1,0,0]…)\\nIt is a loss function that is used for single label categorization. This is when only one category\\nis applicable for each data point. In other words, an example can belong to one class only .\\nUse categorical crossentropy in classification problems where only one result can be correct.\\nExample:  In the  MNIST   problem where you have images of the numbers 0,1, 2, 3, 4, 5, 6, 7, 8,\\nand 9. Categorical crossentropy gives the probability that an image of a number is, for\\nexample, a 4 or a 9.', metadata={'source': 'Deep Learning.pdf', 'page': 27}),\n",
       " Document(page_content='Categorical cross-entropy will compare the distribution of the predictions (the activations in the\\noutput layer , one for each class) with the true distribution, where the probability of the true\\nclass is set to 1 and 0 for the other classes. T o put it in a dif ferent way , the true class is\\nrepresented as a one-hot encoded vector , and the closer the model’ s outputs are to that\\nvector , the lower the loss.\\nSparse Categorical cross-entropy\\nUsed binary and multiclass problem (the label is an integer — 0 or 1 or … n, depends on the\\nnumber of labels)\\nAll Losses : https://keras.io/api/losses/  (https://keras.io/api/losses/)\\nSummary\\nThere are three kinds of classification tasks:\\n   1. Binary classification: two exclusive classes  \\n   2. Multi-class classification: more than two exclusive classes  \\n   3. Multi-label classification: just non-exclusive classes\\nHere, we can say\\n   1. In the case of (1), you need to use binary cross entropy.', metadata={'source': 'Deep Learning.pdf', 'page': 28}),\n",
       " Document(page_content='3. Multi-label classification: just non-exclusive classes\\nHere, we can say\\n   1. In the case of (1), you need to use binary cross entropy.  \\n   2. In the case of (2), you need to use categorical cross entropy.  \\n   3.In the case of (3), you need to use binary cross entropy.', metadata={'source': 'Deep Learning.pdf', 'page': 28}),\n",
       " Document(page_content='Which Loss and Activation Functions should I use?\\nThe motive of the blog is to give you some ideas on the usage of “Activation Function” & “Loss\\nfunction” in dif ferent scenarios.\\nChoosing an activation function and loss function is directly dependent upon the output you\\nwant to predict. There are dif ferent cases and dif ferent outputs of a predictive model. Before I\\nintroduce you to such cases let see an introduction to the activation function and loss function.\\nThe activation function activates the neuron that is required for the desired output, converts\\nlinear input to non-linear output. If you are not aware of the dif ferent activation functions I\\nwould recommend you visit my activation pdf to get an in-depth explanation of dif ferent\\nactivation functions click here :\\nhttps://github.com/pratyusa98/ML_Algo_pdf/tree/main/01_Deep_Learning_PDF\\n(https://github.com/pratyusa98/ML_Algo_pdf/tree/main/01_Deep_Learning_PDF) .', metadata={'source': 'Deep Learning.pdf', 'page': 29}),\n",
       " Document(page_content='activation functions click here :\\nhttps://github.com/pratyusa98/ML_Algo_pdf/tree/main/01_Deep_Learning_PDF\\n(https://github.com/pratyusa98/ML_Algo_pdf/tree/main/01_Deep_Learning_PDF) .\\nLoss function helps you figure out the performance of your model in prediction, how good the\\nmodel is able to generalize. It computes the error for every training. Y ou can read more about\\nloss functions and how to reduce the loss\\nhttps://github.com/pratyusa98/ML_Algo_pdf/tree/main/01_Deep_Learning_PDF\\n(https://github.com/pratyusa98/ML_Algo_pdf/tree/main/01_Deep_Learning_PDF) ..\\nLet’s see the different cases:\\nCASE 1: When the output is a numerical value that you are\\ntrying to predict\\nEx:- Consider predicting the prices of houses provided with dif ferent features of the house. A\\nneural network structure where the final layer or the output later will consist of only one neuron\\nthat reverts the numerical value. For computing the accuracy score the predicted values are\\ncompared to true numeric values.', metadata={'source': 'Deep Learning.pdf', 'page': 29}),\n",
       " Document(page_content='that reverts the numerical value. For computing the accuracy score the predicted values are\\ncompared to true numeric values.\\nActivation Function to be used in Output layer such cases,', metadata={'source': 'Deep Learning.pdf', 'page': 29}),\n",
       " Document(page_content='* Linear Activation - it gives output in a numeric form th\\nat is the demand for this case. Or  \\n         * ReLU Activation - This activation function gives you pos\\nitive numeric outputs as a result.  \\nLoss function to be used in such cases,\\n         * Mean Squared Error (MSE) - This loss function is respons\\nible to compute the average squared difference    between the true v\\nalues and the predicted values.\\nCASE 2: When the output you are trying to predict is\\nBinary\\nEx:- Consider a case where the aim is to predict whether a loan applicant will default or not. In\\nthese types of cases, the output layer consists of only one neuron that is responsible to result\\nin a value that is between 0 and 1 that can be also called probabilistic scores.\\nFor computing the accuracy of the prediction, it is again compared with the true labels. The\\ntrue value is 1 if the data belongs to that class or else it is 0.\\nActivation Function to be used in Output layer such cases,', metadata={'source': 'Deep Learning.pdf', 'page': 30}),\n",
       " Document(page_content='true value is 1 if the data belongs to that class or else it is 0.\\nActivation Function to be used in Output layer such cases,\\n            * Sigmoid Activation -  This activation function gives\\nthe output as 0 and 1.\\nLoss function to be used in such cases,\\n             * Binary Cross Entropy - The difference between the tw\\no probability distributions is given by binary  cross-entropy. (p,1-\\np) is the model distribution predicted by the model, to compare it w\\nith true distribution, the   binary cross-entropy is used.', metadata={'source': 'Deep Learning.pdf', 'page': 30}),\n",
       " Document(page_content='CASE 3: Predicting a single class from many classes\\nEx:- Consider a case where you are predicting the name of the fruit amongst 5 dif ferent fruits.\\nIn the case, the output layer will consist of only one neuron for every class and it will revert a\\nvalue between 0 and 1, the output is the probability distribution that results in 1 when all are\\nadded.\\nEach output is checked with its respective true value to get the accuracy . These values are\\none-hot-encoded which means if will be 1 for the correct class or else for others it would be\\nzero.\\nActivation Function to be used in Output layer such cases,\\n             * Softmax Activation -  This activation function gives  \\nthe output between 0 and 1 that are the probability scores which if\\nadded gives the result as 1.  \\nLoss function to be used in such cases,\\n             * Cross-Entropy - It computes the difference between t\\nwo probability distributions.  \\n             * (p1,p2,p3) is the model distribution that is predict', metadata={'source': 'Deep Learning.pdf', 'page': 31}),\n",
       " Document(page_content='* Cross-Entropy - It computes the difference between t\\nwo probability distributions.  \\n             * (p1,p2,p3) is the model distribution that is predict\\ned by the model where p1+p2+p3=1. This is compared with the true dis\\ntribution using cross-entropy.\\nCASE 4: Predicting multiple labels from multiple\\nclass\\nEx:- Consider the case of predicting dif ferent objects in an image having multiple objects. This\\nis termed as multiclass classification. In these types of cases, the output layer consists of only\\none neuron that is responsible to result in a value that is between 0 and 1 that can be also', metadata={'source': 'Deep Learning.pdf', 'page': 31}),\n",
       " Document(page_content='called probabilistic scores.\\nFor computing the accuracy of the prediction, it is again compared with the true labels. The\\ntrue value is 1 if the data belongs to that class or else it is 0.\\nActivation Function to be used in Output layer such cases,\\n               * Sigmoid Activation -  This activation function giv\\nes the output as 0 and 1.\\nLoss function to be used in such cases,\\n               * Binary Cross Entropy - The difference between the\\ntwo probability distributions is given by binary cross-entropy. (p,\\n1-p) is the model distribution predicted by the model, to compare it  \\nwith true distribution, the binary cross-entropy is used.\\nAll Losses : https://keras.io/api/losses/  (https://keras.io/api/losses/)   \\nAll Activation : https://keras.io/api/layers/activations/  (https://keras.io/api/layers/activations/)\\nSummary\\nThis activation use only output layer and in hidden layer you can use Relu or Leaky Relu.', metadata={'source': 'Deep Learning.pdf', 'page': 32}),\n",
       " Document(page_content='All Activation : https://keras.io/api/layers/activations/  (https://keras.io/api/layers/activations/)\\nSummary\\nThis activation use only output layer and in hidden layer you can use Relu or Leaky Relu.\\nThe following table summarizes the above information to allow you to quickly find the final\\nlayer activation function and loss function that is appropriate to your use-case', metadata={'source': 'Deep Learning.pdf', 'page': 32}),\n",
       " Document(page_content='Weight Initialization\\nThe weight initialization technique you choose for your neural network can determine how\\nquickly the network converges or whether it converges at all. Although the initial values of\\nthese weights are just one parameter among many to tune, they are incredibly important.\\nTheir distribution af fects the gradients and, therefore, the ef fectiveness of training.\\nWhy is weight initialization important?¶\\nImproperly initialized weights can negatively af fect the training process by contributing to the\\nvanishing or exploding gradient problem.\\nWith the vanishing gradient problem, the weight update is minor and results in slower\\nconvergence — this makes the optimization of the loss function slow and in a worst case\\nscenario, may stop the network from converging altogether .\\nConversely , initializing with weights that are too large may result in exploding gradient values\\nduring forward propagation or back-propagation.\\n1. Zero initialization :', metadata={'source': 'Deep Learning.pdf', 'page': 34}),\n",
       " Document(page_content='Conversely , initializing with weights that are too large may result in exploding gradient values\\nduring forward propagation or back-propagation.\\n1. Zero initialization :\\nIf all the weights are initialized with 0, the derivative with respect to loss function is the same\\nfor every weight(w), thus all weights have the same value in subsequent iterations.\\nThis makes hidden units symmetric and continues for all the n iterations i.e. setting weights to\\n0 does not make it better than a linear model.\\nAn important thing to keep in mind is that biases have no ef fect what so ever when initialized\\nwith 0.\\nIt also gives problems like vanishing gradient problem.\\n2. initialization W ith -ve Number :\\nIf all weigth can be negative then it af fect Relu Activation Function.As in -ve Relu comes under\\ndead activation problem. so we cant use this technique.\\nWeights can’t be too high as gives problems like exploding Gradient problem(weights of the', metadata={'source': 'Deep Learning.pdf', 'page': 34}),\n",
       " Document(page_content='dead activation problem. so we cant use this technique.\\nWeights can’t be too high as gives problems like exploding Gradient problem(weights of the\\nmodel explode to infinity), which means that a large space is made available to search for\\nglobal minima hence convergence becomes slow .\\nTo prevent the gradients of the network’ s activations from vanishing or exploding, we need\\nto have following rules:\\n       1. The mean of the activations should be zero.  \\n       2. The variance of the activations should stay the same across e\\nvery layer.\\nIdea 1 : Normal or Naïve Initialization:', metadata={'source': 'Deep Learning.pdf', 'page': 34}),\n",
       " Document(page_content=\"In normal distribution weights can be a part of normal or gaussian distribution with mean as\\nzero and a unit standard deviation.\\nRandom initialization is done so that convergence is not to a false minima.\\nIn Keras it can be simply written as hyperparameter as - kernel_initializer='random_normal'\\nIdea 2: Uniform Initialization:\\nIn uniform initialization of weights , weights belong to a uniform distribution in range a,b with\\nvalues of a and b as below:\\nWhenever sigmoid activation function is used as , Uniform works well.\\nIn Keras it can be simply written as hyperparameter as - kernel_initializer='random_uniform'\\nIdea 3: Xavier/ Glorot Weight Initialization:\\nThe variance of weights in the case normal distribution was not taken care of which resulted in\\ntoo large or too small activation values which again led to exploding gradient and vanishing\\ngradient problems respectively , when back propagation was done.\", metadata={'source': 'Deep Learning.pdf', 'page': 35}),\n",
       " Document(page_content=\"too large or too small activation values which again led to exploding gradient and vanishing\\ngradient problems respectively , when back propagation was done.\\nIn order to overcome this problem Xavier Initialization was introduced. It keeps the variance\\nthe same across every layer . We will assume that our layer ’s activations are normally\\ndistributed around zero.\\nGlorot or Xavier had a belief that if they maintain variance of activations in all the layers going\\nforward and backward convergence will be fast as compared to using standard initialization\\nwhere gap was larger .\\nIt have T wo V arient\\n     a. Normal Distribution - kernel_initializer='glorot_normal'  \\n     b. Uniform Distribution - kernel_initializer='glorot_uniform'\\nPoint :  Works well with tanh , sigmoid activation functions.\\na. Normal Distribution:\\nIn Normal Distribution, weights belong to normal distribution where mean is zero and standard\\ndeviation is as below:\", metadata={'source': 'Deep Learning.pdf', 'page': 35}),\n",
       " Document(page_content=\"b. Uniform Distribution:\\nUniform Distribution , weights belong to uniform distribution in range of a and b defined as\\nbelow:\\nIdea 4: He-Initialization:\\nWhen using activation functions that were zero centered and have output range between-1,1\\nfor activation functions like tanh and softsign, activation outputs were having mean of 0 and\\nstandard deviation around 1 average wise.\\nBut if ReLu is used instead of tanh, it was observed that on average it has standard deviation\\nvery close to square root of 2 divided by input connections.\\nIt have T wo V arient\\n   a. Normal Distribution - kernel_initializer='he_normal'  \\n   b. Uniform Distribution - kernel_initializer='he_uniform'\\nPoint :  Works well with Relu And Leaky Relu activation functions.\\na. Normal Distribution:\\nIn He-Normal initialization method, weights belong to normal distribution where mean is zero\\nand standard deviation is as below:\", metadata={'source': 'Deep Learning.pdf', 'page': 36}),\n",
       " Document(page_content='b. Uniform Initialization :\\nIn He Uniform Initialization weights belong to uniform distribution in range as shown below:', metadata={'source': 'Deep Learning.pdf', 'page': 37}),\n",
       " Document(page_content='Optimization Techniques\\nOptimization algorithms are responsible for reducing losses and provide most accurate results\\npossible.\\nThe weight is initialized using some initialization strategies and is updated with each epoch\\naccording to the equation. The best results are achieved using some optimization strategies or\\nalgorithms called Optimizer .\\nSome of the techniques that we will be discussing in this article is-\\n         * Gradient Descent  \\n         * Stochastic Gradient Descent (SGD)  \\n         * Mini-Batch Stochastic Gradient Descent (MB — SGD)  \\n         * SGD with Momentum  \\n         * Nesterov Accelerated Gradient (NAG)  \\n         * Adaptive Gradient (AdaGrad)  \\n         * AdaDelta  \\n         * RMSProp  \\n         * Adam\\n1. Gradient Descent or Batch Gradient Descent\\nA Gradient Descent is an iterative algorithm, that starts from a random point on the function\\nand traverses down its slope in steps until it reaches lowest point (global minima) of that\\nfunction.', metadata={'source': 'Deep Learning.pdf', 'page': 38}),\n",
       " Document(page_content='A Gradient Descent is an iterative algorithm, that starts from a random point on the function\\nand traverses down its slope in steps until it reaches lowest point (global minima) of that\\nfunction.\\nThis algorithm is apt for cases where optimal points cannot be found by equating the slope of\\nthe function to 0. For the function to reach minimum value, the weights should be altered.\\nWith the help of back propagation, loss is transferred from one layer to another and “weights”\\nparameter are also modified depending on loss so that loss can be minimized.', metadata={'source': 'Deep Learning.pdf', 'page': 38}),\n",
       " Document(page_content='Point :\\n1. Use all training Sample for a forward pass and adjust the weights.\\n2. This makes it computationally intensive. 3. Another drawback is there are chances the iteration\\nvalues may get stuck at local minima or saddle point and never converge to minima. T o obtain the\\nbest solution, the must reach global minima. 4. Good For Small training data.\\nCost function: θ=θ−α ⋅∇ J(θ)\\nAdvantages:\\nEasy computation.\\nEasy to implement.\\nEasy to understand.\\nDisadvantages:\\nMay trap at local minima.\\nWeights are changed after calculating gradient on the whole dataset. So, if the dataset is too\\nlarge than this may take years to converge to the minima.\\nRequires large memory to calculate gradient on the whole dataset.\\n2. Stochastic Gradient Descent\\nStochastic Gradient Descent is an extension of Gradient Descent, where it overcomes some\\nof the disadvantages of Gradient Descent algorithm.\\nSGD tries to overcome the disadvantage of computationally intensive by computing the', metadata={'source': 'Deep Learning.pdf', 'page': 39}),\n",
       " Document(page_content='of the disadvantages of Gradient Descent algorithm.\\nSGD tries to overcome the disadvantage of computationally intensive by computing the\\nderivative of one point at a time.\\nDue to this fact, SGD takes more number of iterations compared to GD to reach minimum and\\nalso contains some noise when compared to Gradient Descent.\\nAs SGD computes derivatives of only 1 point at a time, the time taken to complete one epoch\\nis large compared to Gradient Descent algorithm.\\nPoint :\\n1. Use One (Randomly Picked) Sample for a forward pass and adjust the weights.\\n2. Good when training set is very big and we dont want too much computation.\\ncost function θ=θ−α ⋅∇ J(θ;x(i);y(i)) , where {x(i) ,y(i)} are the training examples.\\nAdvantages:\\nFrequent updates of model parameters hence, converges in less time.\\nRequires less memory as no need to store values of loss functions.\\nMay get new minima’ s.\\nDisadvantages:\\nHigh variance(noisey) in model parameters.', metadata={'source': 'Deep Learning.pdf', 'page': 39}),\n",
       " Document(page_content='May shoot even after achieving global minima.\\nTo get the same convergence as gradient descent needs to slowly reduce the value of\\nlearning rate.\\n3. Mini Batch — Stochastic Gradient Descent\\nMB-SGD is an extension of SGD algorithm. It overcomes the time-consuming complexity of\\nSGD by taking a batch of points / subset of points from dataset to compute derivative.\\nIt’s best among all the variations of gradient descent algorithms. It is an improvement on both\\nSGD and standard gradient descent. It updates the model parameters after every batch. So,\\nthe dataset is divided into various batches and after every batch, the parameters are updated.\\nThis is a mixture of both stochastic and batch gradient descent.\\nThe training set is divided into multiple groups called batches. Each batch has a number of\\ntraining samples in it.\\nAt a time a single batch is passed through the network which computes the loss of every', metadata={'source': 'Deep Learning.pdf', 'page': 40}),\n",
       " Document(page_content='training samples in it.\\nAt a time a single batch is passed through the network which computes the loss of every\\nsample in the batch and uses their average to update the parameters of the neural network.\\nFor example, say the training set has 100 training examples which is divided into 5 batches\\nwith each batch containing 20 training examples. This means that the equation in figure2 will\\nbe iterated over 5 times (number of batches).\\nPoint:\\n1. Use a Batch Of (Randomly Picked) Sample for a forward pass and adjust the weights.\\n2. It is observed that the derivative of loss function of MB-SGD is similar to the loss function of GD\\nafter some iterations. But the number iterations to achieve minima in MB-SGD is large compared\\nto GD and is computationally expensive. The update of weights in much noisier because the\\nderivative is not always towards minima.\\nθ=θ−α ⋅∇ J(θ; B(i)), where {B(i)} are the batches of training examples.\\nAdvantages:', metadata={'source': 'Deep Learning.pdf', 'page': 40}),\n",
       " Document(page_content='derivative is not always towards minima.\\nθ=θ−α ⋅∇ J(θ; B(i)), where {B(i)} are the batches of training examples.\\nAdvantages:\\nFrequently updates the model parameters and also has less variance.\\nRequires medium amount of memory .\\nEasily fits in the memory\\nIt is computationally ef ficient\\nBenefit from vectorization\\nIf stuck in local minimums, some noisy steps can lead the way out of them\\nAverage of the training samples produces stable error gradients and convergence\\n!!!! This ensures the following advantages of both stochastic and batch gradient descent are used\\ndue to which Mini Batch Gradient Descent is most commonly used in practice.\\nSee How in this above three convergence Occure towards minima point', metadata={'source': 'Deep Learning.pdf', 'page': 40}),\n",
       " Document(page_content='Here W e see in SGD Due to frequent updates the steps taken towards the minima are very noisy .\\nThis can often lead the gradient descent into other directions. Also, due to noisy steps it may take\\nlonger to achieve convergence to the minima of the loss function. to reduce this we can use SGD\\nwith Momentum.\\n4. SGD with Momentum\\nMomentum was invented for reducing high variance in SGD and softens the convergence.\\nIt accelerates the convergence towards the relevant direction and reduces the fluctuation to\\nthe irrelevant direction. One more hyperparameter is used in this method known as\\nmomentum symbolized by ‘γ’(gamma).\\nIt is an adaptive optimization algorithm which exponentially uses weighted average gradients\\nover previous iterations to stabilize the convergence, resulting in quicker optimization.\\nThis is done by adding a fraction (gamma) to the previous iteration values.\\nEssentially the momentum term increase when the gradient points are in the same directions', metadata={'source': 'Deep Learning.pdf', 'page': 41}),\n",
       " Document(page_content='This is done by adding a fraction (gamma) to the previous iteration values.\\nEssentially the momentum term increase when the gradient points are in the same directions\\nand reduce when gradients fluctuate. As a result, the value of loss function converges faster\\nthan expected.\\nAdvantages:\\nReduces the oscillations and high variance of the parameters.\\nConverges faster than gradient descent.\\nDisadvantages:\\nOne more hyper-parameter is added which needs to be selected manually and accurately .', metadata={'source': 'Deep Learning.pdf', 'page': 41}),\n",
       " Document(page_content='5. Nesterov accelerated gradient(NAG)\\nMomentum may be a good method but if the momentum is too high the algorithm may miss\\nthe local minima and may continue to rise up. So, to resolve this issue the NAG algorithm was\\ndeveloped.\\nNesterov accelerated gradient (NAG) is a way to give momentum more precision.\\nThe idea of the NAG algorithm is very similar to SGD with momentum with a slight variant. In\\nthe case of SGD with momentum algorithm, the momentum and gradient are computed on\\nprevious updated weight.\\nBoth NAG and SGD with momentum algorithms work equally well and share the same\\nadvantages and disadvantages.', metadata={'source': 'Deep Learning.pdf', 'page': 42}),\n",
       " Document(page_content='figure (a) :\\nIn figure (a), update 1 is positive i.e., the gradient is negative because as w_0 increases L\\ndecreases. Even update 2 is positive as well and you can see that the update is slightly larger\\nthan update 1 because of momentum.\\nBy now , you should be convinced that update 3 will be bigger than both update 1 and 2 simply\\nbecause of momentum and the positive update history .\\nUpdate 4 is where things get interesting. In SGD with Momentum case, due to the positive\\nhistory , the update overshoots and the descent recovers by doing negative updates.\\nfigure (b) :\\nBut in NAG’ s case, every update happens in two steps — first, a partial update, where we get\\nto the look_ahead point and then the final update (see the NAG update rule), see figure (b).\\nFirst 3 updates of NAG are pretty similar to the momentum-based method as both the updates\\n(partial and final) are positive in those cases. But the real dif ference becomes apparent during\\nupdate 4.', metadata={'source': 'Deep Learning.pdf', 'page': 43}),\n",
       " Document(page_content='First 3 updates of NAG are pretty similar to the momentum-based method as both the updates\\n(partial and final) are positive in those cases. But the real dif ference becomes apparent during\\nupdate 4.\\nAs usual, each update happens in two stages, the partial update (4a) is positive, but the final\\nupdate (4b) would be negative as the calculated gradient at w_lookahead would be negative\\n(convince yourself by observing the graph).\\nThis negative final update slightly reduces the overall magnitude of the update, still resulting in\\nan overshoot but a smaller one when compared to the vanilla momentum-based gradient\\ndescent. And that my friend, is how NAG helps us in reducing the overshoots, i.e. making us\\ntake shorter U-turns.\\nAdvantages:\\nDoes not miss the local minima.\\nSlows if minima’ s are occurring.\\nDisadvantages:', metadata={'source': 'Deep Learning.pdf', 'page': 43}),\n",
       " Document(page_content='Still, the hyperparameter needs to be selected manually .\\nPoint :\\nBy using NAG technique, we are now able to adapt error function with the help of previous\\nand future values and thus eventually speed up the convergence. Now , in the next techniques\\nwe will try to adapt alter or vary the individual parameters depending on the importance factor\\nit plays in each case.', metadata={'source': 'Deep Learning.pdf', 'page': 44}),\n",
       " Document(page_content='6. Adaptive Gradient (AdaGrad)\\nAdaptive Gradient as the name suggests adopts the learning rate of parameters by updating it\\nat each iteration depending on the position it is present, i.e- by adapting slower learning rates\\nwhen features are occurring frequently and adapting higher learning rate when features are\\ninfrequent.\\nThe motivation behind Adagrad is to have dif ferent learning rates for each neuron of each\\nhidden layer for each iteration.\\nBut why do we need dif ferent learning rates?\\nData sets have two types of features:\\nDense features, e.g. House Price Data set (Large number of non-zero valued features), where\\nwe should perform smaller updates on such features; and\\nSparse Features, e.g. Bag of words (Large number of zero valued features), where we should\\nperform larger updates on such features.\\nIt has been found that Adagrad greatly improved the robustness of SGD, and is used for training\\nlarge-scale neural nets at Google.\\nη : initial Learning rate', metadata={'source': 'Deep Learning.pdf', 'page': 45}),\n",
       " Document(page_content='perform larger updates on such features.\\nIt has been found that Adagrad greatly improved the robustness of SGD, and is used for training\\nlarge-scale neural nets at Google.\\nη : initial Learning rate\\nϵ : smoothing term that avoids division by zero\\nw: W eight of parameters\\nIn SGD learning Rate same for all weight but in Adagrad this is dif ferent for all.\\nAdvantage:\\nNo need to update the learning rate manually as it changes adaptively with iterations.\\nIf we have some Sparse and Dense feature it automatically takes out what learning rate is\\nsuitable.\\nDisadvantage:', metadata={'source': 'Deep Learning.pdf', 'page': 45}),\n",
       " Document(page_content='As the number of iteration becomes very large learning rate decreases to a very small number\\nwhich leads to slow convergence.\\nComputationally expensive as a need to calculate the second order derivative.\\nAdadelta, RMSProp, and adam tries to resolve Adagrad’ s radically diminishing learning rates.\\n7. AdaDelta\\nIt is simply an extension of AdaGrad that seeks to reduce its monotonically decreasing\\nlearning rate.\\nInstead of summing all the past gradients, AdaDelta restricts the no. of summation values to a\\nlimit (w).\\nIn AdaDelta, the sum of past gradients (w) is defined as “Decaying A verage of all past\\nsquared gradients”. The current average at the iteration then depends only on the previous\\naverage and current gradient.\\nInstead of inef ficiently storing all previous squared gradients, we recursively define a decaying\\naverage of all past squared gradients. The running average at each time step then depends', metadata={'source': 'Deep Learning.pdf', 'page': 46}),\n",
       " Document(page_content='Instead of inef ficiently storing all previous squared gradients, we recursively define a decaying\\naverage of all past squared gradients. The running average at each time step then depends\\n(as a fraction γ , similarly to the Momentum term) only on the previous average and the\\ncurrent gradient.\\nAdvantages:\\nNow the learning rate does not decay and the training does not stop.\\nDisadvantages:\\nComputationally expensive.', metadata={'source': 'Deep Learning.pdf', 'page': 46}),\n",
       " Document(page_content='8. RMSProp\\nRMSProp is Root Mean Square Propagation. It was devised by Geof frey Hinton.\\nRMSProp tries to resolve Adagrad’ s radically diminishing learning rates by using a moving\\naverage of the squared gradient. It utilizes the magnitude of the recent gradient descents to\\nnormalize the gradient.\\nIn RMSProp learning rate gets adjusted automatically and it chooses a dif ferent learning rate\\nfor each parameter .\\nRMSProp divides the learning rate by the average of the exponential decay of squared\\ngradients\\nIts cost function same as Adadelta\\n9. Adam — Adaptive Moment Estimation\\nIt is a combination of RMSProp and Momentum.\\nThis method computes adaptive learning rate for each parameter .\\nIn addition to storing the previous decaying average of squared gradients, it also holds the\\naverage of past gradient similar to Momentum. Thus, Adam behaves like a heavy ball with\\nfriction which prefers flat minima in error surface.', metadata={'source': 'Deep Learning.pdf', 'page': 47}),\n",
       " Document(page_content='average of past gradient similar to Momentum. Thus, Adam behaves like a heavy ball with\\nfriction which prefers flat minima in error surface.\\nAnother method that calculates the individual adaptive learning rate for each parameter from\\nestimates of first (Momentum) and second (RMSProp) moments of the gradients.', metadata={'source': 'Deep Learning.pdf', 'page': 47}),\n",
       " Document(page_content='Advantages:\\nThe method is too fast and converges rapidly .\\nRectifies vanishing learning rate, high variance.\\nDisadvantages:\\nComputationally costly .', metadata={'source': 'Deep Learning.pdf', 'page': 48}),\n",
       " Document(page_content='Regularization Techniques\\nOne of the most common problem data science professionals face is to avoid overfitting. Have\\nyou come across a situation where your model performed exceptionally well on train data, but\\nwas not able to predict test data.\\nHave you seen this image before? As we move towards the right in this image, our model tries\\nto learn too well the details and the noise from the training data, which ultimately results in\\npoor performance on the unseen data.\\nIn other words, while going towards the right, the complexity of the model increases such that\\nthe training error reduces but the testing error doesn’t. This is shown in the image below .\\nIf you’ve built a neural network before, you know how complex they are. This makes them\\nmore prone to overfitting. Regularization is a technique which makes slight modifications to\\nthe learning algorithm such that the model generalizes better . This in turn improves the\\nmodel’ s performance on the unseen data as well.', metadata={'source': 'Deep Learning.pdf', 'page': 49}),\n",
       " Document(page_content='p\\nDifferent Regularization Techniques in Deep Learning\\n1. L1 & L2 regularization\\nL1 and L2 are the most common types of regularization. These update the general cost\\nfunction by adding another term known as the regularization term.\\n                     Cost function = Loss (say, binary cross entrop\\ny) + Regularization term\\nDue to the addition of this regularization term, the values of weight matrices decrease\\nbecause it assumes that a neural network with smaller weight matrices leads to simpler\\nmodels. Therefore, it will also reduce overfitting to quite an extent.\\nNote: Here the value 0.01 is the value of regularization parameter , i.e., lambda, which we\\nneed to optimize further . We can optimize it using the hyper parameter tuning\\nmethod.\\n2. Dropout\\nThis is the one of the most interesting types of regularization techniques. It also produces very\\ngood results and is consequently the most frequently used regularization technique in the field\\nof deep learning.', metadata={'source': 'Deep Learning.pdf', 'page': 50}),\n",
       " Document(page_content='good results and is consequently the most frequently used regularization technique in the field\\nof deep learning.\\nTo understand dropout, let’ s say our neural network structure is akin to the one shown below:## Below is the sample code to apply L2 regularization to a Dense layer.\\n \\nfrom keras import regularizers\\nmodel.add(Dense(64, input_dim=64,\\n                kernel_regularizer=regularizers.l2(0.01)\\n                \\n \\n## Below is the sample code to apply L1 regularization to a Dense layer.\\n \\nfrom keras import regularizers\\nmodel.add(Dense(64, input_dim=64,\\n                kernel_regularizer=regularizers.l1(0.01)', metadata={'source': 'Deep Learning.pdf', 'page': 50}),\n",
       " Document(page_content='So what does dropout do? At every iteration, it randomly selects some nodes and removes\\nthem along with all of their incoming and outgoing connections as shown below . So each\\niteration has a dif ferent set of nodes and this results in a dif ferent set of outputs. It can also be\\nthought of as an ensemble technique in machine learning.\\nPoint:-\\ndropout is usually preferred when we have a large neural network structure in order to\\nintroduce more randomness.\\nNote : As you can see, we have defined 0.25 as the probability of dropping. W e can tune it\\nfurther for better results using the Hyper parameter tuning method.\\n3. Data Augmentation (For Image Data)\\nThe simplest way to reduce overfitting is to increase the size of the training data. In machine\\nlearning, we were not able to increase the size of training data as the labeled data was too\\ncostly .\\nBut, now let’ s consider we are dealing with images. In this case, there are a few ways of', metadata={'source': 'Deep Learning.pdf', 'page': 51}),\n",
       " Document(page_content=\"learning, we were not able to increase the size of training data as the labeled data was too\\ncostly .\\nBut, now let’ s consider we are dealing with images. In this case, there are a few ways of\\nincreasing the size of the training data – rotating the image, flipping, scaling, shifting, etc. In\\nthe below image, some transformation has been done on the handwritten digits dataset.## In keras, we can implement dropout using the keras core layer. Below is the  \\npython code for it:\\n \\nfrom keras.layers.core import Dropout\\n \\nmodel = Sequential([\\n Dense(output_dim=hidden1_num_units, input_dim=input_num_units,  \\nactivation='relu'),\\n Dropout(0.25),\\n \\nDense(output_dim=output_num_units, input_dim=hidden5_num_units,  \\nactivation='softmax'),\\n ])\", metadata={'source': 'Deep Learning.pdf', 'page': 51}),\n",
       " Document(page_content='This technique is known as data augmentation. This usually provides a big leap in improving\\nthe accuracy of the model. It can be considered as a mandatory trick in order to improve our\\npredictions.\\nIn keras, we can perform all of these transformations using ImageDataGenerator . It has a big\\nlist of arguments which you you can use to pre-process your training data.\\n4. Batch Normalization\\nBatch normalization is a technique for improving the speed, performance, and stability of\\nartificial neural networks, also known as batch norm. The idea is to normalize the inputs of\\neach layer in such a way that, they have a mean activation output zero and a unit standard\\ndeviation.\\nThe reason for the ‘batch’ in the term Batch Normalization is because neural networks are\\nusually trained with a collated set of data at a time, this set or group of data is referred to as a\\nbatch. The operation within the BN technique occurs to an entire batch of input values as\\nopposed to a single input value.', metadata={'source': 'Deep Learning.pdf', 'page': 52}),\n",
       " Document(page_content='batch. The operation within the BN technique occurs to an entire batch of input values as\\nopposed to a single input value.\\nWhy should we normalize the input?\\nLet say we have 2D data, X1, and X2. X1 feature has a very wider spread between 200 to\\n-200 whereas the X2 feature has a very narrow spread. The left graph shows the variance of\\nthe data which has dif ferent ranges. The right graph shows data lies between -2 to 2 and it’ s\\nnormally distributed with 0 mean and unit variance.## Below is the sample code to implement it.\\nfrom keras.preprocessing.image import ImageDataGenerator\\ndatagen = ImageDataGenerator(horizontal flip=True)\\ndatagen.fit(train)', metadata={'source': 'Deep Learning.pdf', 'page': 52}),\n",
       " Document(page_content='Essentially , scaling the inputs through normalization gives the error surface a more spherical\\nshape, where it would otherwise be a very high curvature ellipse. Having an error surface with\\nhigh curvature will mean that we take many steps that aren’t necessarily in the optimal\\ndirection.\\nWhen we scale the inputs, we reduce the curvature, which makes methods that ignore\\ncurvature like gradient descent work much better . When the error surface is circular or\\nspherical, the gradient points right at the minimum.\\nBenefits of Batch Normalization\\nInclusion of Batch Normalization technique in deep neural networks improves training time\\nBN enables the utilization of larger learning rates, this shortness the time of convergence\\nwhen training neural networks\\nReduces the common problem of vanishing gradients\\nCovariate shift within neural network is reduced\\nPoint: In Batch normalization just as we standardize the inputs, the same way we', metadata={'source': 'Deep Learning.pdf', 'page': 53}),\n",
       " Document(page_content='Reduces the common problem of vanishing gradients\\nCovariate shift within neural network is reduced\\nPoint: In Batch normalization just as we standardize the inputs, the same way we\\nstandardize the activation at all the layers so that, at each layer we have 0 mean and unit\\nstandard deviation.', metadata={'source': 'Deep Learning.pdf', 'page': 53}),\n",
       " Document(page_content=\"## In keras, we can implement BatchNormalization using the keras layer. Below  \\nis the python code for it:\\nmodel = Sequential([\\n Dense(output_dim=hidden1_num_units, input_dim=input_num_units,  \\nactivation='relu'),\\n keras.layers.BatchNormalization(),\\n \\nDense(output_dim=output_num_units, input_dim=hidden5_num_units,  \\nactivation='softmax'),\\n ])\", metadata={'source': 'Deep Learning.pdf', 'page': 54}),\n",
       " Document(page_content='In [1]:\\nBinary Classifier\\nIn [2]:\\nIn [3]:\\nIn [4]:\\nApply ANNOut[2]:\\nPregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction\\n0 2138 62 35 033.6 0.127\\n1 0 84 82 3112538.2 0.233\\n2 0145 0 0 044.2 0.630\\n3 0135 68 4225042.3 0.365\\n4 1139 62 4148040.7 0.536\\nOut[3]:\\n(2000, 9)import pandas as pd\\nfrom matplotlib  import pyplot as plt\\nimport numpy as np\\n%matplotlib  inline\\nfrom sklearn.model_selection  import train_test_split\\nfrom sklearn.preprocessing  import LabelEncoder\\nfrom sklearn.preprocessing  import StandardScaler\\nfrom sklearn.metrics import mean_absolute_error ,mean_squared_error\\nfrom sklearn.metrics import confusion_matrix  , classification_report ,accuracy_score\\nimport tensorflow  as tf\\nfrom tensorflow  import keras\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\ndf = pd.read_csv (\"kaggle_diabetes.csv\" )\\ndf.head()\\ndf.shape\\nX = df.drop(\\'Outcome\\' ,axis=1)\\ny = df[\\'Outcome\\' ]', metadata={'source': 'Deep Learning.pdf', 'page': 55}),\n",
       " Document(page_content='from keras.models import Sequential\\nfrom keras.layers import Dense\\ndf = pd.read_csv (\"kaggle_diabetes.csv\" )\\ndf.head()\\ndf.shape\\nX = df.drop(\\'Outcome\\' ,axis=1)\\ny = df[\\'Outcome\\' ]\\nX_train, X_test, y_train, y_test = train_test_split (X,y,test_size =0.2,random_state =5)', metadata={'source': 'Deep Learning.pdf', 'page': 55}),\n",
       " Document(page_content=\"In [5]:\\nIn [6]:\\nIn [7]:\\nIn [8]:\\nIn [9]:Epoch 1/150  \\n50/50 [==============================] - 1s 1ms/step - loss: 6.1302 - accu\\nracy: 0.4691  \\nEpoch 2/150  \\n50/50 [==============================] - 0s 1ms/step - loss: 1.5064 - accu\\nracy: 0.6145  \\nEpoch 3/150  \\n50/50 [==============================] - 0s 1ms/step - loss: 1.1917 - accu\\nracy: 0.6264  \\nEpoch 4/150  \\n50/50 [==============================] - 0s 1ms/step - loss: 0.9443 - accu\\nracy: 0.6452  \\nEpoch 5/150  \\n50/50 [==============================] - 0s 1ms/step - loss: 0.8631 - accu\\nracy: 0.6444  \\nEpoch 6/150  \\n50/50 [==============================] - 0s 2ms/step - loss: 0.8198 - accu\\nracy: 0.6555  \\nEpoch 7/150  \\n50/50[ ]01/tl07250\\n13/13 [==============================] - 0s 1ms/step - loss: 0.4997 - accura\\ncy: 0.7775  \\nOut[8]:\\n[0.4997430741786957, 0.7774999737739563]classifier  = Sequential ()\\n##input 1st layer\\nclassifier .add(Dense(16,activation ='relu',input_dim =8))\\n## second hidden layer\", metadata={'source': 'Deep Learning.pdf', 'page': 56}),\n",
       " Document(page_content=\"cy: 0.7775  \\nOut[8]:\\n[0.4997430741786957, 0.7774999737739563]classifier  = Sequential ()\\n##input 1st layer\\nclassifier .add(Dense(16,activation ='relu',input_dim =8))\\n## second hidden layer\\nclassifier .add(Dense(8,activation ='relu'))\\n## output layer\\nclassifier .add(Dense(1,activation ='sigmoid' ))\\nclassifier .compile(optimizer ='adam',\\n              loss='binary_crossentropy' ,\\n              metrics=['accuracy' ])\\nclassifier .fit(X_train, y_train, epochs=150)\\nclassifier .evaluate (X_test, y_test)\\ny_pred = classifier .predict(X_test)\", metadata={'source': 'Deep Learning.pdf', 'page': 56}),\n",
       " Document(page_content=\"In [10]:\\nIn [11]:\\nIn [12]:\\nMulti Classification Using ANN\\nIn [13]:Out[10]:\\narray([[0.1072953 ],  \\n       [0.03356129],  \\n       [0.20200807],  \\n       [0.16860384],  \\n       [0.50361   ]], dtype=float32)\\n              precision    recall  f1-score   support  \\n \\n           0       0.79      0.92      0.85       269  \\n           1       0.74      0.49      0.59       131  \\n \\n    accuracy                           0.78       400  \\n   macro avg       0.77      0.70      0.72       400  \\nweighted avg       0.77      0.78      0.76       400  \\n \\nOut[13]:\\n(150, 5)yp = classifier .predict(X_test)\\nyp[:5]\\ny_pred = []\\nfor element in yp:\\n    if element > 0.5:\\n        y_pred.append(1)\\n    else:\\n        y_pred.append(0)\\nprint(classification_report (y_test,y_pred))\\ndata = pd.read_csv ('https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a\\ndata.shape\", metadata={'source': 'Deep Learning.pdf', 'page': 57}),\n",
       " Document(page_content=\"In [14]:\\nSplit in to X and y\\nIn [15]:\\nEncoding target variable Using Label or Dummy\\nimp1:- If you use dummy then in loss function you use categorical_crossentropy\\nimp2:- if you use label_encoding then in loss function you use sparse_categorical_crossentropy\\nOut[14]:\\nsepal_length sepal_width petal_length petal_width species\\n0 5.1 3.5 1.4 0.2setosa\\n1 4.9 3.0 1.4 0.2setosa\\n2 4.7 3.2 1.3 0.2setosa\\n3 4.6 3.1 1.5 0.2setosa\\n4 5.0 3.6 1.4 0.2setosadata.head()\\nX = data.drop('species' ,axis=1)\\ny = data['species' ]\", metadata={'source': 'Deep Learning.pdf', 'page': 58}),\n",
       " Document(page_content='In [16]:\\nIn [17]:\\nIn [18]:\\nIn [19]:\\nIn [20]:\\nApply ANN\\nIn [21]:\\nIn [22]:Out[16]:\\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  \\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  \\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  \\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  \\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,  \\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,  \\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\\nOut[20]:\\n(150, 4)## label\\nlb = LabelEncoder ()\\ny_enc = lb.fit_transform (y)\\ny_enc\\n# dummy\\n# y_dummy = pd.get_dummies(y).values\\nX_train,X_test,y_train,y_test = train_test_split (X,y_enc,test_size =0.25,random_state =4)\\nsc = StandardScaler ()\\nX_train_scaled  = sc.fit_transform (X_train)\\nX_test_scaled  = sc.transform (X_test)\\nX.shape\\nclassifier  = Sequential ()', metadata={'source': 'Deep Learning.pdf', 'page': 59}),\n",
       " Document(page_content='sc = StandardScaler ()\\nX_train_scaled  = sc.fit_transform (X_train)\\nX_test_scaled  = sc.transform (X_test)\\nX.shape\\nclassifier  = Sequential ()\\nclassifier .add(Dense(10,input_dim  = 4,activation  = \"relu\"))\\nclassifier .add(Dense(3,activation  = \"softmax\" ))\\n## if target dummy encoding use categorical_crossentropy if use label encoding use sparse_c\\nclassifier .compile(optimizer  = \\'adam\\' , loss = \\'sparse_categorical_crossentropy\\' ,  \\n                   metrics = [\\'accuracy\\' ] )', metadata={'source': 'Deep Learning.pdf', 'page': 59}),\n",
       " Document(page_content='In [23]:\\nIn [24]:\\nIn [25]:\\nIn [ ]:\\nIn [ ]:\\nRegression\\nIn [26]:Epoch 1/100  \\n4/4 [==============================] - 0s 2ms/step - loss: 0.8949 - accura\\ncy: 0.6033  \\nEpoch 2/100  \\n4/4 [==============================] - 0s 2ms/step - loss: 0.9165 - accura\\ncy: 0.5705  \\nEpoch 3/100  \\n4/4 [==============================] - 0s 2ms/step - loss: 0.8442 - accura\\ncy: 0.6268  \\nEpoch 4/100  \\n4/4 [==============================] - 0s 4ms/step - loss: 0.8412 - accura\\ncy: 0.6121  \\nEpoch 5/100  \\n4/4 [==============================] - 0s 4ms/step - loss: 0.8268 - accura\\ncy: 0.5933  \\nEpoch 6/100  \\n4/4 [==============================] - 0s 3ms/step - loss: 0.8125 - accura\\ncy: 0.6260  \\nEpoch 7/100  \\n4/4[ ]03/tl07945\\n              precision    recall  f1-score   support  \\n \\n           0       1.00      1.00      1.00        18  \\n           1       1.00      0.88      0.93         8  \\n           2       0.92      1.00      0.96        12  \\n \\n    accuracy                           0.97        38', metadata={'source': 'Deep Learning.pdf', 'page': 60}),\n",
       " Document(page_content=\"1       1.00      0.88      0.93         8  \\n           2       0.92      1.00      0.96        12  \\n \\n    accuracy                           0.97        38  \\n   macro avg       0.97      0.96      0.96        38  \\nweighted avg       0.98      0.97      0.97        38  \\n classifier .fit(X_train_scaled  , y_train ,epochs = 100) \\ny_pred = classifier .predict(X_test_scaled )\\n# y_test= np.argmax(y_test,axis=1) # when use dummy encoding in target\\ny_pred = np.argmax(y_pred,axis=1)\\nprint(classification_report (y_test,y_pred))\\ndf=pd.read_csv ('https://raw.githubusercontent.com/krishnaik06/Keras-Tuner/main/Real_Combine\", metadata={'source': 'Deep Learning.pdf', 'page': 60}),\n",
       " Document(page_content='In [27]:\\nIn [28]:\\nIn [29]:\\nIn [30]:\\nIn [31]:\\nIn [32]:\\nApply ANN\\nIn [33]:Out[27]:\\nT TM Tm SLP HVV V VM PM 2.5\\n07.49.84.81017.693.00.54.39.4219.720833\\n17.812.74.41018.587.00.64.411.1182.187500\\n26.713.42.41019.482.00.64.811.1154.037500\\n38.615.53.31018.772.00.88.120.6223.208333\\n412.420.94.41017.361.01.38.722.2200.645833\\nOut[32]:\\n(1092, 8)df.head()\\ndf.dropna(inplace=True)\\nX=df.drop(\\'PM 2.5\\' ,axis=1) ## independent features\\ny=df[\\'PM 2.5\\' ] ## dependent features\\nX_train, X_test, y_train, y_test = train_test_split (X, y, test_size =0.3, random_state =0)\\nsc = StandardScaler ()\\nX_train = sc.fit_transform (X_train)\\nX_test = sc.transform (X_test)\\nX.shape\\nclassifier  = Sequential ()\\nclassifier .add(Dense(10,input_dim  = 8,activation  = \"relu\"))\\n## second hidden layer\\nclassifier .add(Dense(8,activation =\\'relu\\'))\\nclassifier .add(Dense(1,activation  = \"linear\" ))', metadata={'source': 'Deep Learning.pdf', 'page': 61}),\n",
       " Document(page_content='In [34]:\\nIn [35]:\\nIn [36]:\\nIn [37]:\\nIn [38]:Epoch 1/100  \\n24/24 [==============================] - 1s 2ms/step - loss: 18824.7756 -  \\nmse: 18824.7756  \\nEpoch 2/100  \\n24/24 [==============================] - 0s 2ms/step - loss: 20844.0150 -  \\nmse: 20844.0150  \\nEpoch 3/100  \\n24/24 [==============================] - 0s 2ms/step - loss: 21034.4066 -  \\nmse: 21034.4066  \\nEpoch 4/100  \\n24/24 [==============================] - 0s 2ms/step - loss: 21785.1067 -  \\nmse: 21785.1067  \\nEpoch 5/100  \\n24/24 [==============================] - 0s 2ms/step - loss: 16766.3743 -  \\nmse: 16766.3743  \\nEpoch 6/100  \\n24/24 [==============================] - 0s 2ms/step - loss: 19805.6353 -  \\nmse: 19805.6353  \\nEpoch 7/100  \\n24/24[ ]0s3ms/step loss:189015041\\n11/11 [==============================] - 0s 2ms/step - loss: 3349.8369 - ms\\ne: 3349.8369  \\nOut[36]:\\n[3349.8369140625, 3349.8369140625]\\nMAE 40.52856917885261  \\nMSE 3349.837197332338', metadata={'source': 'Deep Learning.pdf', 'page': 62}),\n",
       " Document(page_content='11/11 [==============================] - 0s 2ms/step - loss: 3349.8369 - ms\\ne: 3349.8369  \\nOut[36]:\\n[3349.8369140625, 3349.8369140625]\\nMAE 40.52856917885261  \\nMSE 3349.837197332338  \\nRMSE 57.87777809602177  ## if target dummy encoding use categorical_crossentropy if use label encoding use sparse_c\\nclassifier .compile(optimizer  = \\'adam\\' , loss = \\'mse\\',  \\n                   metrics = [\\'mse\\'] ) \\nclassifier .fit(X_train , y_train ,epochs = 100) \\nclassifier .evaluate (X_test, y_test)\\ny_pred = classifier .predict(X_test)\\nprint(\"MAE\",mean_absolute_error (y_test,y_pred))\\nprint(\"MSE\",mean_squared_error (y_test,y_pred))\\nprint(\"RMSE\",mean_squared_error (y_test,y_pred,squared=False))', metadata={'source': 'Deep Learning.pdf', 'page': 62}),\n",
       " Document(page_content='Convolutional neural networks (CNN)\\nCnn are one of the most popular models used today . This neural network computational model uses a\\nvariation of multilayer perceptrons and contains one or more convolutional layers that can be either entirely\\nconnected or pooled.\\nThese convolutional layers create feature maps that record a region of image which is ultimately broken\\ninto rectangles and sent out for nonlinear processing.\\nLet us suppose this in the input matrix of 5×5 and a filter of matrix 3X3, for those who don’t know what a\\nfilter is a set of weights in a matrix applied on an image or a matrix to obtain the required features, please\\nsearch on convolution if this is your first time!\\nNote: W e always take the sum or average of all the values while doing a convolution.\\nSteps Involve in CNN', metadata={'source': 'Deep Learning.pdf', 'page': 63}),\n",
       " Document(page_content='1. Edge Detection (Convolution)\\nIn the previous article, we saw that the early layers of a neural network detect edges from an image.\\nDeeper layers might be able to detect the cause of the objects and even more deeper layers might detect\\nthe cause of complete objects (like a person’ s face).\\nIn this section, we will focus on how the edges can be detected from an image. Suppose we are given the\\nbelow image: As you can see, there are many vertical and horizontal edges in the image. The first thing to do is\\nto detect these edges:', metadata={'source': 'Deep Learning.pdf', 'page': 64}),\n",
       " Document(page_content='So, we take the first 3 X 3 matrix from the 7 X 7 image and multiply it with the filter . Now , the first element\\nof the (n-k+1 x n-k+1) i.e (7-3+1 X 7-3+1) 5 X 5 output will be the sum of the element-wise product of these\\nvalues, i.e. 0 0+00+1 0+10+0 1+00+0 0+10+1*0 =0. T o calculate the second element of the 5 X 5 output, we\\nwill shift our filter one step towards the right and again get the sum of the element-wise product:\\n2. Pooling\\nA pooling layer is another building block of a CNN. Its function is to progressively reduce the spatial size of\\nthe representation to reduce the amount of parameters and computation in the network. Pooling layer\\noperates on each feature map independently . The most common approach used in pooling is max pooling.\\nTypes of Pooling Layers :-\\n1. Max Pooling  \\nMax pooling is a pooling operation that selects the maximum element from the region of the feature map', metadata={'source': 'Deep Learning.pdf', 'page': 65}),\n",
       " Document(page_content='Types of Pooling Layers :-\\n1. Max Pooling  \\nMax pooling is a pooling operation that selects the maximum element from the region of the feature map\\ncovered by the filter . Thus, the output after max-pooling layer would be a feature map containing the most\\nprominent features of the previous feature map.\\n2. Average Pooling  \\nAverage pooling computes the average of the elements present in the region of feature map covered by the\\nfilter. Thus, while max pooling gives the most prominent feature in a particular patch of the feature map,\\naverage pooling gives the average of features present in a patch.', metadata={'source': 'Deep Learning.pdf', 'page': 65}),\n",
       " Document(page_content='More On Pooling https://www .geeksforgeeks.org/cnn-introduction-to-pooling-layer/\\n(https://www .geeksforgeeks.org/cnn-introduction-to-pooling-layer/)\\nNow Apply Pooling in our above Feature Map\\nProblem with Simple Convolution Layers\\nWhile applying convolutions we will not obtain the output dimensions the same as input we will lose data\\nover borders so we append a border of zeros and recalculate the convolution covering all the input values.\\n 1. Padding  \\n 2. Striding  \\n1. Padding', metadata={'source': 'Deep Learning.pdf', 'page': 66}),\n",
       " Document(page_content='See In without padding our input is 6x6 but output image goes down into 4x4 . so by using padding we got\\nthe same result.Padding is simply a process of adding layers of zeros to our input images so as to avoid\\nthe problems mentioned above.\\nSo padding prevents shrinking as, if p = number of layers of zeros added to the border of the image, then\\nour (n x n) image becomes (n + 2p) x (n + 2p) image after padding. So, applying convolution-operation\\n(with (f x f) filter) outputs (n + 2p – f + 1) x (n + 2p – f + 1) images. For example, adding one layer of\\npadding to an (8 x 8) image and using a (3 x 3) filter we would get an (8 x 8) output after performing\\nconvolution operation.', metadata={'source': 'Deep Learning.pdf', 'page': 67}),\n",
       " Document(page_content='2. Strides\\nIt uses to reduce the size of matrix. if we sfited by 1 then we called stride=1 and if we sfited by 2 means\\nstride = 2 so on.\\nPadding,Stride Put in One Equation', metadata={'source': 'Deep Learning.pdf', 'page': 68}),\n",
       " Document(page_content='Step3 : Flattening\\nFlattening is converting the data into a 1-dimensional array for inputting it to the next layer . We flatten the\\noutput of the convolutional layers to create a single long feature vector . And it is connected to the final\\nclassification model, which is called a fully-connected layer .\\nStep 4\\nComplete CNN in one V iew\\nhere in last step we use full connection network', metadata={'source': 'Deep Learning.pdf', 'page': 69}),\n",
       " Document(page_content=\"This is a simple CNN Network\\nIn [1]:\\nIn [2]:\\nIn [3]:\\nIn [4]:Found 198 images belonging to 2 classes.  \\nFound 100 images belonging to 2 classes.  # Importing the libraries\\nimport tensorflow  as tf\\nfrom tensorflow .keras.preprocessing .image import ImageDataGenerator\\n#data sugmentation\\n# Preprocessing the Training set\\ntrain_datagen  = ImageDataGenerator (rescale=1./255,\\n                                      rotation_range =40,\\n                                      width_shift_range =0.2,\\n                                      height_shift_range =0.2,\\n                                      shear_range =0.2,\\n                                      zoom_range =0.2,\\n                                      horizontal_flip =True,\\n                                      fill_mode ='nearest' )\\ntraining_set  = train_datagen .flow_from_directory ('image_data/training' ,\\n                                                 target_size  = (64, 64),\", metadata={'source': 'Deep Learning.pdf', 'page': 70}),\n",
       " Document(page_content=\"training_set  = train_datagen .flow_from_directory ('image_data/training' ,\\n                                                 target_size  = (64, 64),\\n                                                 batch_size  = 32,\\n                                                 class_mode  = 'binary' )\\n# Preprocessing the Test set\\ntest_datagen  = ImageDataGenerator (rescale = 1./255)\\ntest_set  = test_datagen .flow_from_directory ('image_data/validation' ,\\n                                            target_size  = (64, 64),\\n                                            batch_size  = 32,\\n                                            class_mode  = 'binary' )\\n## showing some image from training\\nimport matplotlib .pyplot as plt\\ndef plotImages (images_arr ):\\n    fig, axes = plt.subplots (1, 5, figsize=(20, 20))\\n    axes = axes.flatten()\\n    for img, ax in zip(images_arr , axes):\\n        ax.imshow(img)\\n    plt.tight_layout ()\\n    plt.show()\", metadata={'source': 'Deep Learning.pdf', 'page': 70}),\n",
       " Document(page_content='In [5]:\\nModel Build Use Only CNN\\nIn [6]:\\nIn [7]:\\nIn [8]:\\nimages = [training_set [0][0][0] for i in range(5)]\\nplotImages (images)\\nfrom tensorflow .keras.layers import Conv2D\\n# Part 2 - Building the CNN\\n# Initialising the CNN\\ncnn = tf.keras.models.Sequential ()\\n# Step 1 - # Adding a first convolutional layer\\ncnn.add(tf.keras.layers.Conv2D(filters=32,padding=\"same\",kernel_size =3, activation =\\'relu\\', \\n## step 2 - #apply maxpool\\ncnn.add(tf.keras.layers.MaxPool2D (pool_size =2, strides=2)) ## Apply pooing stride\\n# Adding a second convolutional layer\\ncnn.add(tf.keras.layers.Conv2D(filters=32,padding=\\'same\\',kernel_size =3, activation =\\'relu\\'))\\ncnn.add(tf.keras.layers.MaxPool2D (pool_size =2, strides=2))\\n# Step 3 - Flattening\\ncnn.add(tf.keras.layers.Flatten())\\n# Step 4 - Full Connection\\ncnn.add(tf.keras.layers.Dense(units=128, activation =\\'relu\\'))\\ntf.keras.layers.Dropout(0.5)\\n# Step 5 - Output Layer\\ncnn.add(tf.keras.layers.Dense(units=1, activation =\\'sigmoid\\' ))\\n# Part 3 - Training the CNN', metadata={'source': 'Deep Learning.pdf', 'page': 71}),\n",
       " Document(page_content=\"tf.keras.layers.Dropout(0.5)\\n# Step 5 - Output Layer\\ncnn.add(tf.keras.layers.Dense(units=1, activation ='sigmoid' ))\\n# Part 3 - Training the CNN\\n# Compiling the CNN\\ncnn.compile(optimizer  = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy' ])\", metadata={'source': 'Deep Learning.pdf', 'page': 71}),\n",
       " Document(page_content=\"In [9]:\\nSave And Load Model\\nIn [10]:\\nIn [11]:\\nIn [12]:Epoch 1/2  \\n7/7 [==============================] - 4s 410ms/step - loss: 0.7529 - accura\\ncy: 0.4600 - val_loss: 0.6988 - val_accuracy: 0.5000  \\nEpoch 2/2  \\n7/7 [==============================] - 2s 308ms/step - loss: 0.6898 - accura\\ncy: 0.5349 - val_loss: 0.6932 - val_accuracy: 0.5100  \\nOut[12]:\\narray([[0.5059088]], dtype=float32)# Training the CNN on the Training set and evaluating it on the Test set\\nhistory = cnn.fit(x = training_set , validation_data  = test_set , epochs = 2)\\n#save model\\nfrom tensorflow .keras.models import load_model\\ncnn.save('model_rcat_dog.h5' )\\nfrom tensorflow .keras.models import load_model\\n# load model\\nmodel = load_model ('model_rcat_dog.h5' )\\n# Part 4 - Making a single prediction\\nimport numpy as np\\nfrom tensorflow .keras.preprocessing  import image\\ntest_image  = image.load_img ('image_data/test/3285.jpg' , target_size  = (64,64))\\ntest_image  = image.img_to_array (test_image )\\ntest_image =test_image /255\", metadata={'source': 'Deep Learning.pdf', 'page': 72}),\n",
       " Document(page_content=\"test_image  = image.load_img ('image_data/test/3285.jpg' , target_size  = (64,64))\\ntest_image  = image.img_to_array (test_image )\\ntest_image =test_image /255\\ntest_image  = np.expand_dims (test_image , axis = 0)\\nresult = cnn.predict(test_image )\\nresult\", metadata={'source': 'Deep Learning.pdf', 'page': 72}),\n",
       " Document(page_content='In [13]:\\nThe image classified is dog  \\nOut[13]:\\nif result[0]<=0.5:\\n    print(\"The image classified is cat\" )\\nelse:\\n    print(\"The image classified is dog\" )\\n    \\nfrom IPython.display import Image\\nImage(filename =\\'image_data/test/3285.jpg\\' ,height=\\'200\\',width=\\'200\\')', metadata={'source': 'Deep Learning.pdf', 'page': 73}),\n",
       " Document(page_content='Basic Introduction\\nLeNet-5, from the paper Gradient-Based Learning Applied to Document Recognition, is a very ef ficient\\nconvolutional neural network for handwritten character recognition.\\nPaper: Gradient-Based Learning Applied to Document Recognition\\n(http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\\nAuthors : Yann LeCun, Léon Bottou, Y oshua Bengio, and Patrick Haf fner\\nPublished in : Proceedings of the IEEE (1998)\\nStructure of the LeNet network\\nLeNet5 is a small network, it contains the basic modules of deep learning: convolutional layer , pooling layer ,\\nand full link layer . It is the basis of other deep learning models. Here we analyze LeNet5 in depth. At the same\\ntime, through example analysis, deepen the understanding of the convolutional layer and pooling layer .\\nLeNet-5 T otal seven layer , does not comprise an input, each containing a trainable parameters; each layer has', metadata={'source': 'Deep Learning.pdf', 'page': 74}),\n",
       " Document(page_content='LeNet-5 T otal seven layer , does not comprise an input, each containing a trainable parameters; each layer has\\na plurality of the Map the Feature , a characteristic of each of the input FeatureMap extracted by means of a\\nconvolution filter , and then each FeatureMap There are multiple neurons.\\nDetailed explanation of each layer parameter:', metadata={'source': 'Deep Learning.pdf', 'page': 74}),\n",
       " Document(page_content=\"INPUT Layer\\nThe first is the data INPUT layer . The size of the input image is uniformly normalized to 32 * 32.\\nNote: This layer does not count as the network structure of LeNet-5. T raditionally , the input layer\\nis not considered as one of the network hierarchy .\\nC1 layer-convolutional layer\\nInput picture : 32 * 32\\nConvolution kernel size : 5 * 5\\nConvolution kernel types : 6\\nOutput featuremap size : 28 * 28 (32-5 + 1) = 28\\nNumber of neurons : 28 28 6\\nTrainable parameters : (5 5 + 1) 6 (5 * 5 = 25 unit parameters and one bias parameter per filter ,\\na total of 6 filters)\\nNumber of connections : (5 5 + 1) 6 28 28 = 122304\\nDetailed description:\\n1. The first convolution operation is performed on the input image (using 6 convolution kernels of size 5 5) to\\nobtain 6 C1 feature maps (6 feature maps of size 28 28, 32-5 + 1 = 28).\\n2. Let's take a look at how many parameters are needed. The size of the convolution kernel is 5 5, and there\", metadata={'source': 'Deep Learning.pdf', 'page': 75}),\n",
       " Document(page_content=\"obtain 6 C1 feature maps (6 feature maps of size 28 28, 32-5 + 1 = 28).\\n2. Let's take a look at how many parameters are needed. The size of the convolution kernel is 5 5, and there\\nare 6 (5 * 5 + 1) = 156 parameters in total, where +1 indicates that a kernel has a bias.\\n3. For the convolutional layer C1, each pixel in C1 is connected to 5 5 pixels and 1 bias in the input image, so\\nthere are 156 28 * 28 = 122304 connections in total. There are 122,304 connections, but we only need to\\nlearn 156 parameters, mainly through weight sharing.\\nS2 layer-pooling layer (downsampling layer)\\nInput : 28 * 28\\nSampling area : 2 * 2\\nSampling method : 4 inputs are added, multiplied by a trainable parameter , plus a trainable\\noffset. Results via sigmoid\\nSampling type : 6\\nOutput featureMap size : 14 * 14 (28/2)\\nNumber of neurons : 14 14 6\\nTrainable parameters : 2 * 6 (the weight of the sum + the of fset)\\nNumber of connections : (2 2 + 1) 6 14 14\", metadata={'source': 'Deep Learning.pdf', 'page': 75}),\n",
       " Document(page_content='Sampling type : 6\\nOutput featureMap size : 14 * 14 (28/2)\\nNumber of neurons : 14 14 6\\nTrainable parameters : 2 * 6 (the weight of the sum + the of fset)\\nNumber of connections : (2 2 + 1) 6 14 14\\nThe size of each feature map in S2 is 1/4 of the size of the feature map in C1.', metadata={'source': 'Deep Learning.pdf', 'page': 75}),\n",
       " Document(page_content='Detailed description:\\nThe pooling operation is followed immediately after the first convolution. Pooling is performed using 2 2 kernels,\\nand S2, 6 feature maps of 14 14 (28/2 = 14) are obtained.\\nThe pooling layer of S2 is the sum of the pixels in the 2 * 2 area in C1 multiplied by a weight coef ficient plus an\\noffset, and then the result is mapped again.\\nSo each pooling core has two training parameters, so there are 2x6 = 12 training parameters, but there are\\n5x14x14x6 = 5880 connections.\\nC3 layer-convolutional layer\\nInput : all 6 or several feature map combinations in S2\\nConvolution kernel size : 5 * 5\\nConvolution kernel type : 16\\nOutput featureMap size : 10 * 10 (14-5 + 1) = 10\\nEach feature map in C3 is connected to all 6 or several feature maps in S2, indicating that the\\nfeature map of this layer is a dif ferent combination of the feature maps extracted from the\\nprevious layer .\\nOne way is that the first 6 feature maps of C3 take 3 adjacent feature map subsets in S2 as', metadata={'source': 'Deep Learning.pdf', 'page': 76}),\n",
       " Document(page_content='previous layer .\\nOne way is that the first 6 feature maps of C3 take 3 adjacent feature map subsets in S2 as\\ninput. The next 6 feature maps take 4 subsets of neighboring feature maps in S2 as input. The\\nnext three take the non-adjacent 4 feature map subsets as input. The last one takes all the\\nfeature maps in S2 as input.\\nThe trainable parameters are : 6 (3 5 5 + 1) + 6 (4 5 5 + 1) + 3 (4 5 5 + 1) + 1 (6 5 5 +1) = 1516\\nNumber of connections : 10 10 1516 = 151600\\nDetailed description:\\nAfter the first pooling, the second convolution, the output of the second convolution is C3, 16 10x10 feature\\nmaps, and the size of the convolution kernel is 5 5. We know that S2 has 6 14 14 feature maps, how to get 16\\nfeature maps from 6 feature maps? Here are the 16 feature maps calculated by the special combination of the\\nfeature maps of S2. details as follows:\\nThe first 6 feature maps of C3 (corresponding to the 6th column of the first red box in the figure above) are', metadata={'source': 'Deep Learning.pdf', 'page': 76}),\n",
       " Document(page_content='feature maps of S2. details as follows:\\nThe first 6 feature maps of C3 (corresponding to the 6th column of the first red box in the figure above) are\\nconnected to the 3 feature maps connected to the S2 layer (the first red box in the above figure), and the next 6\\nfeature maps are connected to the S2 layer The 4 feature maps are connected (the second red box in the figure\\nabove), the next 3 feature maps are connected with the 4 feature maps that are not connected at the S2 layer ,\\nand the last is connected with all the feature maps at the S2 layer . The convolution kernel size is still 5 5, so\\nthere are 6 (3 5 5 + 1) + 6 (4 5 5 + 1) + 3 (4 5 5 + 1) +1 (6 5 5 + 1) = 1516 parameters. The image size is 10 10,\\nso there are 151600 connections.', metadata={'source': 'Deep Learning.pdf', 'page': 76}),\n",
       " Document(page_content='The convolution structure of C3 and the first 3 graphs in S2 is shown below:\\nS4 layer-pooling layer (downsampling layer)\\nInput : 10 * 10\\nSampling area : 2 * 2\\nSampling method : 4 inputs are added, multiplied by a trainable parameter , plus a trainable\\noffset. Results via sigmoid\\nSampling type : 16\\nOutput featureMap size : 5 * 5 (10/2)\\nNumber of neurons : 5 5 16 = 400\\nTrainable parameters : 2 * 16 = 32 (the weight of the sum + the of fset)\\nNumber of connections : 16 (2 2 + 1) 5 5 = 2000\\nThe size of each feature map in S4 is 1/4 of the size of the feature map in C3\\nDetailed description:', metadata={'source': 'Deep Learning.pdf', 'page': 77}),\n",
       " Document(page_content='S4 is the pooling layer , the window size is still 2 * 2, a total of 16 feature maps, and the 16 10x10 maps of the\\nC3 layer are pooled in units of 2x2 to obtain 16 5x5 feature maps. This layer has a total of 32 training\\nparameters of 2x16, 5x5x5x16 = 2000 connections.\\nThe connection is similar to the S2 layer .\\nC5 layer-convolution layer\\nInput : All 16 unit feature maps of the S4 layer (all connected to s4)\\nConvolution kernel size : 5 * 5\\nConvolution kernel type : 120\\nOutput featureMap size : 1 * 1 (5-5 + 1)\\nTrainable parameters / connection : 120 (16 5 * 5 + 1) = 48120\\nDetailed description:\\nThe C5 layer is a convolutional layer . Since the size of the 16 images of the S4 layer is 5x5, which is the same\\nas the size of the convolution kernel, the size of the image formed after convolution is 1x1. This results in 120\\nconvolution results. Each is connected to the 16 maps on the previous level. So there are (5x5x16 + 1) x120 =', metadata={'source': 'Deep Learning.pdf', 'page': 78}),\n",
       " Document(page_content='convolution results. Each is connected to the 16 maps on the previous level. So there are (5x5x16 + 1) x120 =\\n48120 parameters, and there are also 48120 connections. The network structure of the C5 layer is as follows:\\nF6 layer-fully connected layer\\nInput : c5 120-dimensional vector\\nCalculation method : calculate the dot product between the input vector and the weight vector ,\\nplus an of fset, and the result is output through the sigmoid function.\\nTrainable parameters : 84 * (120 + 1) = 10164\\nDetailed description:', metadata={'source': 'Deep Learning.pdf', 'page': 78}),\n",
       " Document(page_content='Layer 6 is a fully connected layer . The F6 layer has 84 nodes, corresponding to a 7x12 bitmap, -1 means white,\\n1 means black, so the black and white of the bitmap of each symbol corresponds to a code. The training\\nparameters and number of connections for this layer are (120 + 1) x84 = 10164. The ASCII encoding diagram is\\nas follows:\\nThe connection method of the F6 layer is as follows:\\nOutput layer-fully connected layer\\nThe output layer is also a fully connected layer , with a total of 10 nodes, which respectively represent the\\nnumbers 0 to 9, and if the value of node i is 0, the result of network recognition is the number i. A radial basis\\nfunction (RBF) network connection is used. Assuming x is the input of the previous layer and y is the output of\\nthe RBF , the calculation of the RBF output is:\\nThe value of the above formula w_ij is determined by the bitmap encoding of i, where i ranges from 0 to 9, and j', metadata={'source': 'Deep Learning.pdf', 'page': 79}),\n",
       " Document(page_content='the RBF , the calculation of the RBF output is:\\nThe value of the above formula w_ij is determined by the bitmap encoding of i, where i ranges from 0 to 9, and j\\nranges from 0 to 7 * 12-1. The closer the value of the RBF output is to 0, the closer it is to i, that is, the closer to\\nthe ASCII encoding figure of i, it means that the recognition result input by the current network is the character i.\\nThis layer has 84x10 = 840 parameters and connections.', metadata={'source': 'Deep Learning.pdf', 'page': 79}),\n",
       " Document(page_content='Summary\\nLeNet-5 is a very ef ficient convolutional neural network for handwritten character recognition.\\nConvolutional neural networks can make good use of the structural information of images.\\nThe convolutional layer has fewer parameters, which is also determined by the main characteristics of the\\nconvolutional layer , that is, local connection and shared weights.\\nCode Implementation\\nIn [4]:\\nIn [2]:import keras\\nfrom keras.datasets  import mnist\\nfrom keras.layers import Conv2D, MaxPooling2D ,AveragePooling2D\\nfrom keras.layers import Dense, Flatten\\nfrom keras.models import Sequential\\n# Loading the dataset and perform splitting\\n(x_train, y_train), (x_test, y_test) = mnist.load_data ()\\n# Peforming reshaping operation\\nx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)', metadata={'source': 'Deep Learning.pdf', 'page': 80}),\n",
       " Document(page_content=\"In [3]:\\nLeNet Model\\nIn [8]:\\nOR# Normalization\\nx_train = x_train / 255\\nx_test = x_test / 255\\n \\n# One Hot Encoding\\ny_train = keras.utils.to_categorical (y_train, 10)\\ny_test = keras.utils.to_categorical (y_test, 10)\\n# Building the Model Architecture\\nmodel = Sequential ()\\n# Select 6 feature convolution kernels with a size of 5 * 5 (without offset), and get 66 fe\\n# That is, the number of neurons has been reduced from 10241024 to 28 ∗  28 = 784 28 ∗  28 = \\n# Parameters between input layer and C1 layer: 6 ∗  (5 ∗  5 + 1)\\nmodel.add(Conv2D(6, kernel_size =(5, 5), activation ='tanh', input_shape =(28, 28, 1)))\\n# The input of this layer is the output of the first layer, which is a 28 * 28 * 6 node mat\\n# The size of the filter used in this layer is 2 * 2, and the step length and width are bot\\nmodel.add(MaxPooling2D (pool_size =(2, 2)))\\n# The input matrix size of this layer is 14 * 14 * 6, the filter size used is 5 * 5, and th\", metadata={'source': 'Deep Learning.pdf', 'page': 81}),\n",
       " Document(page_content=\"model.add(MaxPooling2D (pool_size =(2, 2)))\\n# The input matrix size of this layer is 14 * 14 * 6, the filter size used is 5 * 5, and th\\n# The output matrix size of this layer is 10 * 10 * 16. This layer has 5 * 5 * 6 * 16 + 16 \\nmodel.add(Conv2D(16, kernel_size =(5, 5), activation ='tanh'))\\n# The input matrix size of this layer is 10 * 10 * 16. The size of the filter used in this \\nmodel.add(MaxPooling2D (pool_size =(2, 2)))\\n# The input matrix size of this layer is 5 * 5 * 16. This layer is called a convolution lay\\n# So it is not different from the fully connected layer. If the nodes in the 5 * 5 * 16 mat\\n# The number of output nodes in this layer is 120, with a total of 5 * 5 * 16 * 120 + 120 =\\nmodel.add(Flatten())\\nmodel.add(Dense(120, activation ='tanh'))\\n# The number of input nodes in this layer is 120 and the number of output nodes is 84. The \\nmodel.add(Dense(84, activation ='tanh'))\\n# The number of input nodes in this layer is 84 and the number of output nodes is 10. The t\", metadata={'source': 'Deep Learning.pdf', 'page': 81}),\n",
       " Document(page_content=\"model.add(Dense(84, activation ='tanh'))\\n# The number of input nodes in this layer is 84 and the number of output nodes is 10. The t\\nmodel.add(Dense(10, activation ='softmax' ))\", metadata={'source': 'Deep Learning.pdf', 'page': 81}),\n",
       " Document(page_content='In [ ]:\\nIn [9]:\\nIn [10]:Model: \"sequential_1\"  \\n_________________________________________________________________  \\nLayer (type)                 Output Shape              Param #    \\n=================================================================  \\nconv2d_2 (Conv2D)            (None, 24, 24, 6)         156        \\n_________________________________________________________________  \\nmax_pooling2d_2 (MaxPooling2 (None, 12, 12, 6)         0          \\n_________________________________________________________________  \\nconv2d_3 (Conv2D)            (None, 8, 8, 16)          2416       \\n_________________________________________________________________  \\nmax_pooling2d_3 (MaxPooling2 (None, 4, 4, 16)          0          \\n_________________________________________________________________  \\nflatten_1 (Flatten)          (None, 256)               0          \\n_________________________________________________________________  \\ndense_3 (Dense)              (None, 120)               30840', metadata={'source': 'Deep Learning.pdf', 'page': 82}),\n",
       " Document(page_content=\"_________________________________________________________________  \\ndense_3 (Dense)              (None, 120)               30840      \\n_________________________________________________________________  \\ndense_4 (Dense)              (None, 84)                10164      \\n_________________________________________________________________  \\ndense_5 (Dense)              (None, 10)                850        \\n=================================================================  \\nTotal params: 44,426  \\nTrainable params: 44,426  \\nNon-trainable params: 0  \\n_________________________________________________________________  \\n469/469 [==============================] - 22s 46ms/step - loss: 0.6072 - ac\\ncuracy: 0.8285 - val_loss: 0.0840 - val_accuracy: 0.9737  \\nOut[10]:\\n<tensorflow.python.keras.callbacks.History at 0x195a9244748>model = keras.Sequential ()\\n \\nmodel.add(Conv2D(filters=6, kernel_size =(5, 5), activation ='tanh', input_shape =(32,32,1)))\\nmodel.add(AveragePooling2D (2,2))\", metadata={'source': 'Deep Learning.pdf', 'page': 82}),\n",
       " Document(page_content=\"model.add(Conv2D(filters=6, kernel_size =(5, 5), activation ='tanh', input_shape =(32,32,1)))\\nmodel.add(AveragePooling2D (2,2))\\n \\nmodel.add(Conv2D(filters=16, kernel_size =(5, 5), activation ='tanh'))\\nmodel.add(AveragePooling2D ())\\n \\nmodel.add(Flatten())\\n \\nmodel.add(Dense(units=120, activation ='tanh'))\\n \\nmodel.add(Dense(units=84, activation ='tanh'))\\n \\nmodel.add(Dense(units=10, activation  = 'softmax' ))\\nmodel.summary()\\nmodel.compile(loss=keras.metrics.categorical_crossentropy , optimizer =keras.optimizers .Adam(\\nmodel.fit(x_train, y_train, batch_size =128, epochs=1, verbose=1, validation_data =(x_test, y\", metadata={'source': 'Deep Learning.pdf', 'page': 82}),\n",
       " Document(page_content=\"In [11]:\\n313/313 [==============================] - 2s 7ms/step - loss: 0.0840 - accu\\nracy: 0.9737  \\nTest Loss: 0.08401492983102798  \\nTest accuracy: 0.9736999869346619  score = model.evaluate (x_test, y_test)\\nprint('Test Loss:' , score[0])\\nprint('Test accuracy:' , score[1])\", metadata={'source': 'Deep Learning.pdf', 'page': 83}),\n",
       " Document(page_content='ALEXNET\\nThe most important features of the AlexNet paper are:\\nAs the model had to train 60 million parameters (which is quite a lot), it was prone to overfitting. According\\nto the paper , the usage of Dropout and Data Augmentation significantly helped in reducing overfitting. The\\nfirst and second fully connected layers in the architecture thus used a dropout of 0.5 for the purpose.\\nArtificially increasing the number of images through data augmentation helped in the expansion of the\\ndataset dynamically during runtime, which helped the model generalize better .\\nAnother distinct factor was using the ReLU activation function instead of tanh or sigmoid, which resulted in\\nfaster training times (a decrease in training time by 6 times). Deep Learning Networks usually employ\\nReLU non-linearity to achieve faster training times as the others start saturating when they hit higher\\nactivation values.\\nAlexNet is a Classic type of Convolutional Neural Network, and it came into existence after the 2012', metadata={'source': 'Deep Learning.pdf', 'page': 84}),\n",
       " Document(page_content='activation values.\\nAlexNet is a Classic type of Convolutional Neural Network, and it came into existence after the 2012\\nImageNet challenge. The network architecture is given below :\\nModel Explanation :\\nThe Input to this model have the dimensions 227x227x3 follwed by a Convolutional Layer with 96 filters of\\n11x11 dimensions and having a ‘same’ padding and a stride of 4. The resulting output dimensions are\\ngiven as :', metadata={'source': 'Deep Learning.pdf', 'page': 84}),\n",
       " Document(page_content='floor(((n + 2*padding - filter)/stride) + 1 ) * floor(((n + 2*padding —  \\nfilter)/stride) + 1)  \\nNote : This formula is for square input with height = width = n\\nExplaining the first Layer with input 227x227x3 and Convolutional layer with 96 filters of 1 1x11 , ‘valid’\\npadding and stride = 4 , output dims will be\\n= floor(((227 + 0–1 1)/4) + 1) * floor(((227 + 0–1 1)/4) + 1)\\n= floor((216/4) + 1) * floor((216/4) + 1)\\n= floor(54 + 1) * floor(54 + 1)\\n= 55 * 55\\nSince number of filters = 96 , thus output of first Layer is : 55x55x96\\nContinuing we have the MaxPooling layer (3, 3) with the stride of 2,making the output size decrease to\\n27x27x96, followed by another Convolutional Layer with 256, (5,5) filters and ‘same’ padding, that is, the\\noutput height and width are retained as the previous layer thus output from this layer is 27x27x256.\\nNext we have the MaxPooling again ,reducing the size to 13x13x256. Another Convolutional Operation', metadata={'source': 'Deep Learning.pdf', 'page': 85}),\n",
       " Document(page_content='output height and width are retained as the previous layer thus output from this layer is 27x27x256.\\nNext we have the MaxPooling again ,reducing the size to 13x13x256. Another Convolutional Operation\\nwith 384, (3,3) filters having same padding is applied twice giving the output as 13x13x384, followed by\\nanother Convulutional Layer with 256 , (3,3) filters and same padding resulting in 13x13x256 output.\\nThis is MaxPooled and dimensions are reduced to 6x6x256. Further the layer is Flatten out and 2 Fully\\nConnected Layers with 4096 units each are made which is further connected to 1000 units softmax layer .\\nThe network is used for classifying much large number of classes as per our requirement. However in our\\ncase, we will make the output softmax layer with 6 units as we ahve to classify into 6 classes. The softmax\\nlayer gives us the probablities for each class to which an Input Image might belong.\\nSize /\\nOperationFilter Depth Stride Padding Number of Parameters Forward Computation', metadata={'source': 'Deep Learning.pdf', 'page': 85}),\n",
       " Document(page_content='layer gives us the probablities for each class to which an Input Image might belong.\\nSize /\\nOperationFilter Depth Stride Padding Number of Parameters Forward Computation\\n3 227 227\\nConv1 + Relu 11 11 96 4 (11113 + 1) 96=34944 (11113 + 1) 96 55 55=105705600\\n96 55 55\\nMax Pooling 3 3 2\\n96 27 27\\nNorm\\nConv2 + Relu 5 5 256 1 2 (5 5 96 + 1) 256=614656 (5 5 96 + 1) 256 27 27=448084224\\n256 27 27\\nMax Pooling 3 3 2\\n256 13 13\\nNorm\\nConv3 + Relu 3 3 384 1 1 (3 3 256 + 1) 384=885120 (3 3 256 + 1) 384 13 13=149585280\\n384 13 13\\nConv4 + Relu 3 3 384 1 1 (3 3 384 + 1) 384=1327488 (3 3 384 + 1) 384 13 13=224345472\\n384 13 13\\nConv5 + Relu 3 3 256 1 1 (3 3 384 + 1) 256=884992 (3 3 384 + 1) 256 13 13=149563648\\n256 13 13', metadata={'source': 'Deep Learning.pdf', 'page': 85}),\n",
       " Document(page_content='Size /\\nOperationFilter Depth Stride Padding Number of Parameters Forward Computation\\nMax Pooling 3 3 2\\n256 6 6\\nDropout (rate\\n0.5)\\nFC6 + Relu 256 6 6 4096=37748736 256 6 6 4096=37748736\\n4096\\nDropout (rate\\n0.5)\\nFC7 + Relu 4096 4096=16777216 4096 4096=16777216\\n4096\\nFC8 + Relu 4096 1000=4096000 4096 1000=4096000\\n1000 classes\\nOverall 62369152=62.3 million 1135906176=1.1 billion\\nConv VS FCConv:3.7million (6%) , FC: 58.6\\nmillion (94% )Conv: 1.08 billion (95%) , FC: 58.6\\nmillion (5%)\\nWhy does AlexNet achieve better results?\\n1. Relu activation function is used.\\nRelu function: f (x) = max (0, x)\\nReLU-based deep convolutional networks are trained several times faster than tanh and sigmoid- based\\nnetworks. The following figure shows the number of iterations for a four-layer convolutional network based on\\nCIFAR-10 that reached 25% training error in tanh and ReLU:', metadata={'source': 'Deep Learning.pdf', 'page': 86}),\n",
       " Document(page_content='1. Standardization ( Local Response Normalization )\\nAfter using ReLU f (x) = max (0, x), you will find that the value after the activation function has no range like the\\ntanh and sigmoid functions, so a normalization will usually be done after ReLU, and the LRU is a steady\\nproposal (Not sure here, it should be proposed?) One method in neuroscience is called \"Lateral inhibition\",\\nwhich talks about the ef fect of active neurons on its surrounding neurons.\\n1. Dropout\\nDropout is also a concept often said, which can ef fectively prevent overfitting of neural networks. Compared to\\nthe general linear model, a regular method is used to prevent the model from overfitting. In the neural network,\\nDropout is implemented by modifying the structure of the neural network itself. For a certain layer of neurons,\\nrandomly delete some neurons with a defined probability , while keeping the individuals of the input layer and', metadata={'source': 'Deep Learning.pdf', 'page': 87}),\n",
       " Document(page_content='randomly delete some neurons with a defined probability , while keeping the individuals of the input layer and\\noutput layer neurons unchanged, and then update the parameters according to the learning method of the\\nneural network. In the next iteration, rerandom Remove some neurons until the end of training.\\n1. Enhanced Data ( Data Augmentation )\\nIn deep learning, when the amount of data is not large enough, there are generally 4 solutions:', metadata={'source': 'Deep Learning.pdf', 'page': 87}),\n",
       " Document(page_content='Data augmentation- artificially increase the size of the training set-create a batch of \"new\" data\\nfrom existing data by means of translation, flipping, noise\\nRegularization——The relatively small amount of data will cause the model to overfit, making\\nthe training error small and the test error particularly large. By adding a regular term after the\\nLoss Function , the overfitting can be suppressed. The disadvantage is that a need is\\nintroduced Manually adjusted hyper-parameter .\\nDropout- also a regularization method. But dif ferent from the above, it is achieved by randomly\\nsetting the output of some neurons to zero\\nUnsupervised Pre-training- use Auto-Encoder or RBM\\'s convolution form to do unsupervised\\npre-training layer by layer , and finally add a classification layer to do supervised Fine-T uning\\nIn [1]:\\n#Importing library\\nimport keras\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Activation , Dropout, Flatten, Conv2D, MaxPooling2D', metadata={'source': 'Deep Learning.pdf', 'page': 88}),\n",
       " Document(page_content='In [1]:\\n#Importing library\\nimport keras\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Activation , Dropout, Flatten, Conv2D, MaxPooling2D\\nfrom keras.layers.normalization  import BatchNormalization\\nimport numpy as np\\n \\nnp.random.seed(1000)', metadata={'source': 'Deep Learning.pdf', 'page': 88}),\n",
       " Document(page_content=\"In [6]:\\nmodel = Sequential () \\n \\n# 1st Convolutional Layer \\nmodel.add(Conv2D(filters = 96, input_shape  = (224, 224, 3), kernel_size  = (11, 11), strides\\nmodel.add(Activation ('relu')) \\n# Max-Pooling \\nmodel.add(MaxPooling2D (pool_size  = (2, 2), strides = (2, 2), padding = 'valid')) \\n# Batch Normalisation \\nmodel.add(BatchNormalization ()) \\n \\n# 2nd Convolutional Layer \\nmodel.add(Conv2D(filters = 256, kernel_size  = (11, 11), strides = (1, 1), padding = 'valid'\\nmodel.add(Activation ('relu')) \\n# Max-Pooling \\nmodel.add(MaxPooling2D (pool_size  = (2, 2), strides = (2, 2), padding = 'valid')) \\n# Batch Normalisation \\nmodel.add(BatchNormalization ()) \\n \\n# 3rd Convolutional Layer \\nmodel.add(Conv2D(filters = 384, kernel_size  = (3, 3), strides = (1, 1), padding = 'valid'))\\nmodel.add(Activation ('relu')) \\n# Batch Normalisation \\nmodel.add(BatchNormalization ()) \\n \\n# 4th Convolutional Layer \\nmodel.add(Conv2D(filters = 384, kernel_size  = (3, 3), strides = (1, 1), padding = 'valid'))\", metadata={'source': 'Deep Learning.pdf', 'page': 89}),\n",
       " Document(page_content=\"# Batch Normalisation \\nmodel.add(BatchNormalization ()) \\n \\n# 4th Convolutional Layer \\nmodel.add(Conv2D(filters = 384, kernel_size  = (3, 3), strides = (1, 1), padding = 'valid'))\\nmodel.add(Activation ('relu')) \\n# Batch Normalisation \\nmodel.add(BatchNormalization ()) \\n \\n# 5th Convolutional Layer \\nmodel.add(Conv2D(filters = 256, kernel_size  = (3, 3), strides = (1, 1), padding = 'valid'))\\nmodel.add(Activation ('relu')) \\n# Max-Pooling \\nmodel.add(MaxPooling2D (pool_size  = (2, 2), strides = (2, 2),padding = 'valid')) \\n# Batch Normalisation \\nmodel.add(BatchNormalization ()) \\n \\n# Flattening \\nmodel.add(Flatten()) \\n \\n# 1st Dense Layer \\nmodel.add(Dense(4096, input_shape  = (224*224*3, ))) \\nmodel.add(Activation ('relu')) \\n# Add Dropout to prevent overfitting \\nmodel.add(Dropout(0.4)) \\n# Batch Normalisation \\nmodel.add(BatchNormalization ()) \\n \\n# 2nd Dense Layer \\nmodel.add(Dense(4096)) \\nmodel.add(Activation ('relu')) \\n# Add Dropout \\nmodel.add(Dropout(0.4)) \\n# Batch Normalisation\", metadata={'source': 'Deep Learning.pdf', 'page': 89}),\n",
       " Document(page_content=\"# Batch Normalisation \\nmodel.add(BatchNormalization ()) \\n \\n# 2nd Dense Layer \\nmodel.add(Dense(4096)) \\nmodel.add(Activation ('relu')) \\n# Add Dropout \\nmodel.add(Dropout(0.4)) \\n# Batch Normalisation \\nmodel.add(BatchNormalization ()) \\n \\n# Output Softmax Layer \\nmodel.add(Dense(10))\", metadata={'source': 'Deep Learning.pdf', 'page': 89}),\n",
       " Document(page_content='In [7]:\\nIn [ ]:Model: \"sequential\"  \\n_________________________________________________________________  \\nLayer (type)                 Output Shape              Param #    \\n=================================================================  \\nconv2d (Conv2D)              (None, 54, 54, 96)        34944      \\n_________________________________________________________________  \\nactivation (Activation)      (None, 54, 54, 96)        0          \\n_________________________________________________________________  \\nmax_pooling2d (MaxPooling2D) (None, 27, 27, 96)        0          \\n_________________________________________________________________  \\nbatch_normalization (BatchNo (None, 27, 27, 96)        384        \\n_________________________________________________________________  \\nconv2d_1 (Conv2D)            (None, 17, 17, 256)       2973952    \\n_________________________________________________________________  \\nactivation_1 (Activation)    (None, 17, 17, 256)       0', metadata={'source': 'Deep Learning.pdf', 'page': 90}),\n",
       " Document(page_content=\"_________________________________________________________________  \\nactivation_1 (Activation)    (None, 17, 17, 256)       0          \\n_________________________________________________________________  \\nmax_pooling2d_1 (MaxPooling2 (None, 8, 8, 256)         0          \\n_________________________________________________________________  \\nbatch_normalization_1 (Batch (None, 8, 8, 256)         1024       model.add(Activation ('softmax' )) \\n \\n#Model Summary\\nmodel.summary()\", metadata={'source': 'Deep Learning.pdf', 'page': 90}),\n",
       " Document(page_content='VGG16\\nVGG16 is a convolution neural net (CNN ) architecture which was used to win ILSVR(Imagenet)\\ncompetition in 2014.\\nIt is considered to be one of the excellent vision model architecture till date. Most unique thing about\\nVGG16 is that instead of having a large number of hyper-parameter they focused on having convolution\\nlayers of 3x3 filter with a stride 1 and always used same padding and maxpool layer of 2x2 filter of stride 2.\\nIt follows this arrangement of convolution and max pool layers consistently throughout the whole\\narchitecture. In the end it has 2 FC(fully connected layers) followed by a softmax for output. The 16 in\\nVGG16 refers to it has 16 layers that have weights. This network is a pretty large network and it has about\\n138 million (approx) parameters.\\nThe following are the layers of the model:\\nConvolutional Layers = 13\\nPooling Layers = 5\\nDense Layers = 3\\nLet us explore the layers in detail:\\n1. Input:  Image of dimensions (224, 224, 3).', metadata={'source': 'Deep Learning.pdf', 'page': 91}),\n",
       " Document(page_content='pg (, ,)\\n2. Convolution Layer Conv1:\\n- Conv1-1: 64 filters  \\n- Conv1-2: 64 filters and Max Pooling  \\n- Image dimensions: (224, 224)  \\n3. Convolution layer Conv2: Now, we increase the filters to 128\\n- Input Image dimensions: (112,112)  \\n- Conv2-1: 128 filters  \\n- Conv2-2: 128 filters and Max Pooling  \\n4. Convolution Layer Conv3:  Again, double the filters to 256, and now add another convolution layer\\n- Input Image dimensions: (56,56)  \\n- Conv3-1: 256 filters  \\n- Conv3-2: 256 filters  \\n- Conv3-3: 256 filters and Max Pooling  \\n5. Convolution Layer Conv4:  Similar to Conv3, but now with 512 filters\\n- Input Image dimensions: (28, 28)  \\n- Conv4-1: 512 filters  \\n- Conv4-2: 512 filters  \\n- Conv4-3: 512 filters and Max Pooling  \\n6. Convolution Layer Conv5:  Same as Conv4\\n- Input Image dimensions: (14, 14)  \\n- Conv5-1: 512 filters  \\n- Conv5-2: 512 filters  \\n- Conv5-3: 512 filters and Max Pooling  \\n- The output dimensions here are (7, 7). At this point, we flatten the output', metadata={'source': 'Deep Learning.pdf', 'page': 92}),\n",
       " Document(page_content='- Conv5-1: 512 filters  \\n- Conv5-2: 512 filters  \\n- Conv5-3: 512 filters and Max Pooling  \\n- The output dimensions here are (7, 7). At this point, we flatten the output\\nof this layer to generate a feature vector  \\n7. Fully Connected/Dense FC1:  4096 nodes, generating a feature vector of size(1, 4096)\\n8. Fully ConnectedDense FC2:  4096 nodes generating a feature vector of size(1, 4096)\\n9. Fully Connected /Dense FC3:  4096 nodes, generating 1000 channels for 1000 classes. This is then\\npassed on to a Softmax activation function\\n10. Output layer\\nVGG16 contains 16 layers and VGG19 contains 19 layers. A series of VGGs are exactly the\\nsame in the last three fully connected layers. The overall structure includes 5 sets of\\nconvolutional layers, followed by a MaxPool. The dif ference is that more and more cascaded\\nconvolutional layers are included in the five sets of convolutional layers .', metadata={'source': 'Deep Learning.pdf', 'page': 92}),\n",
       " Document(page_content='Each convolutional layer in AlexNet contains only one convolution, and the size of the\\nconvolution kernel is 7 7 ,. In VGGNet, each convolution layer contains 2 to 4 convolution\\noperations. The size of the convolution kernel is 3 3, the convolution step size is 1, the pooling\\nkernel is 2 * 2, and the step size is 2. The most obvious improvement of VGGNet is to reduce\\nthe size of the convolution kernel and increase the number of convolution layers.\\nUsing multiple convolution layers with smaller convolution kernels instead of a larger\\nconvolution layer with convolution kernels can reduce parameters on the one hand, and the\\nauthor believes that it is equivalent to more non-linear mapping, which increases the Fit\\nexpression ability .', metadata={'source': 'Deep Learning.pdf', 'page': 93}),\n",
       " Document(page_content='Two consecutive 3 3 convolutions are equivalent to a 5 5 receptive field, and three are\\nequivalent to 7 7. The advantages of using three 3 3 convolutions instead of one 7 7\\nconvolution are twofold : one, including three ReLu layers instead of one , makes the decision\\nfunction more discriminative; and two, reducing parameters . For example, the input and output\\nare all C channels. 3 convolutional layers using 3 3 require 3 (3 3 C C) = 27 C C, and 1\\nconvolutional layer using 7 7 requires 7 7 C C = 49C C. This can be seen as applying a kind of\\nregularization to the 7 7 convolution, so that it is decomposed into three 3 3 convolutions.\\nThe 1 1 convolution layer is mainly to increase the non-linearity of the decision function without\\naffecting the receptive field of the convolution layer . Although the 1 1 convolution operation is\\nlinear , ReLu adds non-linearity .\\nSome basic questions\\nQ1: Why can 3 3x3 convolutions replace 7x7 convolutions?\\nAnswer 1', metadata={'source': 'Deep Learning.pdf', 'page': 94}),\n",
       " Document(page_content='linear , ReLu adds non-linearity .\\nSome basic questions\\nQ1: Why can 3 3x3 convolutions replace 7x7 convolutions?\\nAnswer 1\\n3 3x3 convolutions, using 3 non-linear activation functions, increasing non-linear expression capabilities,\\nmaking the segmentation plane more separable Reduce the number of parameters. For the convolution kernel\\nof C channels, 7x7 contains parameters , and the number of 3 3x3 parameters is greatly reduced.\\nQ2: The role of 1x1 convolution kernel\\nAnswer 2\\nIncrease the nonlinearity of the model without af fecting the receptive field 1x1 winding machine is equivalent to\\nlinear transformation, and the non-linear activation function plays a non-linear role\\nQ3: The effect of network depth on results (in the same year , Google also independently released the\\nnetwork GoogleNet with a depth of 22 layers)\\nAnswer 3', metadata={'source': 'Deep Learning.pdf', 'page': 94}),\n",
       " Document(page_content='VGG and GoogleNet models are deep Small convolution VGG only uses 3x3, while GoogleNet uses 1x1, 3x3,\\n5x5, the model is more complicated (the model began to use a large convolution kernel to reduce the\\ncalculation of the subsequent machine layer)\\nImplement On Keras\\nIn [3]:\\nIn [4]:#Importing library\\nimport keras\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Activation , Dropout, Flatten, Conv2D, MaxPooling2D ,MaxPool2\\nfrom keras.layers.normalization  import BatchNormalization\\nimport numpy as np\\n \\nnp.random.seed(1000)\\n \\nmodel = Sequential ()\\nmodel.add(Conv2D(input_shape =(224,224,3),filters=64,kernel_size =(3,3),padding=\"same\", activ\\nmodel.add(Conv2D(filters=64,kernel_size =(3,3),padding=\"same\", activation =\"relu\"))\\nmodel.add(MaxPool2D (pool_size =(2,2),strides=(2,2)))\\n \\n \\nmodel.add(Conv2D(filters=128, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(Conv2D(filters=128, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))', metadata={'source': 'Deep Learning.pdf', 'page': 95}),\n",
       " Document(page_content='model.add(Conv2D(filters=128, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(Conv2D(filters=128, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(MaxPool2D (pool_size =(2,2),strides=(2,2)))\\n \\n \\nmodel.add(Conv2D(filters=256, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(Conv2D(filters=256, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(Conv2D(filters=256, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(MaxPool2D (pool_size =(2,2),strides=(2,2)))\\n \\n \\nmodel.add(Conv2D(filters=512, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(Conv2D(filters=512, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(Conv2D(filters=512, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(MaxPool2D (pool_size =(2,2),strides=(2,2)))\\n \\n \\nmodel.add(Conv2D(filters=512, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))', metadata={'source': 'Deep Learning.pdf', 'page': 95}),\n",
       " Document(page_content='model.add(MaxPool2D (pool_size =(2,2),strides=(2,2)))\\n \\n \\nmodel.add(Conv2D(filters=512, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(Conv2D(filters=512, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(Conv2D(filters=512, kernel_size =(3,3), padding=\"same\", activation =\"relu\"))\\nmodel.add(MaxPool2D (pool_size =(2,2),strides=(2,2)))\\n \\nmodel.add(Flatten())\\nmodel.add(Dense(units=4096,activation =\"relu\"))\\nmodel.add(Dense(units=4096,activation =\"relu\"))\\nmodel.add(Dense(units=2, activation =\"softmax\" ))', metadata={'source': 'Deep Learning.pdf', 'page': 95}),\n",
       " Document(page_content='In [6]:\\nHere I have started with initialising the model by specifying that the model is a sequential model. AfterThe output of this will be the summary of the model which I just created.  \\nModel: \"sequential_1\"  \\n_________________________________________________________________  \\nLayer (type)                 Output Shape              Param #    \\n=================================================================  \\nconv2d_2 (Conv2D)            (None, 224, 224, 64)      1792       \\n_________________________________________________________________  \\nconv2d_3 (Conv2D)            (None, 224, 224, 64)      36928      \\n_________________________________________________________________  \\nmax_pooling2d (MaxPooling2D) (None, 112, 112, 64)      0          \\n_________________________________________________________________  \\nconv2d_4 (Conv2D)            (None, 112, 112, 128)     73856      \\n_________________________________________________________________', metadata={'source': 'Deep Learning.pdf', 'page': 96}),\n",
       " Document(page_content='conv2d_4 (Conv2D)            (None, 112, 112, 128)     73856      \\n_________________________________________________________________  \\nconv2d_5 (Conv2D)            (None, 112, 112, 128)     147584     \\n_________________________________________________________________  \\nmax_pooling2d_1 (MaxPooling2 (None, 56, 56, 128)       0          \\n_________________________________________________________________  \\nconv2d_6 (Conv2D)            (None, 56, 56, 256)       295168     \\n_________________________________________________________________  \\nconv2d_7 (Conv2D)            (None, 56, 56, 256)       590080     \\n_________________________________________________________________  \\nconv2d_8 (Conv2D)            (None, 56, 56, 256)       590080     \\n_________________________________________________________________  \\nmax_pooling2d_2 (MaxPooling2 (None, 28, 28, 256)       0          \\n_________________________________________________________________', metadata={'source': 'Deep Learning.pdf', 'page': 96}),\n",
       " Document(page_content='max_pooling2d_2 (MaxPooling2 (None, 28, 28, 256)       0          \\n_________________________________________________________________  \\nconv2d_9 (Conv2D)            (None, 28, 28, 512)       1180160    \\n_________________________________________________________________  \\nconv2d_10 (Conv2D)           (None, 28, 28, 512)       2359808    \\n_________________________________________________________________  \\nconv2d_11 (Conv2D)           (None, 28, 28, 512)       2359808    \\n_________________________________________________________________  \\nmax_pooling2d_3 (MaxPooling2 (None, 14, 14, 512)       0          \\n_________________________________________________________________  \\nconv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808    \\n_________________________________________________________________  \\nconv2d_13 (Conv2D)           (None, 14, 14, 512)       2359808    \\n_________________________________________________________________', metadata={'source': 'Deep Learning.pdf', 'page': 96}),\n",
       " Document(page_content='conv2d_13 (Conv2D)           (None, 14, 14, 512)       2359808    \\n_________________________________________________________________  \\nconv2d_14 (Conv2D)           (None, 14, 14, 512)       2359808    \\n_________________________________________________________________  \\nmax_pooling2d_4 (MaxPooling2 (None, 7, 7, 512)         0          \\n_________________________________________________________________  \\nflatten (Flatten)            (None, 25088)             0          \\n_________________________________________________________________  \\ndense (Dense)                (None, 4096)              102764544  \\n_________________________________________________________________  \\ndense_1 (Dense)              (None, 4096)              16781312   \\n_________________________________________________________________  \\ndense_2 (Dense)              (None, 2)                 8194       \\n=================================================================  \\nTotal params: 134,268,738', metadata={'source': 'Deep Learning.pdf', 'page': 96}),\n",
       " Document(page_content='dense_2 (Dense)              (None, 2)                 8194       \\n=================================================================  \\nTotal params: 134,268,738  \\nTrainable params: 134,268,738  \\nNon-trainable params: 0  \\n_________________________________________________________________  print(\"The output of this will be the summary of the model which I just created.\" )\\nmodel.summary()', metadata={'source': 'Deep Learning.pdf', 'page': 96}),\n",
       " Document(page_content='initialising the model I add\\n→ 2 x convolution layer of 64 channel of 3x3 kernal and same padding\\n→ 1 x maxpool layer of 2x2 pool size and stride 2x2\\n→ 2 x convolution layer of 128 channel of 3x3 kernal and same padding\\n→ 1 x maxpool layer of 2x2 pool size and stride 2x2\\n→ 3 x convolution layer of 256 channel of 3x3 kernal and same padding\\n→ 1 x maxpool layer of 2x2 pool size and stride 2x2\\n→ 3 x convolution layer of 512 channel of 3x3 kernal and same padding\\n→ 1 x maxpool layer of 2x2 pool size and stride 2x2\\n→ 3 x convolution layer of 512 channel of 3x3 kernal and same padding\\n→ 1 x maxpool layer of 2x2 pool size and stride 2x2\\nI also add relu(Rectified Linear Unit) activation to each layers so that all the negative values are not passed to\\nthe next layer .\\nAfter creating all the convolution I pass the data to the dense layer so for that I flatten the vector which comes\\nout of the convolutions and add\\n→ 1 x Dense layer of 4096 units\\n→ 1 x Dense layer of 4096 units', metadata={'source': 'Deep Learning.pdf', 'page': 97}),\n",
       " Document(page_content='out of the convolutions and add\\n→ 1 x Dense layer of 4096 units\\n→ 1 x Dense layer of 4096 units\\n→ 1 x Dense Softmax layer of 2 units\\nI will use RELU activation for both the dense layer of 4096 units so that I stop forwarding negative values\\nthrough the network. I use a 2 unit dense layer in the end with softmax activation as I have 2 classes to predict\\nfrom in the end which are dog and cat. The softmax layer will output the value between 0 and 1 based on the\\nconfidence of the model that which class the images belongs to.\\nAfter the creation of softmax layer the model is finally prepared.\\nIn [ ]:', metadata={'source': 'Deep Learning.pdf', 'page': 97}),\n",
       " Document(page_content='Introduction\\nResNet is a network structure proposed by the He Kaiming, Sun Jian and others of Microsoft Research\\nAsia in 2015, and won the first place in the ILSVRC-2015 classification task. At the same time, it won the\\nfirst place in ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation\\ntasks. It was a sensation at the time.\\nResNet, also known as residual neural network, refers to the idea of   adding residual learning to the traditional\\nconvolutional neural network, which solves the problem of gradient dispersion and accuracy degradation\\n(training set) in deep networks, so that the network can get more and more The deeper , both the accuracy and\\nthe speed are controlled.\\nDeep Residual Learning for Image Recognition Original link : ResNet Paper\\n(https://arxiv .org/pdf/1512.03385.pdf)\\nThe problem caused by increasing depth\\nThe first problem brought by increasing depth is the problem of gradient explosion / dissipation . This is', metadata={'source': 'Deep Learning.pdf', 'page': 98}),\n",
       " Document(page_content='(https://arxiv .org/pdf/1512.03385.pdf)\\nThe problem caused by increasing depth\\nThe first problem brought by increasing depth is the problem of gradient explosion / dissipation . This is\\nbecause as the number of layers increases, the gradient of backpropagation in the network will become\\nunstable with continuous multiplication, and become particularly large or special. small. Among them , the\\nproblem of gradient dissipation often occurs .\\nIn order to overcome gradient dissipation, many solutions have been devised, such as using BatchNorm,\\nreplacing the activation function with ReLu, using Xaiver initialization, etc. It can be said that gradient\\ndissipation has been well solved\\nAnother problem of increasing depth is the problem of network degradation, that is, as the depth increases,\\nthe performance of the network will become worse and worse, which is directly reflected in the decrease in', metadata={'source': 'Deep Learning.pdf', 'page': 98}),\n",
       " Document(page_content='the performance of the network will become worse and worse, which is directly reflected in the decrease in\\naccuracy on the training set. The residual network article solves this problem. And after this problem is\\nsolved, the depth of the network has increased by several orders of magnitude.\\nDegradation of deep network\\nWith network depth increasing, accuracy gets saturated (which might be unsurprising) and then\\ndegrades rapidly . Unexpectedly , such degradation is not caused by overfitting, and adding more\\nlayers to a favored deep model leads to higher training error .', metadata={'source': 'Deep Learning.pdf', 'page': 98}),\n",
       " Document(page_content='The above figure is the error rate of the training set classified by the network on the CIF AR10-data set with\\nthe increase of the network depth . It can be seen that if we directly stack the convolutional layers, as the\\nnumber of layers increases, the error rate increases significantly . The trend is that the deepest 56-layer\\nnetwork has the worst accuracy . W e verified it on the VGG network. For the CIF AR-10 dataset, it took 5\\nminutes on the 18-layer VGG network to get the full network training. The 80% accuracy rate was\\nachieved, and the 34-layer VGG model took 8 minutes to get the 72% accuracy rate. The problem of\\nnetwork degradation does exist.\\nThe decrease in the training set error rate indicates that the problem of degradation is not caused by\\noverfitting. The specific reason is that it is left for further study . The author\\'s other paper \"Identity Mappings\\nin Deep Residual Networks\" proved the occurrence of degradation. It is because the optimization', metadata={'source': 'Deep Learning.pdf', 'page': 99}),\n",
       " Document(page_content='in Deep Residual Networks\" proved the occurrence of degradation. It is because the optimization\\nperformance is not good, which indicates that the deeper the network, the more dif ficult the reverse\\ngradient is to conduct.\\nDeep Residual Networks\\nFrom 10 to 100 layers\\nWe can imagine that when we simply stack the network directly to a particularly long length, the internal\\ncharacteristics of the network have reached the best situation in one of the layers. At this time, the remaining\\nlayers should not make any changes to the characteristics and learn automatically . The form of identity\\nmapping . That is to say , for a particularly deep deep network, the solution space of the shallow form of the\\nnetwork should be a subset of the solution space of the deep network, in other words, a network deeper than\\nthe shallow network will not have at least W orse ef fect, but this is not true because of network degradation.', metadata={'source': 'Deep Learning.pdf', 'page': 99}),\n",
       " Document(page_content='the shallow network will not have at least W orse ef fect, but this is not true because of network degradation.\\nThen, we settle for the second best. In the case of network degradation, if we do not add depth, we can improve\\nthe accuracy . Can we at least make the deep network achieve the same performance as the shallow network,\\nthat is, let the layers behind the deep network achieve at least The role of identity mapping . Based on this idea,\\nthe author proposes a residual module to help the network achieve identity mapping.', metadata={'source': 'Deep Learning.pdf', 'page': 99}),\n",
       " Document(page_content='To understand ResNet, we must first understand what kind of problems will occur when the  \\nnetwork becomes deeper.\\nThe first problem brought by increasing the network depth is the disappearance and explosion of the\\ngradient.\\nThis problem was successfully solved after Szegedy proposed the BN (Batch Normalization)  structure. The\\nBN layer can normalize the output of each layer . The size can still be kept stable after the reverse layer transfer ,\\nand it will not be too small or too large.\\nIs it easy to converge after adding BN and then increasing the depth?\\nThe answer is still negative . The author mentioned the second problem- the degradation problem : when the\\nlevel reaches a certain level, the accuracy will saturate and then decline rapidly . This decline is not caused by\\nthe disappearance of the gradient. It is not caused by overfit, but because the network is so complicated that it\\nis difficult to achieve the ideal error rate by unconstrained stocking training alone.', metadata={'source': 'Deep Learning.pdf', 'page': 100}),\n",
       " Document(page_content='is difficult to achieve the ideal error rate by unconstrained stocking training alone.\\nThe degradation problem is not a problem of the network structure itself, but is caused by the current insuf ficient\\ntraining methods. The currently widely used training methods, whether it is SGD, AdaGrad, or RMSProp,\\ncannot reach the theoretically optimal convergence result after the network depth becomes larger .\\nWe can also prove that as long as there is an ideal training method, deeper networks will definitely perform\\nbetter than shallow networks.\\nThe proof process is also very simple : Suppose that several layers are added behind a network A  \\nto form a new network B. If the added level is just an identity mapping of the output of  \\nA, that is, the output of A is after the level of B becomes the output of B, there is no  \\nchange, so the error rates of network A and network B are equal, which proves that the  \\ndeepened network will not be worse than the network before deepening.', metadata={'source': 'Deep Learning.pdf', 'page': 100}),\n",
       " Document(page_content='change, so the error rates of network A and network B are equal, which proves that the  \\ndeepened network will not be worse than the network before deepening.\\nHe Kaiming proposed a residual structure to implement the above identity mapping (Below Figure): In addition\\nto the normal convolution layer output, the entire module has a branch directly connecting the input to the\\noutput. The output and the output of the convolution do The final output is obtained by arithmetic addition. The\\nformula is H (x) = F (x) + x, x is the input, F (x) is the output of the convolution branch, and H (x) is the output of\\nthe entire structure. It can be shown that if all parameters in the F (x) branch are 0, H (x) is an identity mapping.\\nThe residual structure artificially creates an identity map, which can make the entire structure converge in the\\ndirection of the identity map, ensuring that the final error rate will not become worse because the depth', metadata={'source': 'Deep Learning.pdf', 'page': 100}),\n",
       " Document(page_content='direction of the identity map, ensuring that the final error rate will not become worse because the depth\\nbecomes larger . If a network can achieve the desired result by simply setting the parameter values by hand,\\nthen this structure can easily converge to the result through training. This is a rule that is unsuccessful when\\ndesigning complex networks. Recall that in order to restore the original distribution after BN processing, the\\nformula y = rx + delta is used. When r is manually set to standard deviation and delta is the mean, y is the\\ndistribution before BN processing. This is the use of this Rules.', metadata={'source': 'Deep Learning.pdf', 'page': 100}),\n",
       " Document(page_content='What does residual learning mean?\\nThe idea of residual learning is the above picture, which can be understood as a block, defined as follows:\\nThe residual learning block contains two branches or two mappings:\\n1. Identity mapping refers to the curved curve on the right side of the figure above. As its name implies,\\nidentity mapping refers to its own mapping, which is x itself;\\n1. F(x) Residual mapping refers to another branch, that is, part. This part is called residual mapping (y -x) .\\nWhat role does the residual module play in back propagation?\\nThe residual module will significantly reduce the parameter value in the module, so that the parameters in\\nthe network have a more sensitive response ability to the loss of reverse conduction, although the\\nfundamental It does not solve the problem that the loss of backhaul is too small, but it reduces the\\nparameters. Relatively speaking, it increases the ef fect of backhaul loss and also generates a certain\\nregularization ef fect.', metadata={'source': 'Deep Learning.pdf', 'page': 101}),\n",
       " Document(page_content='parameters. Relatively speaking, it increases the ef fect of backhaul loss and also generates a certain\\nregularization ef fect.\\nSecondly , because there are branches of the identity mapping in the forward process, the gradient\\nconduction in the back-propagation process also has more simple paths , and the gradient can be\\ntransmitted to the previous module after only one relu.\\nThe so-called backpropagation is that the network outputs a value, and then compares it with the real value\\nto an error loss. At the same time, the loss is changed to change the parameter . The returned loss depends\\non the original loss and gradient. Since the purpose is to change the parameter , The problem is that if the\\nintensity of changing the parameter is too small, the value of the parameter can be reduced, so that the\\nloss of the intensity of changing the parameter is relatively greater .\\nTherefore, the most important role of the residual module is to change the way of forward and backward', metadata={'source': 'Deep Learning.pdf', 'page': 101}),\n",
       " Document(page_content='loss of the intensity of changing the parameter is relatively greater .\\nTherefore, the most important role of the residual module is to change the way of forward and backward\\ninformation transmission, thereby greatly promoting the optimization of the network.\\nUsing the four criteria proposed by Inceptionv3, we will use them again to improve the residual module.\\nUsing criterion 3, the dimensionality reduction before spatial aggregation will not cause information loss, so\\nthe same method is also used here, adding 1 * 1 convolution The kernel is used to increase the non-\\nlinearity and reduce the depth of the output to reduce the computational cost. Y ou get the form of a residual\\nmodule that becomes a bottleneck. The figure above shows the basic form on the left and the bottleneck\\nform on the right.', metadata={'source': 'Deep Learning.pdf', 'page': 101}),\n",
       " Document(page_content='To sum up, the shortcut module will help the features in the network perform identity mapping in the\\nforward process, and help conduct gradients in the reverse process, so that deeper models can be\\nsuccessfully trained.\\nWhy can the residual learning solve the problem of \"the accuracy of the network deepening\\ndeclines\"?\\nFor a neural network model, if the model is optimal, then training can easily optimize the residual mapping to 0,\\nand only identity mapping is left at this time. No matter how you increase the depth, the network will always be\\nin an optimal state in theory . Because it is equivalent to all the subsequent added networks to carry information\\ntransmission along the identity mapping (self), it can be understood that the number of layers behind the\\noptimal network is discarded (without the ability to extract features), and it does not actually play a role. . In this\\nway, the performance of the network will not decrease with increasing depth.', metadata={'source': 'Deep Learning.pdf', 'page': 102}),\n",
       " Document(page_content='optimal network is discarded (without the ability to extract features), and it does not actually play a role. . In this\\nway, the performance of the network will not decrease with increasing depth.\\nThe author used two types of data, ImageNet  and CIFAR, to prove the ef fectiveness of ResNet:\\nThe first is ImageNet. The authors compared the training ef fect of ResNet structure and traditional structure with\\nthe same number of layers. The left side of Figure is a VGG-19 network with a traditional structure (each\\nfollowed by BN), the middle is a 34-layer network with a traditional structure (each followed by BN), and the\\nright side is 34 layers ResNet (the solid line indicates a direct connection, and the dashed line indicates a\\ndimensional change using 1x1 convolution to match the number of features of the input and output). Figure 3\\nshows the results after training these types of networks.', metadata={'source': 'Deep Learning.pdf', 'page': 102}),\n",
       " Document(page_content='dimensional change using 1x1 convolution to match the number of features of the input and output). Figure 3\\nshows the results after training these types of networks.\\nThe data on the left shows that the 34-layer network (red line) with the traditional structure has a higher error\\nrate than the VGG-19 (blue-green line). Because the BN structure is added to each layer Therefore, the high\\nerror is not caused by the gradient disappearing after the level is increased, but by the degradation problem; the\\nResNet structure on the right side of Figure 3 shows that the 34-layer network (red line) has a higher error rate\\nthan the 18-layer network (blue-green line). Low , this is because the ResNet structure has overcome the\\ndegradation problem. In addition, the final error rate of the ResNet 18-layer network on the right is similar to the\\nerror rate of the traditional 18-layer network on the left. This is because the 18-layer network is simpler and can', metadata={'source': 'Deep Learning.pdf', 'page': 102}),\n",
       " Document(page_content='error rate of the traditional 18-layer network on the left. This is because the 18-layer network is simpler and can\\nconverge to a more ideal result even without the ResNet structure.', metadata={'source': 'Deep Learning.pdf', 'page': 102}),\n",
       " Document(page_content='The ResNet structure like the left side of Fig. 4 is only used for shallow ResNet networks. If there are many\\nnetwork layers, the dimensions near the output end of the network will be very large. Still using the structure on\\nthe left side of Fig. 4 will cause a huge amount of calculation. For deeper networks, we all use the bottleneck\\nstructure on the right side of Figure 4, first using a 1x1 convolution for dimensionality reduction, then 3x3\\nconvolution, and finally using 1x1 dimensionality to restore the original dimension.\\nIn practice, considering the cost of the calculation, the residual block is calculated and optimized, that is, the\\ntwo 3x3 convolution layers are replaced with 1x1 + 3x3 + 1x1 , as shown below . The middle 3x3 convolutional\\nlayer in the new structure first reduces the calculation under one dimensionality-reduced 1x1 convolutional layer\\n, and then restores it under another 1x1 convolutional layer , both maintaining accuracy and reducing the\\namount of calculation .', metadata={'source': 'Deep Learning.pdf', 'page': 104}),\n",
       " Document(page_content=', and then restores it under another 1x1 convolutional layer , both maintaining accuracy and reducing the\\namount of calculation .\\nThis is equivalent to reducing the amount of parameters for the same number of layers , so it can be extended\\nto deeper models. So the author proposed ResNet with 50, 101 , and 152 layers , and not only did not have\\ndegradation problems, the error rate was greatly reduced, and the computational complexity was also kept at a\\nvery low level .', metadata={'source': 'Deep Learning.pdf', 'page': 104}),\n",
       " Document(page_content='At this time, the error rate of ResNet has already dropped other networks a few streets, but it does not seem to\\nbe satisfied. Therefore, a more abnormal 1202 layer network has been built. For such a deep network,\\noptimization is still not dif ficult, but it appears The problem of overfitting is quite normal. The author also said\\nthat the 1202 layer model will be further improved in the future.\\nThere are two main types of blocks are used in a ResNet, depending mainly on\\nwhether the input/output dimensions are the same or different.\\n1. Identity Block\\nThe identity block is the standard block used in ResNets and corresponds to the case where the input activation\\nhas the same dimension as the output activation.\\n2. Convolutional Block\\nWe can use this type of block when the input and output dimensions don’t match up. The dif ference with the\\nidentity block is that there is a CONV2D layer in the shortcut path.\\nResNet-50', metadata={'source': 'Deep Learning.pdf', 'page': 105}),\n",
       " Document(page_content='We can use this type of block when the input and output dimensions don’t match up. The dif ference with the\\nidentity block is that there is a CONV2D layer in the shortcut path.\\nResNet-50\\nThe ResNet-50 model consists of 5 stages each with a convolution and Identity block. Each convolution block\\nhas 3 convolution layers and each identity block also has 3 convolution layers. The ResNet-50 has over 23\\nmillion trainable parameters.\\nDiffrent V ariants  : -', metadata={'source': 'Deep Learning.pdf', 'page': 105}),\n",
       " Document(page_content='Below is the transcript of resnet, winning the championship at ImageNet2015\\nCode Implement\\nIn [1]:\\nimport cv2\\nimport numpy as np\\nimport os\\nfrom keras.preprocessing .image import ImageDataGenerator\\nfrom keras import backend as K\\nimport keras\\nfrom keras.models import Sequential , Model,load_model\\nfrom keras.optimizers  import SGD\\nfrom keras.callbacks  import EarlyStopping ,ModelCheckpoint\\nfrom keras.layers import Input, Add, Dense, Activation , ZeroPadding2D , BatchNormalization , \\nfrom keras.preprocessing  import image\\nfrom keras.initializers  import glorot_uniform', metadata={'source': 'Deep Learning.pdf', 'page': 106}),\n",
       " Document(page_content=\"In [2]:\\nIn [3]:def identity_block (X, f, filters, stage, block):\\n   \\n    conv_name_base  = 'res' + str(stage) + block + '_branch'\\n    bn_name_base  = 'bn' + str(stage) + block + '_branch'\\n    F1, F2, F3 = filters\\n \\n    X_shortcut  = X\\n   \\n    X = Conv2D(filters=F1, kernel_size =(1, 1), strides=(1, 1), padding='valid', name=conv_n\\n    X = BatchNormalization (axis=3, name=bn_name_base  + '2a')(X)\\n    X = Activation ('relu')(X)\\n \\n    X = Conv2D(filters=F2, kernel_size =(f, f), strides=(1, 1), padding='same', name=conv_na\\n    X = BatchNormalization (axis=3, name=bn_name_base  + '2b')(X)\\n    X = Activation ('relu')(X)\\n \\n    X = Conv2D(filters=F3, kernel_size =(1, 1), strides=(1, 1), padding='valid', name=conv_n\\n    X = BatchNormalization (axis=3, name=bn_name_base  + '2c')(X)\\n \\n    X = Add()([X, X_shortcut ])# SKIP Connection\\n    X = Activation ('relu')(X)\\n \\n    return X\\ndef convolutional_block (X, f, filters, stage, block, s=2):\", metadata={'source': 'Deep Learning.pdf', 'page': 107}),\n",
       " Document(page_content=\"X = Add()([X, X_shortcut ])# SKIP Connection\\n    X = Activation ('relu')(X)\\n \\n    return X\\ndef convolutional_block (X, f, filters, stage, block, s=2):\\n   \\n    conv_name_base  = 'res' + str(stage) + block + '_branch'\\n    bn_name_base  = 'bn' + str(stage) + block + '_branch'\\n \\n    F1, F2, F3 = filters\\n \\n    X_shortcut  = X\\n \\n    X = Conv2D(filters=F1, kernel_size =(1, 1), strides=(s, s), padding='valid', name=conv_n\\n    X = BatchNormalization (axis=3, name=bn_name_base  + '2a')(X)\\n    X = Activation ('relu')(X)\\n \\n    X = Conv2D(filters=F2, kernel_size =(f, f), strides=(1, 1), padding='same', name=conv_na\\n    X = BatchNormalization (axis=3, name=bn_name_base  + '2b')(X)\\n    X = Activation ('relu')(X)\\n \\n    X = Conv2D(filters=F3, kernel_size =(1, 1), strides=(1, 1), padding='valid', name=conv_n\\n    X = BatchNormalization (axis=3, name=bn_name_base  + '2c')(X)\\n \\n    X_shortcut  = Conv2D(filters=F3, kernel_size =(1, 1), strides=(s, s), padding='valid', na\", metadata={'source': 'Deep Learning.pdf', 'page': 107}),\n",
       " Document(page_content=\"X = BatchNormalization (axis=3, name=bn_name_base  + '2c')(X)\\n \\n    X_shortcut  = Conv2D(filters=F3, kernel_size =(1, 1), strides=(s, s), padding='valid', na\\n    X_shortcut  = BatchNormalization (axis=3, name=bn_name_base  + '1')(X_shortcut )\\n \\n    X = Add()([X, X_shortcut ])\\n    X = Activation ('relu')(X)\\n \\n    return X\", metadata={'source': 'Deep Learning.pdf', 'page': 107}),\n",
       " Document(page_content=\"In [4]:\\nIn [6]:\\nIn [7]:\\nIn [8]:def ResNet50 (input_shape =(224, 224, 3)):\\n \\n    X_input = Input(input_shape )\\n \\n    X = ZeroPadding2D ((3, 3))(X_input)\\n \\n    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer =glorot_uniform (\\n    X = BatchNormalization (axis=3, name='bn_conv1' )(X)\\n    X = Activation ('relu')(X)\\n    X = MaxPooling2D ((3, 3), strides=(2, 2))(X)\\n \\n    X = convolutional_block (X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\\n    X = identity_block (X, 3, [64, 64, 256], stage=2, block='b')\\n    X = identity_block (X, 3, [64, 64, 256], stage=2, block='c')\\n \\n \\n    X = convolutional_block (X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\\n    X = identity_block (X, 3, [128, 128, 512], stage=3, block='b')\\n    X = identity_block (X, 3, [128, 128, 512], stage=3, block='c')\\n    X = identity_block (X, 3, [128, 128, 512], stage=3, block='d')\\n \\n    X = convolutional_block (X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\", metadata={'source': 'Deep Learning.pdf', 'page': 108}),\n",
       " Document(page_content=\"X = identity_block (X, 3, [128, 128, 512], stage=3, block='d')\\n \\n    X = convolutional_block (X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\\n    X = identity_block (X, 3, [256, 256, 1024], stage=4, block='b')\\n    X = identity_block (X, 3, [256, 256, 1024], stage=4, block='c')\\n    X = identity_block (X, 3, [256, 256, 1024], stage=4, block='d')\\n    X = identity_block (X, 3, [256, 256, 1024], stage=4, block='e')\\n    X = identity_block (X, 3, [256, 256, 1024], stage=4, block='f')\\n \\n    X = X = convolutional_block (X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\\n    X = identity_block (X, 3, [512, 512, 2048], stage=5, block='b')\\n    X = identity_block (X, 3, [512, 512, 2048], stage=5, block='c')\\n \\n    X = AveragePooling2D (pool_size =(2, 2), padding='same')(X)\\n    \\n    model = Model(inputs=X_input, outputs=X, name='ResNet50' )\\n \\n    return model\\nbase_model  = ResNet50 (input_shape =(224, 224, 3))\\nheadModel  = base_model .output\\nheadModel  = Flatten()(headModel )\", metadata={'source': 'Deep Learning.pdf', 'page': 108}),\n",
       " Document(page_content=\"return model\\nbase_model  = ResNet50 (input_shape =(224, 224, 3))\\nheadModel  = base_model .output\\nheadModel  = Flatten()(headModel )\\nheadModel =Dense(256, activation ='relu', name='fc1',kernel_initializer =glorot_uniform (seed=0\\nheadModel =Dense(128, activation ='relu', name='fc2',kernel_initializer =glorot_uniform (seed=0\\nheadModel  = Dense( 1,activation ='sigmoid' , name='fc3',kernel_initializer =glorot_uniform (see\\nmodel = Model(inputs=base_model .input, outputs=headModel )\", metadata={'source': 'Deep Learning.pdf', 'page': 108}),\n",
       " Document(page_content='In [9]:\\nIn [ ]:Model: \"model\"  \\n__________________________________________________________________________\\n________________________  \\nLayer (type)                    Output Shape         Param #     Connected  \\nto                      \\n==========================================================================\\n========================  \\ninput_1 (InputLayer)            [(None, 224, 224, 3) 0                     \\n__________________________________________________________________________\\n________________________  \\nzero_padding2d (ZeroPadding2D)  (None, 230, 230, 3)  0           input_1\\n[0][0]                     \\n__________________________________________________________________________\\n________________________  \\nconv1 (Conv2D)                  (None, 112, 112, 64) 9472        zero_padd\\ning2d[0][0]              \\n__________________________________________________________________________\\n________________________  \\nbn_conv1 (BatchNormalization)   (None, 112, 112, 64) 256         conv1[0]', metadata={'source': 'Deep Learning.pdf', 'page': 109}),\n",
       " Document(page_content='__________________________________________________________________________\\n________________________  \\nbn_conv1 (BatchNormalization)   (None, 112, 112, 64) 256         conv1[0]\\n[0]model.summary()', metadata={'source': 'Deep Learning.pdf', 'page': 109}),\n",
       " Document(page_content='Introduction\\nInception net achieved a milestone in CNN classifiers when previous models were just going deeper to\\nimprove the performance and accuracy but compromising the computational cost. The Inception network,\\non the other hand, is heavily engineered.\\nIt uses a lot of tricks to push performance, both in terms of speed and accuracy . It is the winner of the\\nImageNet Large Scale V isual Recognition Competition in 2014, an image classification competition, which\\nhas a significant improvement over ZFNet (The winner in 2013), AlexNet (The winner in 2012) and has\\nrelatively lower error rate compared with the VGGNet (1st runner-up in 2014).\\nThe major issues faced by deeper CNN models such as VGGNet were:\\n1. Although, previous networks such as VGG achieved a remarkable accuracy on the ImageNet dataset,\\ndeploying these kinds of models is highly computationally expensive because of the deep architecture.', metadata={'source': 'Deep Learning.pdf', 'page': 110}),\n",
       " Document(page_content='deploying these kinds of models is highly computationally expensive because of the deep architecture.\\n2. Very deep networks are susceptible to overfitting. It is also hard to pass gradient updates through the entire\\nnetwork.\\nMotivation for Inception Network\\nInstead of deciding whether to use a 1x1 convolution, or a 3x3 or a 5x5 Convolution, or whether to use a\\nPooling layer - Why not use all of them?\\nIn the example above, all the filters are applied to the input to generate a stacked outpu, whioch contains the\\noutput of each filter stacked on top of each other . The Padding is kept at ‘same’ to ensure that the output from\\nall the filters are of the same size.\\nDisadvantage: Huge memory cost\\nSolving the problem of memory cost\\nFor example, the computational cost of th 5x5 filer in the above diagram:', metadata={'source': 'Deep Learning.pdf', 'page': 110}),\n",
       " Document(page_content='Input: 28x28x192\\nFilter: Conv 5x5x192, same, 32\\nOutput: 28x28x32\\nTotal number of calculations = (28 28 32) (5 5 * 192 ) = 120 Million !!\\nUsing 1x1 Convolution to reduce computation cost\\nA 1x1 convolution is added before the 5x5 cvonvolution -= Also called a bottleneck layer\\nTotal number of calculations = [(28 28 16) (1 1 192)] + [(28 28 32) (5 5 16)] = 12.4 Million !! (earlier the cost was\\n120 Million)', metadata={'source': 'Deep Learning.pdf', 'page': 111}),\n",
       " Document(page_content='The popular versions are as follows:\\nInception v1.\\nInception v2 and Inception v3.\\nInception v4 and Inception-ResNet.\\nInception V1\\nThis architecture has 22 layers in total! Using the dimension-reduced inception module, a neural network\\narchitecture is constructed. This is popularly known as GoogLeNet (Inception v1).\\nGoogLeNet has 9 such inception modules fitted linearly . It is 22 layers deep (27, including the pooling\\nlayers). At the end of the architecture, fully connected layers were replaced by a global average pooling\\nwhich calculates the average of every feature map. This indeed dramatically declines the total number of\\nparameters.\\nThe above are the explainof V1\\nProblems of Inception V1 architecture:', metadata={'source': 'Deep Learning.pdf', 'page': 112}),\n",
       " Document(page_content='Inception V1 have sometimes use convolutions such as 5*5 that causes the input dimensions to decrease\\nby a large margin. This causes the neural network some accuracy decrease. The reason behind that the\\nneural network is susceptible to information loss if the input dimension decreases too drastically .\\nFurthermore, there is also complexity decrease when we use bigger convolutions like 5×5 as compared to\\n3×3.W e can go further in terms of factorization i.e. that we can divide a 3×3 convolution into an asymmetric\\nconvolution of 1×3 then followed by 3×1 convolution. This is equivalent to sliding a two-layer network with\\nthe same receptive field as in a 3×3 convolution but 33% more cheaper than 3×3. This factorization does\\nnot work well for early layers when input dimensions are big but only when the input size mxm (m is\\nbetween 12 and 20). According to the Inception V1 architecture, the auxiliary classifier improves the', metadata={'source': 'Deep Learning.pdf', 'page': 113}),\n",
       " Document(page_content='between 12 and 20). According to the Inception V1 architecture, the auxiliary classifier improves the\\nconvergence of the network. They argue that it can help reduce the ef fect of the vanishing gradient problem\\nin deep network by pushing the useful gradient to earlier layers (to reduce the loss). But, the authors of this\\npaper found that this classifier didn’t improve the convergence very much early in the training.\\nInception V2 And Inception V3\\nInception-v2(2015)\\nArchitectural Changes in Inception V2:\\nIn the Inception V2 architecture. The 5×5 convolution is replaced by the two 3×3 convolutions. This also\\ndecreases computational time and thus increase computational speed because a 5×5 convolution is 2.78 more\\nexpensive than 3×3 convolution. So, Using two 3×3 layers instead of 5×5 increases the performance of\\narchitecture.\\nThis architecture also converts nXn factorization into 1xn and nx1 factorization. As we discuss above that a 3×3', metadata={'source': 'Deep Learning.pdf', 'page': 113}),\n",
       " Document(page_content='architecture.\\nThis architecture also converts nXn factorization into 1xn and nx1 factorization. As we discuss above that a 3×3\\nconvolution can be converted into 1×3 then followed by 3×1 convolution which is 33% cheaper in terms of\\ncomputational complexity as compared to 3×3.', metadata={'source': 'Deep Learning.pdf', 'page': 113}),\n",
       " Document(page_content='To deal with the problem of the representational bottleneck, the feature banks of the module were expanded\\ninstead of making it deeper . This would prevent the loss of information that causes when we make it deeper .\\nThe above three principles were used to build three dif ferent types of inception modules (Let’ s call them\\nmodules A,B and C in the order they were introduced. These names are introduced for clarity , and not the\\nofficial names). The architecture is as follows:', metadata={'source': 'Deep Learning.pdf', 'page': 114}),\n",
       " Document(page_content='Algorithm advantages:\\n1. Improved learning rate  : In the BN model, a higher learning rate is used to accelerate training\\nconvergence, but it will not cause other ef fects. Because if the scale of each layer is dif ferent, then the\\nlearning rate required by each layer is dif ferent. The scale of the same layer dimension often also needs\\ndifferent learning rates. Usually , the minimum learning is required to ensure the loss function to decrease,\\nbut The BN layer keeps the scale of each layer and dimension consistent, so you can directly use a higher\\nlearning rate for optimization.\\n1. Remove the dropout layer  : The BN layer makes full use of the goals of the dropout layer . Remove the\\ndropout layer from the BN-Inception model, but no overfitting will occur .\\n1. Decrease the attenuation coefficient of L2 weight  : Although the L2 loss controls the overfitting of the\\nInception model, the loss of weight has been reduced by five times in the BN-Inception model.', metadata={'source': 'Deep Learning.pdf', 'page': 115}),\n",
       " Document(page_content='Inception model, the loss of weight has been reduced by five times in the BN-Inception model.\\n1. Accelerate the decay of the learning rate  : When training the Inception model, we let the learning rate\\ndecrease exponentially . Because our network is faster than Inception, we will increase the speed of\\nreducing the learning rate by 6 times.\\n1. Remove the local response layer  : Although this layer has a certain role, but after the BN layer is added,\\nthis layer is not necessary .', metadata={'source': 'Deep Learning.pdf', 'page': 115}),\n",
       " Document(page_content='1. Scramble training samples more thoroughly  : We scramble training samples, which can prevent the\\nsame samples from appearing in a mini-batch. This can improve the accuracy of the validation set by 1%,\\nwhich is the advantage of the BN layer as a regular term. In our method, random selection is more ef fective\\nwhen the model sees dif ferent samples each time.\\n1. To reduce image distortion : Because BN network training is faster and observes each training sample\\nless often, we want the model to see a more realistic image instead of a distorted image.\\nInception-v3-2015\\nThis architecture focuses, how to use the convolution kernel two or more smaller size of the convolution kernel\\nto replace, but also the introduction of asymmetrical layers i.e. a convolution dimensional convolution  has\\nalso been proposed for pooling layer Some remedies that can cause loss of spatial information; there are ideas\\nsuch as label-smoothing , BN-ahxiliary  .', metadata={'source': 'Deep Learning.pdf', 'page': 116}),\n",
       " Document(page_content='also been proposed for pooling layer Some remedies that can cause loss of spatial information; there are ideas\\nsuch as label-smoothing , BN-ahxiliary  .\\nExperiments were performed on inputs with dif ferent resolutions . The results show that although low-resolution\\ninputs require more time to train, the accuracy and high-resolution achieved are not much dif ferent.\\nThe computational cost is reduced while improving the accuracy of the network.\\nGeneral Design Principles\\nWe will describe some design principles that have been proposed through extensive experiments with dif ferent\\narchitectural designs for convolutional networks. At this point, full use of the following principles can be\\nguessed, and some additional experiments in the future will be necessary to estimate their accuracy and\\neffectiveness.\\n1. Prevent bottlenecks in characterization  . The so-called bottleneck of feature description is that a large', metadata={'source': 'Deep Learning.pdf', 'page': 116}),\n",
       " Document(page_content='effectiveness.\\n1. Prevent bottlenecks in characterization  . The so-called bottleneck of feature description is that a large\\nproportion of features are compressed in the middle layer (such as using a pooling operation). This\\noperation will cause the loss of feature space information and the loss of features. Although the operation\\nof pooling in CNN is important, there are some methods that can be used to avoid this loss as much as\\npossible (I note: later hole convolution operations ).\\n2. The higher the dimensionality of the feature, the faster the training converges  . That is, the\\nindependence of features has a great relationship with the speed of model convergence. The more\\nindependent features, the more thoroughly the input feature information is decomposed. It is easier to\\nconverge if the correlation is strong. Hebbin principle : fire together , wire together .\\n3. Reduce the amount of calculation through dimensionality reduction  . In v1, the feature is first reduced', metadata={'source': 'Deep Learning.pdf', 'page': 116}),\n",
       " Document(page_content='converge if the correlation is strong. Hebbin principle : fire together , wire together .\\n3. Reduce the amount of calculation through dimensionality reduction  . In v1, the feature is first reduced\\nby 1x1 convolutional dimensionality reduction. There is a certain correlation between dif ferent dimensions.\\nDimension reduction can be understood as a lossless or low-loss compression. Even if the dimensions are\\nreduced, the correlation can still be used to restore its original information.\\n4. Balance the depth and width of the network  . Only by increasing the depth and width of the network in\\nthe same proportion can the performance of the model be maximized.\\nInception V3 is similar to and contains all the features of Inception V2 with following changes/additions:\\n   1. Use of RMSprop optimizer.  \\n   2. Batch Normalization in the fully connected layer of Auxiliary classifier.  \\n   3. Use of 7×7 factorized Convolution', metadata={'source': 'Deep Learning.pdf', 'page': 116}),\n",
       " Document(page_content='1. Use of RMSprop optimizer.  \\n   2. Batch Normalization in the fully connected layer of Auxiliary classifier.  \\n   3. Use of 7×7 factorized Convolution  \\n   4. Label Smoothing Regularization: It is a method to regularize the classifier  \\nby estimating the effect of label-dropout during training. It prevents the classif\\nier to predict too confidently a class. The addition of label smoothing gives 0.2%  \\nimprovement from the error rate.', metadata={'source': 'Deep Learning.pdf', 'page': 116}),\n",
       " Document(page_content='Inception-v4-2016\\nInception V4 was introduced in combination with Inception-ResNet by thee researchers a Google in 2016.\\nThe main aim of the paper was to reduce the complexity of Inception V3 model which give the state-of-the-\\nart accuracy on ILSVRC 2015 challenge. This paper also explores the possibility of using residual networks\\non Inception model.\\nIntroduction\\nResidual conn works well when training very deep networks. Because the Inception network architecture can be\\nvery deep, it is reasonable to use residual conn instead of concat.\\nCompared with v3, Inception-v4 has more unified simplified structure and more inception modules.\\nThe big picture of Inception-v4:', metadata={'source': 'Deep Learning.pdf', 'page': 117}),\n",
       " Document(page_content='Fig9 is an overall picture, and Fig3,4,5,6,7,8 are all local structures. For the specific structure of each module,\\nsee the end of the article.\\nResidual Inception Blocks\\nFor the residual version in the Inception network, we use an Inception module that consumes less than the\\noriginal Inception. The convolution kernel (followed by 1x1) of each Inception module is used to modify the\\ndimension, which can compensate the reduction of the Inception dimension to some extent.\\nOne is named Inception-ResNet-v1 , which is consistent with the calculation cost of Inception-v3. One is\\nnamed Inception-ResNet-v2 , which is consistent with the calculation cost of Inception-v4.\\nFigure 15 shows the structure of both. However , Inception-v4 is actually slower in practice, probably because it\\nhas more layers.', metadata={'source': 'Deep Learning.pdf', 'page': 118}),\n",
       " Document(page_content='Another small technique is that we use the BN layer in the header of the traditional layer in the Inception-\\nResNet module, but not in the header of the summations. ** There is reason to believe that the BN layer is\\neffective. But in order to add more Inception modules, we made a compromise between the two.\\nInception-ResNet-v1\\nInception-ResNet-v2\\nScaling of the Residuals\\nThis paper finds that when the number of convolution kernels exceeds 1,000 , the residual variants will start to\\nshow instability , and the network will die in the early stages of training, which means that the last layer before\\nthe average pooling layer is in the V ery few iterations start with just a zero value . This situation cannot be', metadata={'source': 'Deep Learning.pdf', 'page': 119}),\n",
       " Document(page_content=\"prevented by reducing the learning rate or by adding a BN layer . Hekaiming's ResNet article also mentions this\\nphenomenon.\\nThis article finds that scale can stabilize the training process before adding the residual module to the activation\\nlayer . This article sets the scale coef ficient between 0.1 and 0.3.\\nIn order to prevent the occurrence of unstable training of deep residual networks, He suggested in the article\\nthat it is divided into two stages of training. The first stage is called warm-up (preheating) , that is, training the\\nmodel with a very low learning first. In the second stage, a higher learning rate is used. And this article finds\\nthat if the convolution sum is very high, even a learning rate of 0.00001 cannot solve this training instability\\nproblem, and the high learning rate will also destroy the ef fect. But this article considers scale residuals to be\\nmore reliable than warm-up.\", metadata={'source': 'Deep Learning.pdf', 'page': 120}),\n",
       " Document(page_content='problem, and the high learning rate will also destroy the ef fect. But this article considers scale residuals to be\\nmore reliable than warm-up.\\nEven if scal is not strictly necessary , it has no ef fect on the final accuracy , but it can stabilize the training\\nprocess.\\nConclusion\\nInception-ResNet-v1 : a network architecture combining inception module and resnet module with similar\\ncalculation cost to Inception-v3;\\nInception-ResNet-v2 : A more expensive but better performing network architecture.\\nInception-v4 : A pure inception module, without residual connections, but with performance similar to Inception-\\nResNet-v2.', metadata={'source': 'Deep Learning.pdf', 'page': 120}),\n",
       " Document(page_content='Q. Why do we use an RNN instead of a simple neural network?\\nWe use Recurrent Neural Networks mostly in sequential data.W e use RNN over standard neural networks due\\nto the following reasons :\\nIn case of sequential data, the inputs and outputs can be of dif ferent lengths. For e.g. , in sentiment\\nanalysis, we map the input sentences to one number describing the sentiment of the text.\\nStandard neural network does not share features learnt across dif ferent positions of text. For e.g. , in\\nnamed entity recognition(identifying names of person in sentences), suppose we identify Henry occurs in\\nfirst position as name, we would want the algorithm to use this information in case Henry occurs later again\\nin the sentence. W e want things learnt in one part to generalize in others parts in sequence data.\\nThe parameters required for handling text will be very large in case of Standard neural networks. RNN\\nrequires much less parameters to learn.\\nRecurrent Neural Network', metadata={'source': 'Deep Learning.pdf', 'page': 121}),\n",
       " Document(page_content='The parameters required for handling text will be very large in case of Standard neural networks. RNN\\nrequires much less parameters to learn.\\nRecurrent Neural Network\\nRecurrent Neural Network(RNN) are a type of Neural Network where the output from previous step are fed\\nas input to the current step.\\nIn traditional neural networks, all the inputs and outputs are independent of each other , but in cases like\\nwhen it is required to predict the next word of a sentence, the previous words are required and hence there\\nis a need to remember the previous words.\\nThus RNN came into existence, which solved this issue with the help of a Hidden Layer . The main and\\nmost important feature of RNN is Hidden state, which remembers some information about a sequence.\\nRNN have a “memory” which remembers all information about what has been calculated. It uses the same\\nparameters for each input as it performs the same task on all the inputs or hidden layers to produce the\\noutput.', metadata={'source': 'Deep Learning.pdf', 'page': 121}),\n",
       " Document(page_content='parameters for each input as it performs the same task on all the inputs or hidden layers to produce the\\noutput.\\nThis reduces the complexity of parameters, unlike other neural networks.', metadata={'source': 'Deep Learning.pdf', 'page': 121}),\n",
       " Document(page_content='Types of RNN\\nOne-to-one:\\nThis is also called Plain Neural networks. It deals with a fixed size of the input to the fixed size of output, where\\nthey are independent of previous information/output.\\nExample: Image classification.\\nOne-to-Many:\\nIt deals with a fixed size of information as input that gives a sequence of data as output.\\nExample: Image Captioning takes the image as input and outputs a sentence of words.\\nMany-to-One:\\nIt takes a sequence of information as input and outputs a fixed size of the output.\\nExample: sentiment analysis where any sentence is classified as expressing the positive or negative sentiment.\\nMany-to-Many:\\nIt takes a Sequence of information as input and processes the recurrently outputs as a Sequence of data.\\nExample: Machine T ranslation, where the RNN reads any sentence in English and then outputs the sentence in\\nFrench.\\nBidirectional Many-to-Many:\\nSynced sequence input and output. Notice that in every case are no pre-specified constraints on the lengths', metadata={'source': 'Deep Learning.pdf', 'page': 122}),\n",
       " Document(page_content='French.\\nBidirectional Many-to-Many:\\nSynced sequence input and output. Notice that in every case are no pre-specified constraints on the lengths\\nsequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.\\nExample: V ideo classification where we wish to label every frame of the video.', metadata={'source': 'Deep Learning.pdf', 'page': 122}),\n",
       " Document(page_content=\"S.no CNN RNN\\n1 CNN  stands for Convolutional Neural Network .RNN  stands for Recurrent Neural\\nNetwork .\\n2 CNN is considered to be more potent than RNN.RNN includes less feature\\ncompatibility when compared to CNN.\\n3 CNN is ideal for images and video processing.RNN is ideal for text and speech\\nAnalysis.\\n4 It is suitable for spatial data like images.RNN is used for temporal data, also\\ncalled sequential data.\\n5 The network takes fixed-size inputs and generates fixed size outputs.RNN can handle arbitrary input/\\noutput lengths.\\n6CNN is a type of feed-forward artificial neural network with variations of\\nmultilayer perceptron's designed to use minimal amounts of preprocessing.RNN, unlike feed-forward neural\\nnetworks- can use their internal\\nmemory to process arbitrary\\nsequences of inputs.\\n7CNN's use of connectivity patterns between the neurons. CNN is af fected by the\\norganization of the animal visual cortex , whose individual neurons are\", metadata={'source': 'Deep Learning.pdf', 'page': 123}),\n",
       " Document(page_content=\"sequences of inputs.\\n7CNN's use of connectivity patterns between the neurons. CNN is af fected by the\\norganization of the animal visual cortex , whose individual neurons are\\narranged in such a way that they can respond to overlapping regions in the\\nvisual field.Recurrent neural networks use time-\\nseries information- what a user spoke\\nlast would impact what he will speak\\nnext.\", metadata={'source': 'Deep Learning.pdf', 'page': 123}),\n",
       " Document(page_content='Q. What is the Problem W ith Simple RNN ?\\nAns:- Lets take Many-Many (Same Length)\\nHere yi4 value depends a lot on xi4 and 03 and less depends on xi1 and 01 which is a limitation of this\\nsimple RNN because in real world application yi4 may depends more on xi1 and 01 and less xi4 and 03.\\nthis is called long term dependency .\\nSimple RNN Cant handle long term dependency .\\nlong term dependency - Later output depends a lot on earlier input i.e. ( yi4 -> xi1 )\\nSo that small change in yi4 should create similar change in weight of xi1 result vanish the gradient. in\\nsimple RNN with Sigmoid and tanh later output nodes of network are less sensitive to the input this is\\nhappen due to vanishing gradient problem.', metadata={'source': 'Deep Learning.pdf', 'page': 124}),\n",
       " Document(page_content='If a sequence is long enough, they’ll have a hard time carrying information from earlier time steps to later\\nones. So if you are trying to process a paragraph of text to do predictions, RNN’ s may leave out important\\ninformation from the beginning.\\nDuring back propagation, recurrent neural networks suf fer from the vanishing gradient problem. Gradients\\nare values used to update a neural networks weights. The vanishing gradient problem is when the gradient\\nshrinks as it back propagates through time. If a gradient value becomes extremely small, it doesn’t\\ncontribute too much learning.\\nSo in recurrent neural networks, layers that get a small gradient update stops learning. Those are usually\\nthe earlier layers. So because these layers don’t learn, RNN’ s can forget what it seen in longer sequences,\\nthus having a short-term memory . If you want to know more about the mechanics of recurrent neural\\nnetworks in general, you can read my previous post here.', metadata={'source': 'Deep Learning.pdf', 'page': 125}),\n",
       " Document(page_content='thus having a short-term memory . If you want to know more about the mechanics of recurrent neural\\nnetworks in general, you can read my previous post here.\\nTo overcome this we use LSTM (Long Short T erm Memory) And GRU (Gated Recurrent Unit)\\nLSTM (Long Short Term Memory)\\nIt takes care both long and short term dependency .\\n1. Input gate- It discover which value from input should be used to modify the memory . Sigmoid function\\ndecides which values to let through 0 or 1. And tanh function gives weightage to the values which are\\npassed, deciding their level of importance ranging from -1 to 1.  \\n \\n \\n2. Forget gate- It discover the details to be discarded from the block. A sigmoid function decides it. It looks at\\nthe previous state (ht-1) and the content input (Xt) and outputs a number between 0(omit this) and 1(keep\\nthis) for each number in the cell state Ct-1.  \\n \\n \\n3. Output gate- The input and the memory of the block are used to decide the output. Sigmoid function', metadata={'source': 'Deep Learning.pdf', 'page': 125}),\n",
       " Document(page_content='this) for each number in the cell state Ct-1.  \\n \\n \\n3. Output gate- The input and the memory of the block are used to decide the output. Sigmoid function\\ndecides which values to let through 0 or 1. And tanh function decides which values to let through 0, 1. And\\ntanh function gives weightage to the values which are passed, deciding their level of importance ranging\\nfrom -1 to 1 and multiplied with an output of sigmoid.', metadata={'source': 'Deep Learning.pdf', 'page': 125}),\n",
       " Document(page_content='It represents a full RNN cell that takes the current input of the sequence xi, and outputs the current hidden\\nstate, hi, passing this to the next RNN cell for our input sequence. The inside of an LSTM cell is a lot more\\ncomplicated than a traditional RNN cell, while the conventional RNN cell has a single \"internal layer\" acting\\non the current state (ht-1) and input (xt).\\nStep-by-Step LSTM Walk Through\\nIt takes care both long and short term dependency .\\nStep-1 (forget gate layer)', metadata={'source': 'Deep Learning.pdf', 'page': 126}),\n",
       " Document(page_content='The first step in our LSTM is to decide what information we’re going to throw away from the cell state. This\\ndecision is made by a sigmoid layer called the “forget gate layer .”\\nIt looks at ht−1 and xt, and outputs a number between 0 and 1 for each number in the cell state Ct−1. A 1\\nrepresents “completely keep this” while a 0 represents “completely get rid of this.”\\nStep-2 (input gate layer)\\nIn this step is to decide what new information we’re going to store in the cell state.\\nThis has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update.\\nNext, a tanh layer creates a vector of new candidate values, C~t, that could be added to the state. In the\\nnext step, we’ll combine these two to create an update to the state.\\nStep-3 (input gate layer)\\nIt’s now time to update the old cell state, Ct−1, into the new cell state Ct. The previous steps already\\ndecided what to do, we just need to actually do it.', metadata={'source': 'Deep Learning.pdf', 'page': 127}),\n",
       " Document(page_content='Step-3 (input gate layer)\\nIt’s now time to update the old cell state, Ct−1, into the new cell state Ct. The previous steps already\\ndecided what to do, we just need to actually do it.\\nWe multiply the old state by ft, forgetting the things we decided to forget earlier . Then we add it ∗ C~t. This is\\nthe new candidate values, scaled by how much we decided to update each state value.', metadata={'source': 'Deep Learning.pdf', 'page': 127}),\n",
       " Document(page_content='Step-4 (output gate layer)\\nFinally , we need to decide what we’re going to output.\\nThis output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which\\ndecides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push\\nthe values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output\\nthe parts we decided to.\\nGRU (Gated Recurrent Units)\\nSo now we know how an LSTM work, let’ s briefly look at the GRU. The GRU is the newer generation of\\nRecurrent Neural networks and is pretty similar to an LSTM. GRU’ s got rid of the cell state and used the\\nhidden state to transfer information. It also only has two gates, a reset gate and update gate.', metadata={'source': 'Deep Learning.pdf', 'page': 128}),\n",
       " Document(page_content='1. Update Gate - The update gate acts similar to the forget and input gate of an LSTM. It decides what\\ninformation to throw away and what new information to add.\\n2. Reset Gate- The reset gate is another gate is used to decide how much past information to forget.\\nGRU’ s has fewer tensor operations; therefore, they are a little speedier to train then LSTM’ s. There isn’t a\\nclear winner which one is better . Researchers and engineers usually try both to determine which one works\\nbetter for their use case.', metadata={'source': 'Deep Learning.pdf', 'page': 129})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\langchain_concepts\\.conda\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "#VEctor Embeddings and vector store\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(documents[:20],OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Function\n",
      "Activation function decides, whether a neuron should be activated or not by calculating\n",
      "weighted sum and further adding bias with it. The purpose of the activation function is to\n",
      "introduce non-linearity into the output of a neuron.\n",
      "Explanation :-\n",
      "We know , neural network has neurons that work in correspondence of weight, bias and their\n",
      "respective activation function. In a neural network, we would update the weights and biases of the\n",
      "neurons on the basis of the error at the output. This process is known as back-propagation.\n",
      "Activation functions make the back-propagation possible since the gradients are supplied along\n",
      "with the error to update the weights and biases.  \n",
      "Why do we need Non-linear activation functions :-  \n",
      "A neural network without an activation function is essentially just a linear regression model. The\n",
      "activation function does the non-linear transformation to the input making it capable to learn and\n",
      "perform more complex tasks.\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you explain Activation function?\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FAISS Vector Database\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(documents[:15], OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs and then normalizing it using the activation functions assigned to the neurons.\n",
      "The leftmost layer in a Neural Network is called the input layer , and the rightmost layer is\n",
      "called the output layer . The layers between the input and the output, are called the hidden\n",
      "layers. Any Neural Network has 1 input layer and 1 output layer .\n",
      "The number of hidden layers dif fer between dif ferent networks depending on the complexity of\n",
      "the problem. Also, each hidden layer can have its own activation function.\n",
      " \n",
      " \n",
      "Here 3 terms Comes in picture 1. Neuron , 2. W eights , 3. Bias , 4. Actiation_Function\n"
     ]
    }
   ],
   "source": [
    "query = \"what is dead activation?\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
